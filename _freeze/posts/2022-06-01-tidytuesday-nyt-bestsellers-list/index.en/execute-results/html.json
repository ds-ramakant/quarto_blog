{
  "hash": "f0fa597d3d034e90ae39de792abc0cb4",
  "result": {
    "markdown": "---\ntitle: \"TidyTuesday NYT Bestsellers list\"\nauthor: ''\ndate: '2022-06-01'\nslug: tidytuesday-nyt-bestsellers-list\ncategories: [tidytuesday,R]\ntags: [dataviz, tidytuesday, ggplot]\nsubtitle: ''\nsummary: ''\nauthors: [Me]\nlastmod: '2022-06-22T10:32:21+05:30'\nfeatured: no\nimage: graph1.png\nprojects: [Tidy Tuesday]\nformat:\n  html:\n    code-fold: true\n    code-block-border-left: true\n    toc: true\n---\n\n\n## Tidy Tuesday, Week 19 (2022)\n\n\"Learn by practice!\" is a maxim that every coder/analyst agrees upon. One of the admirable initiatives by the R/ RStudio community is [Tidy Tuesday](https://github.com/rfordatascience/tidytuesday) - every week a new dataset is released for enthusiasts to dig into. A few days back, an interesting dataset caught my eye - [NYT's Bestsellers List](https://github.com/rfordatascience/tidytuesday/blob/master/data/2022/2022-05-10/readme.md) from 1930 to 2021. This one was particularly unique as it mirrored a lot of projects that I've been doing on the OTT side as well. So I cracked my knuckles and jumped right in!\n\n## Objective\n\n1.  Understanding longevity & seasonality of how books track on the NYT bestseller's list\n2.  Deeper understanding of using customizing themes and fonts on the ggplot package\n\n### Loading the data\n\nStarting off by loading the data and the libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknitr::opts_chunk$set(warning = FALSE, message = FALSE) \nsuppressMessages({\n  library(tidyverse)\n  library(scales)\n  library(knitr)\n  library(tidytuesdayR)\n  library(forcats)\n  library(lubridate)\n  library(RColorBrewer)\n})\n\ntt_raw <- tt_load(\"2022-05-10\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n--- Compiling #TidyTuesday Information for 2022-05-10 ----\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n--- There are 2 files available ---\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n--- Starting Download ---\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tDownloading file 1 of 2: `nyt_titles.tsv`\n\tDownloading file 2 of 2: `nyt_full.tsv`\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n--- Download complete ---\n```\n:::\n\n```{.r .cell-code}\ndata <- tt_raw$nyt_titles\n```\n:::\n\n\nWhat's in the Tidy Tuesday dataset?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 7,431\nColumns: 8\n$ id          <dbl> 0, 1, 10, 100, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1…\n$ title       <chr> \"\\\"H\\\" IS FOR HOMICIDE\", \"\\\"I\\\" IS FOR INNOCENT\", \"''G'' I…\n$ author      <chr> \"Sue Grafton\", \"Sue Grafton\", \"Sue Grafton\", \"W. Bruce Cam…\n$ year        <dbl> 1991, 1992, 1990, 2012, 2006, 2016, 1985, 1994, 2002, 1999…\n$ total_weeks <dbl> 15, 11, 6, 1, 1, 3, 16, 5, 4, 1, 3, 2, 11, 6, 9, 8, 1, 1, …\n$ first_week  <date> 1991-05-05, 1992-04-26, 1990-05-06, 2012-05-27, 2006-02-1…\n$ debut_rank  <dbl> 1, 14, 4, 3, 11, 1, 9, 7, 7, 12, 13, 5, 12, 2, 11, 13, 2, …\n$ best_rank   <dbl> 2, 2, 8, 14, 14, 7, 2, 10, 12, 17, 13, 13, 8, 5, 5, 11, 4,…\n```\n:::\n:::\n\n\n### Exploratory Data Analysis (sort of)\n\nQuick EDA tells us that there the number of books in the #1 spot each year during the 50s have been increasing while the number of weeks they've spent on the NYT list has been decreasing. 2020-21 is excluded as I'm breaking up the period into decades for easy analysis\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata %>% \n  mutate(decade = factor(10*year %/% 10)) %>% \n  filter(best_rank==1, year<2020) %>% \n  group_by(decade) %>% \n  summarise(avg_weeks = mean(total_weeks),\n            no_of_rank1 = n_distinct(title))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 9 × 3\n  decade avg_weeks no_of_rank1\n  <fct>      <dbl>       <int>\n1 1930        17.1          74\n2 1940        30.1          59\n3 1950        52.4          35\n4 1960        45.7          31\n5 1970        38.6          46\n6 1980        29.5          78\n7 1990        25.7          99\n8 2000        12.5         220\n9 2010        10.3         306\n```\n:::\n:::\n\n\nThis is a fantastic starting point. Intuitively, this makes a lot of sense. There's far more competition for the #1 spot in the last 20 years which is driving down the longevity. Compare the 50's to the 2010's and the trend is hard to miss. This table is only for the books that made it to the #1 position. But how about the rest of the other books? A visual representation draws the same conclusion more elegantly.\n\n### Visualising Longevity\n\nHat-tip to a few outstanding viz I came across while researching the NYT theme. [Bob Rudis' Supreme Annotations](https://rud.is/b/2016/03/16/supreme-annotations/) and [Rahul Sangole's Visualizing Correlations](https://rsangole.netlify.app/posts/2021-04-13-30-day-chart-challenge/2021-04-13-30-day-chart-challenge)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#loading fonts that resemble the NYT viz\n#inspired by https://rud.is/b/2016/03/16/supreme-annotations/\n\nlibrary(showtext)\nshowtext_auto()\nfont_add(family = \"Open Sans\", \n         regular = \"OpenSans-CondLight.ttf\", \n         italic = \"OpenSans-CondLightItalic.ttf\", \n         bold = \"OpenSans-CondBold.ttf\")\n\n#changing facet labels as shown here \n#https://ggplot2.tidyverse.org/reference/as_labeller.html\nfacet_labels <- as_labeller(c(`1930`= \"1930 to 1939\",\n                              `1940`= \"1940 to 1949\",\n                              `1950`= \"1950 to 1959\",\n                              `1960`= \"1960 to 1969\",\n                              `1970`= \"1970 to 1979\",\n                              `1980`= \"1980 to 1989\",\n                              `1990`= \"1990 to 1999\",\n                              `2000`= \"2000 to 2009\",\n                              `2010`= \"2010 to 2019\"))\n\n\n#annotations for individual facet as discussed here https://stackoverflow.com/a/11889798/7938068\nannot_x <- data.frame(debut_rank = 5, \n                      total_weeks = 111,\n                      lab = \"Each dot\\n is a book\",\n                      decade = 1940)\n\ngraph1 <- data %>% \n  filter(best_rank==1,year<2020) %>% \n  mutate(decade = factor(10*year %/% 10)) %>% \n  ggplot(aes(x = debut_rank, y  = total_weeks))+\n  geom_point(aes(color = decade, group = debut_rank))+\n  facet_grid(~decade ,labeller = facet_labels)\n\ngraph1 <- graph1+\n  theme_minimal(base_family = \"Open Sans\")+\n  scale_color_brewer(palette = \"Paired\")+\n  labs(title = \"Longevity of NYT bestsellers has been decreasing\", \n       subtitle = \"Analysis of books that reached highest of #1 on the NYT chart tells us that starting from the 1950s, the bestsellers have reduced their longevity - or time spent on the chart.\\nFor instance, the top ranked books released in the 50s spent around 52 weeks on the chart while in contrast by the 2010s, they only spent 10 weeks.\",\n       caption = \"TidyTuesday Week 19, 2022\\n Prepared by D.S.Ramakant Raju (www.ds-ramakant.com)\",\n       x = \"Rank of title on debut week\",\n       y = \"Number of weeks on the bestsellers list\")+\n  theme(panel.border = element_rect(color = \"#2b2b2b\", \n                                    fill = NA), #borders for each facet panel\n        legend.position = \"none\", #removing legend\n        strip.text = element_text(face = \"italic\"),\n        plot.title = element_text(size = 14, face = \"bold\"),\n        panel.grid.major.x = element_line(linetype = \"dotted\", \n                                          color = \"black\"),\n        panel.grid.minor.x = element_blank(),\n        panel.grid.minor.y = element_blank(),\n        plot.caption = element_text(size = 12),\n        plot.subtitle = element_text(size = 12)) +\n  scale_y_continuous(breaks = seq(from = 25, to = 175, by = 25))+\n  #annotations by default is applied to all facets\n  #for individual facet annotations, check https://stackoverflow.com/a/11889798/7938068\n  geom_text(data = annot_x, \n            aes(x = debut_rank, y = total_weeks, \n                family = \"Open Sans\", alpha = 0.8,\n                hjust = -0.2, vjust = -0.2),\n            label = annot_x$lab\n            )\n\n\n\nprint(graph1)\n```\n\n::: {.cell-output-display}\n![](index.en_files/figure-html/Longevity-1.png){width=672}\n:::\n:::\n\n\n### Visualising Seasonality\n\nNow lets look at seasonality - is there any trend as far as the launch month is concerned? For the sake of analysis, I've truncated the analysis period to 2010 onwards to keep it more relevant and exlcude irrelvant historical data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngraph2 <- data %>% \n  filter(best_rank<11, year> 2010) %>% \n  mutate(month = month(first_week, label = T),\n         stage = case_when(year<=2015 ~ \"2011-2015\",\n                           year> 2015 ~ \"2016-2020\",\n                           T ~ \"x\")) %>% \n  group_by(stage,year,month) %>% \n  summarise(n = n_distinct(title)) %>% \n  mutate(all_titles = ave(n, year, FUN = sum),\n         pct = n/all_titles) %>%  \n  ggplot(aes(x = month, y = pct, group = 1))+\n  geom_point(size = 2, \n             alpha = 0.5, position = \"jitter\")+\n  geom_smooth(se = T) \n\n\ngraph2 <- graph2+\n  theme_minimal(base_family = \"Open Sans\")+\n  scale_color_brewer(palette = \"Paired\")+\n  labs(title = \"Monthly seasonality of books that featured in the top 10 of NYT Bestsellers list (2010-2021)\", \n       subtitle = \"Books launched in Summer (Apr-May) or Fall (Sep-Oct) were more likely to make it feature in the top 10\",\n       caption = \"TidyTuesday Week 19, 2022\\n Prepared by D.S.Ramakant Raju, www.ds-ramakant.com\",\n       x = \"Months (2010-2019)\",\n       y = \"%age of books launched within that year\")+\n  scale_y_continuous(labels = label_percent(accuracy = 1),\n                     breaks = seq(from = 0, to = 0.2, by= 0.05), \n                     limits = c(0,0.15))+\n  theme(axis.line.x = element_line(color = \"grey\"),\n        panel.grid.minor.y = element_blank())\n\ngraph2\n```\n\n::: {.cell-output-display}\n![](index.en_files/figure-html/Seasonality-1.png){width=672}\n:::\n:::\n\n\nThis is a fairly straightforward and replicable analysis. If you're a #TidyTuesday fan please feel free to share your work in the comments below\n",
    "supporting": [
      "index.en_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}