{
  "hash": "977be3ab0c5b02e26b1ab1e3bb723d2f",
  "result": {
    "markdown": "---\ntitle: 'Day 10 of #50daysofkaggle'\nauthor: ''\ndate: \"2022-10-16\"\nslug: \"day-10-of-50daysofkaggle\"\ncategories: [kaggle]\nsubtitle: Support Vector Machines\ndescription: Classification through SVM\nauthors: [Me]\nlastmod: \"2022-10-18T10:40:27+05:30\"\nfeatured: no\nimage: titanic_SVM.jpg\n#removes warning messages\nexecute: \n  warning: true\n#enabling folding code blocks, title of contents on upper-right & left border for code\nformat: \n  html: \n    code-fold: true\n    code-block-border-left: true\n    toc: true\n---\n\n# Day 10: Titanic Dataset\n\nPart of an ongoing series to [familiarise working on kaggle](https://www.ds-ramakant.com/tag/50daysofkaggle/)\n\nProgress till date:\n\n-   Download titanic dataset and assign to `train` & `test`\n-   Rearranging the data\n-   EDA (including plots and finding survival rate using `.groupby()`)\n-   Modelling\n-   Data preparation - one-hot encoding the `Sex`, `Pclass` & `Embarked` columns - appending these to the numerical columns - normalising the data - splitting between `train` into `X_train`, `y_train`, `X_test`, `y_test`\n-   Applying KNN algo\n    -   finding the right K based on accuracy. (best at K = 7)\n    -   Calculating the accuracy based on `test`\n-   Applying Decision Trees algo\n    -   with `criterion = entropy` and `max_depth = 3`\n    -   sligthly better accuracy in prediction than KNN\n\nTo do today: - classification using Support Vector Machines algo\n\n## Reading the data\n\nReading and printing the top 5 rows\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport zipfile\n\n#importing the zipfile already saved in the other folder. \nzf = zipfile.ZipFile(\"../2022-10-12-day-6-of-50daysofkaggle/titanic.zip\")\ntrain = pd.read_csv(zf.open(\"train.csv\"))\ntest = pd.read_csv(zf.open(\"test.csv\"))\n\n#Selecting only the numerical columns\nnum_col = train.select_dtypes(include=np.number).columns.tolist()\n\n#deslecting passenger ID and 'Survived' \ndel num_col[0:2] #.remove() can remove only 1 item. so for more than 1, use for loop \nselect_col = num_col\n\n#remaining columns\nstr_col= [\"Sex\", \"Embarked\", \"Survived\"]\n#Adding more elements into a list using `extend` and not `append`\nselect_col.extend(str_col)\n\ntrain_eda= train[train.columns.intersection(select_col)]\ntrain_eda\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Fare</th>\n      <th>Embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>3</td>\n      <td>male</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>7.2500</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>female</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>71.2833</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>3</td>\n      <td>female</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.9250</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>1</td>\n      <td>female</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>53.1000</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>3</td>\n      <td>male</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8.0500</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>886</th>\n      <td>0</td>\n      <td>2</td>\n      <td>male</td>\n      <td>27.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>13.0000</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>887</th>\n      <td>1</td>\n      <td>1</td>\n      <td>female</td>\n      <td>19.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>30.0000</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>888</th>\n      <td>0</td>\n      <td>3</td>\n      <td>female</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>2</td>\n      <td>23.4500</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>889</th>\n      <td>1</td>\n      <td>1</td>\n      <td>male</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>30.0000</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>890</th>\n      <td>0</td>\n      <td>3</td>\n      <td>male</td>\n      <td>32.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.7500</td>\n      <td>Q</td>\n    </tr>\n  </tbody>\n</table>\n<p>891 rows Ã— 8 columns</p>\n</div>\n```\n:::\n:::\n\n\n## Cleaning up the data\n\nChecking all `na` values in the existing dataset.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ntrain_eda.isna().sum().sort_values()\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\nSurvived      0\nPclass        0\nSex           0\nSibSp         0\nParch         0\nFare          0\nEmbarked      2\nAge         177\ndtype: int64\n```\n:::\n:::\n\n\nReplacing empty cells with median age (28)\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nmedian_age = train_eda.Age.median() #28\ntrain_eda.loc[train_eda.Age.isna(), \"Age\"] = median_age #.loc returns the view and doesn't throw warning msg\ntrain_eda.isna().sum().sort_values()\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\nSurvived    0\nPclass      0\nSex         0\nAge         0\nSibSp       0\nParch       0\nFare        0\nEmbarked    2\ndtype: int64\n```\n:::\n:::\n\n\n## Model Building\n\nSeperating X & y\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ntrain_eda = train_eda.dropna(axis = 0) #removing all rows with NA\n\nX = train_eda[[\"Age\", \"SibSp\", \"Parch\", \"Fare\"]]\nX = pd.concat([X,pd.get_dummies(data = train_eda[[\"Sex\", \"Embarked\", \"Pclass\"]], columns = [\"Sex\", \"Embarked\", \"Pclass\"])], axis = 1)\n\ny = train_eda[\"Survived\"].values\n```\n:::\n\n\n### Normalising the data\n\nTransform `X` and printing the first 5 datapoints\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nfrom sklearn import preprocessing\n\nX= preprocessing.StandardScaler().fit(X).transform(X)\nX[0:5]\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\narray([[-0.56367407,  0.43135024, -0.47432585, -0.50023975, -0.73534203,\n         0.73534203, -0.48271079, -0.30794088,  0.61679395, -0.56306042,\n        -0.51087465,  0.90032807],\n       [ 0.66921696,  0.43135024, -0.47432585,  0.78894661,  1.35991138,\n        -1.35991138,  2.07163382, -0.30794088, -1.62128697,  1.77600834,\n        -0.51087465, -1.11070624],\n       [-0.25545131, -0.47519908, -0.47432585, -0.48664993,  1.35991138,\n        -1.35991138, -0.48271079, -0.30794088,  0.61679395, -0.56306042,\n        -0.51087465,  0.90032807],\n       [ 0.43804989,  0.43135024, -0.47432585,  0.42286111,  1.35991138,\n        -1.35991138, -0.48271079, -0.30794088,  0.61679395,  1.77600834,\n        -0.51087465, -1.11070624],\n       [ 0.43804989, -0.47519908, -0.47432585, -0.4841333 , -0.73534203,\n         0.73534203, -0.48271079, -0.30794088,  0.61679395, -0.56306042,\n        -0.51087465,  0.90032807]])\n```\n:::\n:::\n\n\n### Splitting into Test & Train data\n\nSplitting into `test` & `train` data and comparing the dimensions.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\nprint ('Train set\\t :', X_train.shape,  y_train.shape,\n'\\nTest set\\t :', X_test.shape,  y_test.shape)\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTrain set\t : (711, 12) (711,) \nTest set\t : (178, 12) (178,)\n```\n:::\n:::\n\n\n## Support Vector Machines\n\nLets check the classification results using SVM. First 10 are as follows:\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nfrom sklearn import svm\nclf = svm.SVC(kernel='rbf')\nclf.fit(X_train, y_train) \n\nyhat_svm = clf.predict(X_test)\n\nprint(\"First 10 actual\\t\\t:\", y_test[0:10],\"\\nFirst 10 predicted\\t:\", yhat_svm[0:10])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFirst 10 actual\t\t: [1 1 0 1 1 1 0 0 0 0] \nFirst 10 predicted\t: [0 1 0 1 1 0 0 0 0 0]\n```\n:::\n:::\n\n\n### Confusion matrix using SVM\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport itertools\n\n# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_test, yhat_svm)\nnp.set_printoptions(precision=2)\n\nprint (classification_report(y_test, yhat_svm))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              precision    recall  f1-score   support\n\n           0       0.78      0.95      0.85       105\n           1       0.90      0.60      0.72        73\n\n    accuracy                           0.81       178\n   macro avg       0.84      0.78      0.79       178\nweighted avg       0.83      0.81      0.80       178\n\n```\n:::\n:::\n\n\n### Checking the accuracy\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nfrom sklearn import metrics\n\nprint(\"SVM Accuracy\\t:\", metrics.accuracy_score(y_test, yhat_svm),\"\\nRMSE\\t\\t\\t:\", metrics.mean_squared_error(y_test,yhat_svm),\"\\nNormalised RMSE\\t:\", metrics.mean_squared_error(y_test,yhat_svm)/np.std(y_test))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSVM Accuracy\t: 0.8089887640449438 \nRMSE\t\t\t: 0.19101123595505617 \nNormalised RMSE\t: 0.38834957789472496\n```\n:::\n:::\n\n\nAchieved **81%** accuracy using SVM with **RMSE of 0.1911**. This is is not as good as Decision Trees which resulted in [RMSE of 0.168](https://www.ds-ramakant.com/post/day-8-of-kaggle/)\n\nTherefore after 10 days of struggle, I have come to the conclusion that Decision Trees is a good classification algorithm for the Titanic dataset.\n\n",
    "supporting": [
      "index.en_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}