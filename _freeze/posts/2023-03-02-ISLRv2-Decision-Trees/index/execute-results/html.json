{
  "hash": "4bc707155bf759b69e825d00d66a6613",
  "result": {
    "markdown": "---\ntitle: \"ISLR Lab - Decision Trees\"\nsubtitle: \"Back to basics\"\ndescription: \"Revisiting concepts of Bagging, RF, Boosting & BART \"\ndate: '2023-03-02'\ncategories: [R, ISLR]\nfeatured: no\nexecute:\n  warning: false\nimage: dtree.jpg\n---\n\n\nAll the code here is derived from the legendary book [ISRL 2nd edition's](https://www.statlearning.com/) chapter 8 \"Decision Trees\". Its sometimes a wonder how elegant the base R language can be. The ISRL lab rarely mentions `tidyverse` syntax but yet manages to make the code so easy to read. The more you learn!🤓\n\n# Quick Revision\n\n1.  In $Bagging$ , the trees are grown independently on random samples of the observations. Consequently, the trees tend to be quite similar to each other. Thus, bagging can get caught in local optima and can fail to thoroughly explore the model space\n2.  In $Random Forest$, the trees are grown independently on random samples of the observations. However, each split on each tree is performed on random subset of *predictors*, thereby decorrelating the trees and leading to a better exploration relative to bagging. Both bagging & RF are ensemble methods which makes prediction from average of regression trees. Both also use *bootstrap* sampling.\n3.  In $Boosting$, we only use the original data and don't draw random samples. The trees are grown successively using a \"slow\" learning approach; each new tree is fit to the signal that is left over from the earlier trees. Boosting is an ensemble method that uses weighted sum and doesn't involve bootstrap sampling as the trees are fitted on a modified version of the original dataset.\n4.  In $BART$, we only use the original data and we grow the trees successively. However each tree is perturbed in order to avoid local minima. BART is related to the Boosting & RF --- each tree is created in a random manner like bagging & RF and each tree tries to capture some signal not yet accounted for in the current model like Boosting. BART tries to improve the partial residual of current tree by slightly modifying the previous iteration (changing the structure by altering number of nodes)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tree)\nlibrary(ISLR2)\n```\n:::\n\n\nIn today's lab, we will be using the `Carseats` dataset from the `ISLR2` package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr(Carseats)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'data.frame':\t400 obs. of  11 variables:\n $ Sales      : num  9.5 11.22 10.06 7.4 4.15 ...\n $ CompPrice  : num  138 111 113 117 141 124 115 136 132 132 ...\n $ Income     : num  73 48 35 100 64 113 105 81 110 113 ...\n $ Advertising: num  11 16 10 4 3 13 0 15 0 0 ...\n $ Population : num  276 260 269 466 340 501 45 425 108 131 ...\n $ Price      : num  120 83 80 97 128 72 108 120 124 124 ...\n $ ShelveLoc  : Factor w/ 3 levels \"Bad\",\"Good\",\"Medium\": 1 2 3 3 1 1 3 2 3 3 ...\n $ Age        : num  42 65 59 55 38 78 71 67 76 76 ...\n $ Education  : num  17 10 12 14 13 16 15 10 10 17 ...\n $ Urban      : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 1 2 2 1 1 ...\n $ US         : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 1 2 1 2 1 2 ...\n```\n:::\n:::\n\n\nCreating a column called `High` which takes a Y/N value depending on the sales and then merge it with the `Carseats` df.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nattach(Carseats)\nHigh <- factor(ifelse(Sales <= 8, \"No\", \"Yes\"))\nCarseats <- data.frame(Carseats, High)\n```\n:::\n\n\n# Fitting Classification Trees on `Carseats`\n\nCreating a classification tree to predict `High` using all variables except `Sales`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\ntree.carseats <- tree(High ~ .-Sales, data = Carseats)\nsummary(tree.carseats)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nClassification tree:\ntree(formula = High ~ . - Sales, data = Carseats)\nVariables actually used in tree construction:\n[1] \"ShelveLoc\"   \"Price\"       \"Income\"      \"CompPrice\"   \"Population\" \n[6] \"Advertising\" \"Age\"         \"US\"         \nNumber of terminal nodes:  27 \nResidual mean deviance:  0.4575 = 170.7 / 373 \nMisclassification error rate: 0.09 = 36 / 400 \n```\n:::\n:::\n\n\nMisclassification error of 9% is a good fit. Let's try plotting it\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(tree.carseats)\ntext(tree.carseats, pretty  =0 )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n## Splitting and fitting the model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2)\ntrain <- sample(1:nrow(Carseats), 200)\nCarseats.test <- Carseats[-train,]\nHigh.test <- High[-train]\ntree.carseats <- tree(High ~ .-Sales, data = Carseats, \n                      subset = train)\n```\n:::\n\n\nchecking the top few rows of predicted columns\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntree.predict <- predict(tree.carseats, Carseats.test, \n                        type = \"class\") #type is needed to declare classification model\nhead(tree.predict)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] Yes No  No  Yes No  No \nLevels: No Yes\n```\n:::\n:::\n\n\nComparing predicted with actual values\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(tree.predict, High.test)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            High.test\ntree.predict  No Yes\n         No  104  33\n         Yes  13  50\n```\n:::\n:::\n\n\nWhat's the accuracy?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(104+50)/200\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.77\n```\n:::\n:::\n\n\n**77%** Accuracy\n\n## Pruning the tree for improved classification\n\nTo improve the accuracy, lets attempt to prune the tree. For this `cv.tree()` function is used to determine the optimal level of tree complexity. Here the `FUN` argument is taken as `prune.misclass` to indicate that the cross-validation and tree pruning should be guided by the **classification error** instead of the default **deviance.**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(7)\ncv.carseats <- cv.tree(tree.carseats, FUN = prune.misclass)\nnames(cv.carseats)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"size\"   \"dev\"    \"k\"      \"method\"\n```\n:::\n:::\n\n\nNote to self:\n\n-   `k` is the regularisation parameter $\\alpha$ (alpha)\n-   `size` is \\# of terminal nodes for each tree\n-   `dev` is the number of cross-validation errors\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncv.carseats\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$size\n[1] 21 19 14  9  8  5  3  2  1\n\n$dev\n[1] 75 75 75 74 82 83 83 85 82\n\n$k\n[1] -Inf  0.0  1.0  1.4  2.0  3.0  4.0  9.0 18.0\n\n$method\n[1] \"misclass\"\n\nattr(,\"class\")\n[1] \"prune\"         \"tree.sequence\"\n```\n:::\n:::\n\n\nVisualising the tree. The classification error is least (74) at `size  = 9`\n\n\n::: {.cell .fig-cap-location-top}\n\n```{.r .cell-code}\npar(mfrow = c(1,2))\nplot(cv.carseats$size, cv.carseats$dev, type = \"b\")\nplot(cv.carseats$k, cv.carseats$dev, type = \"b\")\n```\n\n::: {.cell-output-display}\n![Relation between Deviance with tree size & regularisation parameter](index_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\nUsing the `prune.misclass()` function to prune the tree to the 9-node specification.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprune.carseats= prune.misclass(tree.carseats, best = 9)\nplot(prune.carseats)\ntext(prune.carseats, pretty = 0)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nChecking the accuracy in the good-old fashioned way (*its really that simple!)*🤓\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprune.tree.pred <- predict(prune.carseats, Carseats.test, type = \"class\")\ntable(prune.tree.pred, High.test)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n               High.test\nprune.tree.pred No Yes\n            No  97  25\n            Yes 20  58\n```\n:::\n:::\n\n\nSo what's the accuracy?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(97+58)/200\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.775\n```\n:::\n:::\n\n\n**77.5%** which is slightly better than the non-pruned tree. Not bad.\n\n## Key Takeaways\n\n1.  Without tuning the model, the default DT algo creates a tree with 27 nodes\n2.  deviance measured as a result of changing the number of nodes indicates the best DT of 9 nodes.\n3.  The code needed to write this is surprisingly simple. However, the `tidymodels` interface allows for managing the resulting output and models in a more structured way.\n\n# Fitting regression trees on `Boston` dataset\n\n`Boston` dataset contains housing values of 506 suburbs of Boston. We are trying to predict the median value of the owner-occupied homes `medv`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr(Boston)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'data.frame':\t506 obs. of  13 variables:\n $ crim   : num  0.00632 0.02731 0.02729 0.03237 0.06905 ...\n $ zn     : num  18 0 0 0 0 0 12.5 12.5 12.5 12.5 ...\n $ indus  : num  2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ...\n $ chas   : int  0 0 0 0 0 0 0 0 0 0 ...\n $ nox    : num  0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ...\n $ rm     : num  6.58 6.42 7.18 7 7.15 ...\n $ age    : num  65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ...\n $ dis    : num  4.09 4.97 4.97 6.06 6.06 ...\n $ rad    : int  1 2 2 3 3 3 5 5 5 5 ...\n $ tax    : num  296 242 242 222 222 222 311 311 311 311 ...\n $ ptratio: num  15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ...\n $ lstat  : num  4.98 9.14 4.03 2.94 5.33 ...\n $ medv   : num  24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ...\n```\n:::\n:::\n\n\nCreating the training set for `Boston` which is half the size of the original\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\ntrain.boston <- sample(1:nrow(Boston), nrow(Boston)/2)\n```\n:::\n\n\nBuilding the tree\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntree.boston <- tree(medv ~ ., data = Boston, subset = train.boston)\nsummary(tree.boston)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nRegression tree:\ntree(formula = medv ~ ., data = Boston, subset = train.boston)\nVariables actually used in tree construction:\n[1] \"rm\"    \"lstat\" \"crim\"  \"age\"  \nNumber of terminal nodes:  7 \nResidual mean deviance:  10.38 = 2555 / 246 \nDistribution of residuals:\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-10.1800  -1.7770  -0.1775   0.0000   1.9230  16.5800 \n```\n:::\n:::\n\n\nonly 4 predictors `rm, lstat, crim, age` were used. (*wonder why?)* Plotting the decision tree\n\n\n::: {.cell .fig-cap-location-top}\n\n```{.r .cell-code}\nplot(tree.boston)\ntext(tree.boston, pretty = 0)\n```\n\n::: {.cell-output-display}\n![Representing the decision tree with 7 nodes](index_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n## Making the predictions\n\n\n::: {.cell .fig-cap-location-top}\n\n```{.r .cell-code}\nyhat <- predict(tree.boston, newdata = Boston[-train.boston,])\ntest.boston <- Boston[-train.boston,\"medv\"]\n\nplot(yhat, test.boston)\nabline(0,1, col = \"red\")\n```\n\n::: {.cell-output-display}\n![This plot visualises the Predicted v/s Actuals for Boston test data](index_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\nMean Square Error is defined as $$MSE = \\frac{1}{n} \\sum_{i=1}^{n}(y_i - \\hat{y_i})^2$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean((yhat - test.boston)^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 35.28688\n```\n:::\n:::\n\n\nRMSE which uses the same units as the output variable is:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(mean((yhat - test.boston)^2))^0.5\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5.940276\n```\n:::\n:::\n\n\n## Key Takeaways:\n\nAs the SD is the same units as the outcome variable, we can say that this model leads to predictions which on an average are within ±\\$5,940 of the true median home value. Can we do better? Let's keep digging\n\n# Regression using Bagging & Random Forests\n\nNote: Bagging is a special case of Random Forest where $m = p$. The `randomForest()` function can be used for evaluating predictions from both bagging & RF. So first up is the **Bagging** process\n\n::: {.column-margin style=\"font-size: 0.8em\"}\n$m$ = sample number of predictors\n\n$p$ = total number of available predictors\n:::\n\n## Bagging\n\n::: {.column-margin style=\"font-size: 0.7em\"}\n`importance` parameter here will compute and return the importance measures of each predictor variable. `Importance` measures provide a way to assess the relative importance of each predictor variable in the random forest model, based on the decrease in accuracy that occurs when that variable is excluded from the model. This increases the runtime significantly on large datasets\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(randomForest)\nset.seed(1)\nbag.boston <- randomForest(medv ~ . , data = Boston, \n                           subset = train.boston, \n                           mtry = 12, # m = p\n                           importance = T)\nbag.boston\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\n randomForest(formula = medv ~ ., data = Boston, mtry = 12, importance = T,      subset = train.boston) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 12\n\n          Mean of squared residuals: 11.40162\n                    % Var explained: 85.17\n```\n:::\n:::\n\n::: {.cell .fig-cap-location-top}\n\n```{.r .cell-code}\nyhat.bag <- predict(bag.boston, newdata = Boston[-train.boston, ])\nplot(yhat.bag, test.boston)\nabline(0,1,col = \"red\")\n```\n\n::: {.cell-output-display}\n![Predicted v/s Actuals for Boston test data using Bagging](index_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\n\nWhat's the accuracy here? Checking the MSE\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean((yhat.bag - test.boston)^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 23.41916\n```\n:::\n:::\n\n\nAnd square root of MSE or RMSE is:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(mean((yhat.bag - test.boston)^2))^0.5\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4.839335\n```\n:::\n:::\n\n\nThat's \\$4,812 which is better than \\$5,950 derived from the 7-node decision tree discussed in [Key Takeaways]. Moving to Random Forest now.\n\n## Random Forest\n\nIts the same code, but we alter the number of predicted variables to $m= 6$ which is the `mtry` parameter\n\n::: {.column-margin style=\"font-size: 0.8em\"}\nDefault settings for `randomForest()`\n\nfor regression analysis, $m = p/3$\n\nfor classification analysis, $m = \\sqrt p$\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\nrf.boston <- randomForest(medv ~ . , data = Boston, \n                           subset = train.boston, \n                           mtry = 6, # m = p/2\n                           importance = T)\nrf.boston\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\n randomForest(formula = medv ~ ., data = Boston, mtry = 6, importance = T,      subset = train.boston) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 6\n\n          Mean of squared residuals: 10.09466\n                    % Var explained: 86.87\n```\n:::\n:::\n\n::: {.cell .fig-cap-location-top}\n\n```{.r .cell-code}\nyhat.rf <- predict(rf.boston, newdata = Boston[-train.boston, ])\nplot(yhat.rf, test.boston)\nabline(0,1,col = \"red\")\n```\n\n::: {.cell-output-display}\n![Predicted v/s Actuals for Boston test data using RandomForest](index_files/figure-html/unnamed-chunk-28-1.png){width=672}\n:::\n:::\n\n\nWhat's the MSE here?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean((yhat.rf - test.boston)^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 20.06644\n```\n:::\n:::\n\n\n.. and therefore RMSE is:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean((yhat.rf - test.boston)^2)^0.5\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4.479558\n```\n:::\n:::\n\n\nThat's ±\\$4,479 from the mean predicted values - which is better than \\$4,839 by using the Bagging method.\n\nBefore moving ahead, we can also check the `importance()` function to determine key predictors\n\n\n::: {.cell}\n\n```{.r .cell-code}\nimportance(rf.boston)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          %IncMSE IncNodePurity\ncrim    19.435587    1070.42307\nzn       3.091630      82.19257\nindus    6.140529     590.09536\nchas     1.370310      36.70356\nnox     13.263466     859.97091\nrm      35.094741    8270.33906\nage     15.144821     634.31220\ndis      9.163776     684.87953\nrad      4.793720      83.18719\ntax      4.410714     292.20949\nptratio  8.612780     902.20190\nlstat   28.725343    5813.04833\n```\n:::\n:::\n\n\nWhat are these columns?\n\n-   [**%IncMSE:**]{.underline} Avg decrease in accuracy of predictions on out-of-bag samples when given variable is calculated\n-   [**IncNodePurity:**]{.underline}Total decrease in node purity that results from split on that variable averaged over all trees.\n    -   in regression trees, the node impurity measured by the training Residual Sum of Squares(RSS)\n\n    -   in classification trees, it is the deviance\n\n\n::: {.cell .fig-cap-location-top}\n\n```{.r .cell-code}\nvarImpPlot(rf.boston)\n```\n\n::: {.cell-output-display}\n![Predicted v/s Actuals for Boston test data using Bagging](index_files/figure-html/varimport_plot-1.png){width=672}\n:::\n:::\n\n\nThis shows that the two most important variables are `rm` (average number of rooms per dwelling) and `lstat` (lower status of the population in %)\n\n# Boosting\n\nUsing the `gbm` package (Gradient Boosting Model) for boosted trees. Few notes:\n\n-   `distribution = \"gaussian\"` is considered for regression trees. For classification, it should be `distribution = \"bernoulli\"`\n-   `n.trees = 5000` is the number of trees we want to iterate over\n-   `interaction.depth = 4` limits the depth of each tree\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(gbm) #Gradient Boosting Models\nset.seed(1)\nboost.boston <- gbm(medv ~ ., data = Boston[train,], \n                    distribution = \"gaussian\",\n                    n.trees = 5000,\n                    interaction.depth = 4)\nsummary(boost.boston)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/boosting-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n            var    rel.inf\nlstat     lstat 37.7145639\nrm           rm 32.0396810\ndis         dis  6.2532723\ncrim       crim  5.9078403\nage         age  4.8163355\nindus     indus  3.7365846\ntax         tax  2.5457121\nnox         nox  2.5286998\nptratio ptratio  2.5091014\nrad         rad  1.5427771\nchas       chas  0.2451445\nzn           zn  0.1602876\n```\n:::\n:::\n\n\nAs seen earlier, `lm` and `rstat` show up as the most important variables.\n\n## partial dependence plots\n\nBy plotting the partial dependence of `rm` and `lstat` on outcome variable, we see that\n\n-   `rm` has a direct relation viz. more the number of rooms, higher the price increases\n-   `lstat` has an inverse relation viz. higher the lower stata in the neighbourhood, lower the price\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(1,2))\nplot(boost.boston, i = \"rm\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-32-1.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(boost.boston, i = \"lstat\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-32-2.png){width=672}\n:::\n:::\n\n\n## Predictions\n\n\n::: {.cell}\n\n```{.r .cell-code}\nyhat.boost <- predict(boost.boston, newdata = Boston[-train.boston,], \n                      n.trees = 5000)\n\n#| fig-cap: \"Predicted v/s Actuals for Boston test data using Boosted RF\"\n#| fig-cap-location: top\nplot(yhat.boost, test.boston)\nabline(0,1,col = \"red\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-33-1.png){width=672}\n:::\n:::\n\n\nFigure looks so much better. testing the accuracy now. starting with the MSE\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean((yhat.boost - test.boston)^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 12.98195\n```\n:::\n:::\n\n\nWow.. that's significantly lower. How about the RMSE?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean((yhat.boost - test.boston)^2)^0.5\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3.603047\n```\n:::\n:::\n\n\nAmazing. This means our predicted value on an average is ±\\$3,603 from the actual which is a signifcant improvement from the RMSE calculated by Random Forest ±\\$4,479\n\n## Adding regularisation parameter 𝛌\n\nAlso referred as the shrinkage parameter, the default value is 0.001 but we will change this to 0.01\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\nboost.boston2 <- gbm(medv ~ ., data = Boston[train,], \n                    distribution = \"gaussian\",\n                    n.trees = 5000,\n                    interaction.depth = 4, \n                    shrinkage = 0.01)\nyhat.boost2 <- predict(boost.boston2, newdata = Boston[-train.boston,], \n                      n.trees = 5000)\n```\n:::\n\n\nThe resulting MSE therefore is calculated as:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean((yhat.boost2 - test.boston)^2)^0.5\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3.593472\n```\n:::\n:::\n\n\nNow we've got it even lower at ±\\$3,593\n\n# Bayesian Additive Regression Trees\n\nThe function `gbart()` is used for regression analysis. This syntax slightly reminds me of the python syntax as we're back to creating matrices for each test, train, x & y.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(BART)\nx_BART <- Boston[,1:12]\ny_BART <- Boston[,\"medv\"]\n\nxtrain_BART <- x_BART[train.boston, ]\nytrain_BART <- y_BART[train.boston]\n\nxtest_BART <-  x_BART[-train.boston, ]\nytest_BART <- y_BART[-train.boston]\n```\n:::\n\n\nCreating the model now:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\nbart_model <- gbart(xtrain_BART, ytrain_BART, x.test = xtest_BART)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n*****Calling gbart: type=1\n*****Data:\ndata:n,p,np: 253, 12, 253\ny1,yn: 0.213439, -5.486561\nx1,x[n*p]: 0.109590, 20.080000\nxp1,xp[np*p]: 0.027310, 7.880000\n*****Number of Trees: 200\n*****Number of Cut Points: 100 ... 100\n*****burn,nd,thin: 100,1000,1\n*****Prior:beta,alpha,tau,nu,lambda,offset: 2,0.95,0.795495,3,3.71636,21.7866\n*****sigma: 4.367914\n*****w (weights): 1.000000 ... 1.000000\n*****Dirichlet:sparse,theta,omega,a,b,rho,augment: 0,0,1,0.5,1,12,0\n*****printevery: 100\n\nMCMC\ndone 0 (out of 1100)\ndone 100 (out of 1100)\ndone 200 (out of 1100)\ndone 300 (out of 1100)\ndone 400 (out of 1100)\ndone 500 (out of 1100)\ndone 600 (out of 1100)\ndone 700 (out of 1100)\ndone 800 (out of 1100)\ndone 900 (out of 1100)\ndone 1000 (out of 1100)\ntime: 8s\ntrcnt,tecnt: 1000,1000\n```\n:::\n:::\n\n\nComputing the test error MSE\n\n\n::: {.cell}\n\n```{.r .cell-code}\nyhat_bart <- bart_model$yhat.test.mean\nmean((yhat_bart - test.boston)^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 15.94718\n```\n:::\n:::\n\n\nuhoh.. it was 12.9 for the Boosted RF trees. So the RMSE can be calculated as:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean((yhat_bart - test.boston)^2)^0.5\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3.993392\n```\n:::\n:::\n\n\nThat's ±\\$3,958 which is not as good as \\$3,593 RMSE that the Boosted Tree gave us.\n\n# Summary\n\nThe calculations show that as per the RMSE, the accuracy of models can be ordered as:\n\n::: {style=\"text-align: center; font-size: 1.4em\"}\nBagging \\< Random Forest \\< BART \\< Boosting\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}