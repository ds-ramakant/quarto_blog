{
  "hash": "30f06fd52ad04a657568bce989bf48a9",
  "result": {
    "markdown": "---\ntitle: \"ISLR Lab - Decision Trees\"\nsubtitle: \"Back to basics\"\ndescription: \"Revising Bagging, RF, Boosting & BART \"\ndate: '2023-03-02'\ncategories: [R, ISLR]\nfeatured: no\nexecute:\n  warning: false\nimage: dtree.jpg\n---\n\n\nAll the code here is derived from the legendary book [ISRL 2nd edition's](https://www.statlearning.com/) chapter 8 \"Decision Trees\". Its sometimes a wonder how elegant the base R language can be. The ISRL lab rarely mentions `tidyverse` syntax but yet manages to make the code so easy to read. The more you learn!ðŸ¤“\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tree)\nlibrary(ISLR2)\n```\n:::\n\n\nIn today's lab, we will be using the `Carseats` dataset from the `ISLR2` package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr(Carseats)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'data.frame':\t400 obs. of  11 variables:\n $ Sales      : num  9.5 11.22 10.06 7.4 4.15 ...\n $ CompPrice  : num  138 111 113 117 141 124 115 136 132 132 ...\n $ Income     : num  73 48 35 100 64 113 105 81 110 113 ...\n $ Advertising: num  11 16 10 4 3 13 0 15 0 0 ...\n $ Population : num  276 260 269 466 340 501 45 425 108 131 ...\n $ Price      : num  120 83 80 97 128 72 108 120 124 124 ...\n $ ShelveLoc  : Factor w/ 3 levels \"Bad\",\"Good\",\"Medium\": 1 2 3 3 1 1 3 2 3 3 ...\n $ Age        : num  42 65 59 55 38 78 71 67 76 76 ...\n $ Education  : num  17 10 12 14 13 16 15 10 10 17 ...\n $ Urban      : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 1 2 2 1 1 ...\n $ US         : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 1 2 1 2 1 2 ...\n```\n:::\n:::\n\n\nCreating a column called `High` which takes a Y/N value depending on the sales and then merge it with the `Carseats` df.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nattach(Carseats)\nHigh <- factor(ifelse(Sales <= 8, \"No\", \"Yes\"))\nCarseats <- data.frame(Carseats, High)\n```\n:::\n\n\n# Fitting Classification Trees on `Carseats`\n\nCreating a classification tree to predict `High` using all variables except `Sales`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\ntree.carseats <- tree(High ~ .-Sales, data = Carseats)\nsummary(tree.carseats)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nClassification tree:\ntree(formula = High ~ . - Sales, data = Carseats)\nVariables actually used in tree construction:\n[1] \"ShelveLoc\"   \"Price\"       \"Income\"      \"CompPrice\"   \"Population\" \n[6] \"Advertising\" \"Age\"         \"US\"         \nNumber of terminal nodes:  27 \nResidual mean deviance:  0.4575 = 170.7 / 373 \nMisclassification error rate: 0.09 = 36 / 400 \n```\n:::\n:::\n\n\nMisclassification error of 9% is a good fit. Let's try plotting it\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(tree.carseats)\ntext(tree.carseats, pretty  =0 )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n## Splitting and fitting the model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2)\ntrain <- sample(1:nrow(Carseats), 200)\nCarseats.test <- Carseats[-train,]\nHigh.test <- High[-train]\ntree.carseats <- tree(High ~ .-Sales, data = Carseats, \n                      subset = train)\n```\n:::\n\n\nchecking the top few rows of predicted columns\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntree.predict <- predict(tree.carseats, Carseats.test, \n                        type = \"class\") #type is needed to declare classification model\nhead(tree.predict)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] Yes No  No  Yes No  No \nLevels: No Yes\n```\n:::\n:::\n\n\nComparing predicted with actual values\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(tree.predict, High.test)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            High.test\ntree.predict  No Yes\n         No  104  33\n         Yes  13  50\n```\n:::\n:::\n\n\nWhat's the accuracy?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(104+50)/200\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.77\n```\n:::\n:::\n\n\n**77%** Accuracy\n\n## Pruning the tree for improved classification\n\nTo improve the accuracy, lets attempt to prune the tree. For this `cv.tree()` function is used to determine the optimal level of tree complexity. Here the `FUN` argument is taken as `prune.misclass` to indicate that the cross-validation and tree pruning should be guided by the **classification error** instead of the default **deviance.**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(7)\ncv.carseats <- cv.tree(tree.carseats, FUN = prune.misclass)\nnames(cv.carseats)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"size\"   \"dev\"    \"k\"      \"method\"\n```\n:::\n:::\n\n\nNote to self:\n\n-   `k` is the regularisation parameter $\\alpha$ (alpha)\n-   `size` is \\# of terminal nodes for each tree\n-   `dev` is the number of cross-validation errors\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncv.carseats\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$size\n[1] 21 19 14  9  8  5  3  2  1\n\n$dev\n[1] 75 75 75 74 82 83 83 85 82\n\n$k\n[1] -Inf  0.0  1.0  1.4  2.0  3.0  4.0  9.0 18.0\n\n$method\n[1] \"misclass\"\n\nattr(,\"class\")\n[1] \"prune\"         \"tree.sequence\"\n```\n:::\n:::\n\n\nVisualising the tree. The classification error is least (74) at `size  = 9`\n\n\n::: {.cell .fig-cap-location-top}\n\n```{.r .cell-code}\npar(mfrow = c(1,2))\nplot(cv.carseats$size, cv.carseats$dev, type = \"b\")\nplot(cv.carseats$k, cv.carseats$dev, type = \"b\")\n```\n\n::: {.cell-output-display}\n![Relation between Deviance with tree size & regularisation parameter](index_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\nUsing the `prune.misclass()` function to prune the tree to the 9-node specification.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprune.carseats= prune.misclass(tree.carseats, best = 9)\nplot(prune.carseats)\ntext(prune.carseats, pretty = 0)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nChecking the accuracy in the good-old fashioned way (*its really that simple!)*ðŸ¤“\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprune.tree.pred <- predict(prune.carseats, Carseats.test, type = \"class\")\ntable(prune.tree.pred, High.test)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n               High.test\nprune.tree.pred No Yes\n            No  97  25\n            Yes 20  58\n```\n:::\n:::\n\n\nSo what's the accuracy?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(97+58)/200\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.775\n```\n:::\n:::\n\n\n**77.5%** which is slightly better than the non-pruned tree. Not bad.\n\n## Key Takeaways\n\n1.  Without tuning the model, the default DT algo creates a tree with 27 nodes\n2.  deviance measured as a result of changing the number of nodes indicates the best DT of 9 nodes.\n3.  The code needed to write this is surprisingly simple. However, the `tidymodels` interface allows for managing the resulting output and models in a more structured way.\n\n# Fitting regression trees on `Boston` dataset\n\n`Boston` dataset contains housing values of 506 suburbs of Boston. We are trying to predict the median value of the owner-occupied homes `medv`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr(Boston)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'data.frame':\t506 obs. of  13 variables:\n $ crim   : num  0.00632 0.02731 0.02729 0.03237 0.06905 ...\n $ zn     : num  18 0 0 0 0 0 12.5 12.5 12.5 12.5 ...\n $ indus  : num  2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ...\n $ chas   : int  0 0 0 0 0 0 0 0 0 0 ...\n $ nox    : num  0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ...\n $ rm     : num  6.58 6.42 7.18 7 7.15 ...\n $ age    : num  65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ...\n $ dis    : num  4.09 4.97 4.97 6.06 6.06 ...\n $ rad    : int  1 2 2 3 3 3 5 5 5 5 ...\n $ tax    : num  296 242 242 222 222 222 311 311 311 311 ...\n $ ptratio: num  15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ...\n $ lstat  : num  4.98 9.14 4.03 2.94 5.33 ...\n $ medv   : num  24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ...\n```\n:::\n:::\n\n\nCreating the training set for `Boston` which is half the size of the original\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\ntrain.boston <- sample(1:nrow(Boston), nrow(Boston)/2)\n```\n:::\n\n\nBuilding the tree\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntree.boston <- tree(medv ~ ., data = Boston, subset = train.boston)\nsummary(tree.boston)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nRegression tree:\ntree(formula = medv ~ ., data = Boston, subset = train.boston)\nVariables actually used in tree construction:\n[1] \"rm\"    \"lstat\" \"crim\"  \"age\"  \nNumber of terminal nodes:  7 \nResidual mean deviance:  10.38 = 2555 / 246 \nDistribution of residuals:\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-10.1800  -1.7770  -0.1775   0.0000   1.9230  16.5800 \n```\n:::\n:::\n\n\nonly 4 predictors `rm, lstat, crim, age` were used. (*wonder why?)* Plotting the decision tree\n\n\n::: {.cell .fig-cap-location-top}\n\n```{.r .cell-code}\nplot(tree.boston)\ntext(tree.boston, pretty = 0)\n```\n\n::: {.cell-output-display}\n![Representing the decision tree with 7 nodes](index_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n## Making the predictions\n\n\n::: {.cell .fig-cap-location-top}\n\n```{.r .cell-code}\nyhat <- predict(tree.boston, newdata = Boston[-train.boston,])\ntest.boston <- Boston[-train.boston,\"medv\"]\n\nplot(yhat, test.boston)\nabline(0,1, col = \"red\")\n```\n\n::: {.cell-output-display}\n![This plot visualises the Predicted v/s Actuals for Boston test data](index_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\nMean Square Error is defined as $$MSE = \\frac{1}{n} \\sum_{i=1}^{n}(y_i - \\hat{y_i})^2$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean((yhat - test.boston)^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 35.28688\n```\n:::\n:::\n\n\nStandard Deviation the square root of MSE which is\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(mean((yhat - test.boston)^2))^0.5\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5.940276\n```\n:::\n:::\n\n\n## Key Takeaways: \n\nAs the SD is the same units as the outcome variable, we can say that this model leads to predictions which on an average are within Â±\\$5,940 of the true median home value. Can we do better? Let's keep digging\n\n# Regression using Bagging & Random Forests\n\nNote: Bagging is a special case of Random Forest where $m = p$. The `randomForest()` function can be used for evaluating predictions from both bagging & RF.\n\n::: {.column-margin style=\"font-size: 4\"}\n$m$ = sample number of predictors\n\n$p$ = total number of available predictors\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}