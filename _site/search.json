[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Page that gives more information about me"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Null Hypothesis",
    "section": "",
    "text": "Hi there üëãüèº My name is Ramakant and I am a consumer marketing enthusiast with a natural curiosity towards data and technology. I started this blog as a way to sharpen my data science skills and occasionally pour those creative juices."
  },
  {
    "objectID": "index.html#get-in-touch",
    "href": "index.html#get-in-touch",
    "title": "The Null Hypothesis",
    "section": "Get In Touch ‚úçÔ∏è",
    "text": "Get In Touch ‚úçÔ∏è\nI‚Äôm always on the lookout for a good conversation. Feel free to mail me at d.s.ramakant@gmail.com or check out my Linkedin profile\n\nLink to my resume (broken for now) üìÉ"
  },
  {
    "objectID": "index.html#blog-posts",
    "href": "index.html#blog-posts",
    "title": "The Null Hypothesis",
    "section": "Blog Posts",
    "text": "Blog Posts"
  },
  {
    "objectID": "posts/2022-01-14-hello-world2/index.en.html",
    "href": "posts/2022-01-14-hello-world2/index.en.html",
    "title": "Hello World2",
    "section": "",
    "text": "A lot of this is courtesy of Apres Hill‚Äôs blog\nhttps://www.apreshill.com/blog/2020-12-new-year-new-blogdown/#step-1-create-repo"
  },
  {
    "objectID": "posts/2022-04-05-what-a-wonderful-wordle/index.en.html",
    "href": "posts/2022-04-05-what-a-wonderful-wordle/index.en.html",
    "title": "What a wonderful ‚Äòwordle‚Äô!",
    "section": "",
    "text": "Who ever thought that a bunch of black and green boxes would bring out the logophile in us all? With friends and family groups sharing their progress, I find this to be an entertaining mind-puzzle to kickstart the day.\nAnd I was not alone in my quest for 5 letter words. Wordle has tickled the fascination of many in the data science community. I found Arthur Holtz‚Äôs lucid breakdown of the Wordle dataset quite interesting. Of course, there is 3B1B‚Äôs incredibly detailed videos on applying Information Theory to this 6-by-5 grid. (original video as well as the follow-up errata)\nOthers have simulated the wordle game (like here) or even solved it for you (like this blog). I‚Äôve read at least one blog post that has an academic take on the matter.\nFortunately for the reader, none of the above will be attempted by me. My inspiration comes from Gerry Chng‚Äôs Frequency Analysis Approach where I‚Äôve tried to understand the most commonly occuring letters in the official word list by position by considering a ranking mechanism"
  },
  {
    "objectID": "posts/2022-04-05-what-a-wonderful-wordle/index.en.html#what-is-a-wordle",
    "href": "posts/2022-04-05-what-a-wonderful-wordle/index.en.html#what-is-a-wordle",
    "title": "What a wonderful ‚Äòwordle‚Äô!",
    "section": "What is a wordle?",
    "text": "What is a wordle?\nThe game rules are fairly simple:\n\nYou need to guess a 5-letter word. One new word is given every day\nYou are given 6 guesses\nAfter every guess, each square is coded by a color\n\nGREY: chosen letter is not in the word\nYELLOW: chosen letter is in the word by wrong position\nGREEN: chosen letter is in the word and in the correct position\n\nRepetition of letters is allowed\n\nThat‚Äôs it!\nIn my opinion, one of the reasons for the game going viral is the way the results are shared. You‚Äôve possibly seen something like this floating around:\n\n\n\nSample world share\n\n\n‚Ä¶And if your family too has been bitten hard by the Wordle bug, then you would be familiar with group messages like this!\n\n\n\nWorld share in whatsapp"
  },
  {
    "objectID": "posts/2022-04-05-what-a-wonderful-wordle/index.en.html#frequency-analysis",
    "href": "posts/2022-04-05-what-a-wonderful-wordle/index.en.html#frequency-analysis",
    "title": "What a wonderful ‚Äòwordle‚Äô!",
    "section": "Frequency analysis",
    "text": "Frequency analysis\nArthur Hotlz‚Äôs blog is a good place to start for extracting and loading the Official Wordle list. After parsing and cleaning the data, here‚Äôs all the words broken down into a single rectangular dataframe word_list .\nUpdate 29th Jan ‚Äô23: NYT‚Äôs .js file is not retrieving any list for some reason. I‚Äôve referred to Arjun Vikram‚Äôs repo on dagshub\n\n\nCode\nknitr::opts_chunk$set(warning = FALSE, message = FALSE) \nsuppressMessages({ \nlibrary(httr)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(ggthemes)\nlibrary(scales)\nlibrary(tidyr)\nlibrary(tibble)\nlibrary(forcats)\nlibrary(knitr)\nlibrary(kableExtra)\ntheme_set(theme_light())\n})\n\nurl <- \"https://www.nytimes.com/games/wordle/main.18637ca1.js\" #not working\nurl2 <- \"https://dagshub.com/arjvik/wordle-wordlist/raw/e8d07d33a59a6b05f3b08bd827385604f89d89a0/answerlist.txt\"\nwordle_script_text <- GET(url2) %>% \n  content(as = \"text\", encoding = \"UTF-8\")\n# word_list = substr(\n#   wordle_script_text,\n#   # cigar is the first word\n#   str_locate(wordle_script_text, \"cigar\")[,\"start\"],\n#   # shave is the last word\n#   str_locate(wordle_script_text, \"shave\")[,\"end\"]) %>%\n#   str_remove_all(\"\\\"\") %>%\n#   str_split(\",\") %>%\n#   data.frame() %>%\n#   select(word = 1) %>%\n#   mutate(word = toupper(word))\n\n\nwordle_list <- str_split(wordle_script_text, \"\\n\")\n\nwordle_list <- data.frame(wordle_list) \n\nwordle_list <- rename(wordle_list, word = names(wordle_list)[1] ) %>% mutate(word = toupper(word)) #renaming column to 'word'\n\ndim(wordle_list)\n\n\n[1] 2310    1\n\n\nCode\nhead(wordle_list)\n\n\n   word\n1 CIGAR\n2 REBUT\n3 SISSY\n4 HUMPH\n5 AWAKE\n6 BLUSH\n\n\nModification to the above is another dataframe with each of the characters separated into columns which we‚Äôll call position_word_list\nThe line select(-x) removes the empty column that is created due to the seperate() function\n\n\nCode\nposition_word_list <- wordle_list %>% \n  separate(word, \n           sep = \"\", \n           into = c(\"x\",\"p1\",\"p2\",\"p3\",\"p4\",\"p5\")) %>% \n  select(-x)\nhead(position_word_list,10)\n\n\n   p1 p2 p3 p4 p5\n1   C  I  G  A  R\n2   R  E  B  U  T\n3   S  I  S  S  Y\n4   H  U  M  P  H\n5   A  W  A  K  E\n6   B  L  U  S  H\n7   F  O  C  A  L\n8   E  V  A  D  E\n9   N  A  V  A  L\n10  S  E  R  V  E\n\n\nNow onto some frequency analysis. Here‚Äôs a breakdown of all the letters in the wordle list sorted by number of occurrences stored in letter_list and creating a simple bar graph.\n\n\nCode\nletter_list <- wordle_list %>%\n  as.character() %>%\n  str_split(\"\") %>% \n  as.data.frame() %>% \n  select(w_letter = 1) %>% \n  filter(row_number()!=1) %>%\n  filter(w_letter %in% LETTERS) %>% \n  mutate(type = case_when(w_letter %in% c(\"A\",\"E\",\"I\",\"O\",\"U\") ~ \"vowel\",\n                          T ~ \"consonant\")) %>% \n  group_by(w_letter, type) %>% \n  summarise(freq = n()) %>% \n  arrange(desc(freq))\n\nletter_list %>% ungroup() %>% \n  ggplot(aes(x = reorder(w_letter, -freq), y = freq))+\n  geom_col(aes(fill = type))+\n  scale_y_continuous(labels = comma)+\n  geom_text(aes(label = freq), \n            size = 3)+\n  labs(x = \"Letter\", y = \"Frequency\",\n       title = \"Frequency of letters in Official Wordle list\")\n\n\n\n\n\nThis is interesting. Now I‚Äôm curious to know the top words by each position. To do this, I created a single table called freq_table that provides me with the frequency of occurrences by position for each letter. To iterate this process across all the 5 places, I used a for loop. Output is generated via the kableExtra package which provides a neat scrollable window\n\n\nCode\n#declaring null table\nfreq_table <- tibble(alpha = LETTERS)\n\nfor(i in 1:5){\n    test <- position_word_list %>% \n    select(all_of(i)) %>%\n# group_by_at() used for column index ID\n    group_by_at(1) %>% \n    summarise(f = n()) %>% \n    arrange(desc(f)) %>% \n#first column returns p1, p2.. etc and is standardised\n    rename(a = 1) \n\n#adding the freq values to a new dataframe\n    freq_table <- freq_table %>%\n    left_join(test, by = c(\"alpha\" = \"a\")) \n\n#renaming column name to reflect the position number\n    colnames(freq_table)[1+i] = paste0(\"p\",i)\n    rm(test)\n}\n#replacing NA with zero\nfreq_table[is.na(freq_table)] <- 0 \n#output using kable's scrollable window \nkable(freq_table, \n      format = \"html\", \n      caption = \"Frequency Table\") %>%\n    kable_styling() %>%\n    scroll_box(width = \"70%\", height = \"300px\") %>% \n  kable_classic()\n\n\n\n\nFrequency Table\n \n  \n    alpha \n    p1 \n    p2 \n    p3 \n    p4 \n    p5 \n  \n \n\n  \n    A \n    140 \n    304 \n    306 \n    162 \n    63 \n  \n  \n    B \n    173 \n    16 \n    56 \n    24 \n    11 \n  \n  \n    C \n    198 \n    40 \n    56 \n    150 \n    31 \n  \n  \n    D \n    111 \n    20 \n    75 \n    69 \n    118 \n  \n  \n    E \n    72 \n    241 \n    177 \n    318 \n    422 \n  \n  \n    F \n    135 \n    8 \n    25 \n    35 \n    26 \n  \n  \n    G \n    115 \n    11 \n    67 \n    76 \n    41 \n  \n  \n    H \n    69 \n    144 \n    9 \n    28 \n    137 \n  \n  \n    I \n    34 \n    201 \n    266 \n    158 \n    11 \n  \n  \n    J \n    20 \n    2 \n    3 \n    2 \n    0 \n  \n  \n    K \n    20 \n    10 \n    12 \n    55 \n    113 \n  \n  \n    L \n    87 \n    200 \n    112 \n    162 \n    155 \n  \n  \n    M \n    107 \n    38 \n    61 \n    68 \n    42 \n  \n  \n    N \n    37 \n    87 \n    137 \n    182 \n    130 \n  \n  \n    O \n    41 \n    279 \n    243 \n    132 \n    58 \n  \n  \n    P \n    141 \n    61 \n    57 \n    50 \n    56 \n  \n  \n    Q \n    23 \n    5 \n    1 \n    0 \n    0 \n  \n  \n    R \n    105 \n    267 \n    163 \n    150 \n    212 \n  \n  \n    S \n    365 \n    16 \n    80 \n    171 \n    36 \n  \n  \n    T \n    149 \n    77 \n    111 \n    139 \n    253 \n  \n  \n    U \n    33 \n    185 \n    165 \n    82 \n    1 \n  \n  \n    V \n    43 \n    15 \n    49 \n    45 \n    0 \n  \n  \n    W \n    82 \n    44 \n    26 \n    25 \n    17 \n  \n  \n    X \n    0 \n    14 \n    12 \n    3 \n    8 \n  \n  \n    Y \n    6 \n    22 \n    29 \n    3 \n    364 \n  \n  \n    Z \n    3 \n    2 \n    11 \n    20 \n    4 \n  \n\n\n\n\n\nThis table looks good. However, for my visualisation, I want to plot the top 10 letters in each position. For this, I‚Äôm going to use pivot_longer() to make it easier to generate the viz.\n\n\nCode\nfreq_table_long10 <- freq_table %>% \n  pivot_longer(cols = !alpha, names_to = \"position\", values_to = \"freq\") %>% \n  select(position, alpha, freq) %>% \n  arrange(position, -freq) %>% \n  group_by(position) %>% \n  slice_head(n = 10) %>% ungroup\n\nkable(freq_table_long10, \n      format = \"html\", \n      caption = \"Top 10 letters within each position\") %>%\n    kable_styling() %>%\n    scroll_box(height = \"200px\") %>% \n  kable_classic()\n\n\n\n\nTop 10 letters within each position\n \n  \n    position \n    alpha \n    freq \n  \n \n\n  \n    p1 \n    S \n    365 \n  \n  \n    p1 \n    C \n    198 \n  \n  \n    p1 \n    B \n    173 \n  \n  \n    p1 \n    T \n    149 \n  \n  \n    p1 \n    P \n    141 \n  \n  \n    p1 \n    A \n    140 \n  \n  \n    p1 \n    F \n    135 \n  \n  \n    p1 \n    G \n    115 \n  \n  \n    p1 \n    D \n    111 \n  \n  \n    p1 \n    M \n    107 \n  \n  \n    p2 \n    A \n    304 \n  \n  \n    p2 \n    O \n    279 \n  \n  \n    p2 \n    R \n    267 \n  \n  \n    p2 \n    E \n    241 \n  \n  \n    p2 \n    I \n    201 \n  \n  \n    p2 \n    L \n    200 \n  \n  \n    p2 \n    U \n    185 \n  \n  \n    p2 \n    H \n    144 \n  \n  \n    p2 \n    N \n    87 \n  \n  \n    p2 \n    T \n    77 \n  \n  \n    p3 \n    A \n    306 \n  \n  \n    p3 \n    I \n    266 \n  \n  \n    p3 \n    O \n    243 \n  \n  \n    p3 \n    E \n    177 \n  \n  \n    p3 \n    U \n    165 \n  \n  \n    p3 \n    R \n    163 \n  \n  \n    p3 \n    N \n    137 \n  \n  \n    p3 \n    L \n    112 \n  \n  \n    p3 \n    T \n    111 \n  \n  \n    p3 \n    S \n    80 \n  \n  \n    p4 \n    E \n    318 \n  \n  \n    p4 \n    N \n    182 \n  \n  \n    p4 \n    S \n    171 \n  \n  \n    p4 \n    A \n    162 \n  \n  \n    p4 \n    L \n    162 \n  \n  \n    p4 \n    I \n    158 \n  \n  \n    p4 \n    C \n    150 \n  \n  \n    p4 \n    R \n    150 \n  \n  \n    p4 \n    T \n    139 \n  \n  \n    p4 \n    O \n    132 \n  \n  \n    p5 \n    E \n    422 \n  \n  \n    p5 \n    Y \n    364 \n  \n  \n    p5 \n    T \n    253 \n  \n  \n    p5 \n    R \n    212 \n  \n  \n    p5 \n    L \n    155 \n  \n  \n    p5 \n    H \n    137 \n  \n  \n    p5 \n    N \n    130 \n  \n  \n    p5 \n    D \n    118 \n  \n  \n    p5 \n    K \n    113 \n  \n  \n    p5 \n    A \n    63 \n  \n\n\n\n\n\nSo we have the # of occurences in each position laid out in a tidy format in one long rectangular dataframe. Now sprinkling some magic courtesy ggplot\n\nSide note on reordering within facets\nI tried my best to understand why I was unable to sort within each facet in spite of using free_y. Apparently that‚Äôs a known issue and a workaround has been discussed by David Robinson, Julia Silger and Tyler Rinker. To achieve this, two more functions need to be created reorder_within and scale_y_reordered\n\n\nCode\nreorder_within <- function(x, by, within, fun = mean, sep = \"___\", ...) {\n  new_x <- paste(x, within, sep = sep)\n  stats::reorder(new_x, by, FUN = fun)\n}\n\nscale_y_reordered <- function(..., sep = \"___\") {\n  reg <- paste0(sep, \".+$\")\n  ggplot2::scale_y_discrete(labels = function(x) gsub(reg, \"\", x), ...)\n}\n\nfreq_table_long10 %>% \n  mutate(type = case_when(alpha %in% c(\"A\",\"E\",\"I\",\"O\",\"U\") ~ \"vowel\",\n                          T ~ \"consonant\")) %>% \n  ggplot(aes(y = reorder_within(alpha, freq, position), x = freq))+\n  geom_col(aes(fill = type))+\n  scale_y_reordered()+\n  facet_wrap(~position, \n             scales = \"free_y\", \n             ncol = 5)+\n  labs(x = \"Frequency\", y = \"Letter\",\n       title = \"Frequency of top 10 letters by position in Official Wordle list \",\n       caption = \"D.S.Ramakant Raju\\nwww.linkedin.com/in/dsramakant/\")\n\n\n\n\n\nAha! Things are starting to get more clearer. Highly common letters in the 1st position are S, C, B, T and P - notice there‚Äôs only 1 vowel (A) that occurs in the top 10. Vowels appear more frequently in the 2nd and 3rd positions. Last position has a higher occurrence of E, Y, T, R & L"
  },
  {
    "objectID": "posts/2022-04-05-what-a-wonderful-wordle/index.en.html#which-words-can-be-the-best-worlde-openers",
    "href": "posts/2022-04-05-what-a-wonderful-wordle/index.en.html#which-words-can-be-the-best-worlde-openers",
    "title": "What a wonderful ‚Äòwordle‚Äô!",
    "section": "Which words can be the best Worlde openers?",
    "text": "Which words can be the best Worlde openers?\nArmed with the above knowledge, we now can filter out the commonly occurring words. Also I use a naive method to rank these words basis the occurrence of the letters. For instance, in the picture above, the word S A I N T seems to be a valid word comprising of the top occurring letters.\nAdmittedly, I use a pretty crude method to determine the best openers. Known drawbacks of this methodology are:\n\nDoesn‚Äôt consider the future path of the word (number of steps to get to the right word)\nOnly considers the rank of the letters and not the actual probability of occurrence\n\nWith that out of the way, I was able to determine that there are 39 words that can be formed with the top 5 occurring letters in each position. I‚Äôve created a score that is determined by the rank of each letter within its position. For instance, S A I N T gets a score of 9 by summing up 1 (S in first position) + 1 (A in second position) + 2 (I in third) + 2 (N in fourth) + 3 (T in fifth). The lower the score, the higher the frequency of occurrences. Scroll below to read the rest of the words.\n\n\nCode\n#function to pick the top 5 letters\ntop5_selection <- function(x)\n{x %>% arrange(desc(x[2])) %>% head(5) %>% select(1)}\n#defining null table\nfinal_grid <- tibble(ranking = 1:5)\n\nfor(i in 2:length(freq_table)){\n  t <- top5_selection(select(freq_table,1,all_of(i)))\n  final_grid <- cbind(final_grid,t)\n  colnames(final_grid)[i] = paste0(\"p\",i-1)\n}\ntopwords <- position_word_list %>% \nfilter(p1 %in% final_grid$p1,\n       p2 %in% final_grid$p2,\n       p3 %in% final_grid$p3,\n       p4 %in% final_grid$p4,\n       p5 %in% final_grid$p5) \n\n#finding consolidated score of each word\ntopwords %<>%\n  rowwise() %>% \n  mutate(p1_rank = which(p1 == final_grid$p1),\n         p2_rank = which(p2 == final_grid$p2),\n         p3_rank = which(p3 == final_grid$p3),\n         p4_rank = which(p4 == final_grid$p4),\n         p5_rank = which(p5 == final_grid$p5))\n\ntopwords2 <- topwords %>% \n  transmute(word = paste0(p1,p2,p3,p4,p5),\n         score = sum(p1_rank, p2_rank,p3_rank, p4_rank, p5_rank)) %>% \n  arrange(score)\n\nkable(topwords2, \n      format = \"html\",\n      caption = \"Top 39 words\") %>%\n    kable_styling() %>%\n    scroll_box(width = \"50%\", height = \"400px\") %>% \n  kable_classic()\n\n\n\n\nTop 39 words\n \n  \n    word \n    score \n  \n \n\n  \n    SAINT \n    9 \n  \n  \n    CRANE \n    9 \n  \n  \n    COAST \n    11 \n  \n  \n    BRINE \n    11 \n  \n  \n    CEASE \n    11 \n  \n  \n    CRONE \n    11 \n  \n  \n    CAUSE \n    12 \n  \n  \n    CRIER \n    12 \n  \n  \n    BRINY \n    12 \n  \n  \n    BOAST \n    12 \n  \n  \n    TAINT \n    12 \n  \n  \n    CRONY \n    12 \n  \n  \n    TEASE \n    13 \n  \n  \n    POISE \n    13 \n  \n  \n    TOAST \n    13 \n  \n  \n    PAINT \n    13 \n  \n  \n    BOOST \n    14 \n  \n  \n    POINT \n    14 \n  \n  \n    COUNT \n    14 \n  \n  \n    PRONE \n    14 \n  \n  \n    BEAST \n    14 \n  \n  \n    PRINT \n    15 \n  \n  \n    PAUSE \n    15 \n  \n  \n    TAUNT \n    15 \n  \n  \n    PROSE \n    15 \n  \n  \n    CREST \n    15 \n  \n  \n    CRUST \n    16 \n  \n  \n    BRIAR \n    16 \n  \n  \n    BOULE \n    16 \n  \n  \n    POESY \n    16 \n  \n  \n    CRUEL \n    16 \n  \n  \n    PRUNE \n    16 \n  \n  \n    BRUNT \n    16 \n  \n  \n    TRUER \n    17 \n  \n  \n    TREAT \n    18 \n  \n  \n    TRIAL \n    18 \n  \n  \n    TRUST \n    18 \n  \n  \n    TRULY \n    19 \n  \n  \n    TROLL \n    20 \n  \n\n\n\n\n\nThere we have it. My take on the best opening words.\nI‚Äôve used words such as SAINT, CRANE, COAST etc and they‚Äôve been reasonably useful to me.\nWhich are your favourite opening words? Please do leave a comment to let me know!"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn‚Äôt specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/2022-06-01-tidytuesday-nyt-bestsellers-list/index.en.html",
    "href": "posts/2022-06-01-tidytuesday-nyt-bestsellers-list/index.en.html",
    "title": "TidyTuesday NYT Bestsellers list",
    "section": "",
    "text": "‚ÄúLearn by practice!‚Äù is a maxim that every coder/analyst agrees upon. One of the admirable initiatives by the R/ RStudio community is Tidy Tuesday - every week a new dataset is released for enthusiasts to dig into. A few days back, an interesting dataset caught my eye - NYT‚Äôs Bestsellers List from 1930 to 2021. This one was particularly unique as it mirrored a lot of projects that I‚Äôve been doing on the OTT side as well. So I cracked my knuckles and jumped right in!"
  },
  {
    "objectID": "posts/2022-06-01-tidytuesday-nyt-bestsellers-list/index.en.html#objective",
    "href": "posts/2022-06-01-tidytuesday-nyt-bestsellers-list/index.en.html#objective",
    "title": "TidyTuesday NYT Bestsellers list",
    "section": "Objective",
    "text": "Objective\n\nUnderstanding longevity & seasonality of how books track on the NYT bestseller‚Äôs list\nDeeper understanding of using customizing themes and fonts on the ggplot package\n\n\nLoading the data\nStarting off by loading the data and the libraries\n\n\nCode\nknitr::opts_chunk$set(warning = FALSE, message = FALSE) \nsuppressMessages({\n  library(tidyverse)\n  library(scales)\n  library(knitr)\n  library(tidytuesdayR)\n  library(forcats)\n  library(lubridate)\n  library(RColorBrewer)\n})\n\ntt_raw <- tt_load(\"2022-05-10\")\n\n\n--- Compiling #TidyTuesday Information for 2022-05-10 ----\n\n\n--- There are 2 files available ---\n\n\n--- Starting Download ---\n\n\n\n    Downloading file 1 of 2: `nyt_titles.tsv`\n    Downloading file 2 of 2: `nyt_full.tsv`\n\n\n--- Download complete ---\n\n\nCode\ndata <- tt_raw$nyt_titles\n\n\nWhat‚Äôs in the Tidy Tuesday dataset?\n\n\nCode\nglimpse(data)\n\n\nRows: 7,431\nColumns: 8\n$ id          <dbl> 0, 1, 10, 100, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1‚Ä¶\n$ title       <chr> \"\\\"H\\\" IS FOR HOMICIDE\", \"\\\"I\\\" IS FOR INNOCENT\", \"''G'' I‚Ä¶\n$ author      <chr> \"Sue Grafton\", \"Sue Grafton\", \"Sue Grafton\", \"W. Bruce Cam‚Ä¶\n$ year        <dbl> 1991, 1992, 1990, 2012, 2006, 2016, 1985, 1994, 2002, 1999‚Ä¶\n$ total_weeks <dbl> 15, 11, 6, 1, 1, 3, 16, 5, 4, 1, 3, 2, 11, 6, 9, 8, 1, 1, ‚Ä¶\n$ first_week  <date> 1991-05-05, 1992-04-26, 1990-05-06, 2012-05-27, 2006-02-1‚Ä¶\n$ debut_rank  <dbl> 1, 14, 4, 3, 11, 1, 9, 7, 7, 12, 13, 5, 12, 2, 11, 13, 2, ‚Ä¶\n$ best_rank   <dbl> 2, 2, 8, 14, 14, 7, 2, 10, 12, 17, 13, 13, 8, 5, 5, 11, 4,‚Ä¶\n\n\n\n\nExploratory Data Analysis (sort of)\nQuick EDA tells us that there the number of books in the #1 spot each year during the 50s have been increasing while the number of weeks they‚Äôve spent on the NYT list has been decreasing. 2020-21 is excluded as I‚Äôm breaking up the period into decades for easy analysis\n\n\nCode\ndata %>% \n  mutate(decade = factor(10*year %/% 10)) %>% \n  filter(best_rank==1, year<2020) %>% \n  group_by(decade) %>% \n  summarise(avg_weeks = mean(total_weeks),\n            no_of_rank1 = n_distinct(title))\n\n\n# A tibble: 9 √ó 3\n  decade avg_weeks no_of_rank1\n  <fct>      <dbl>       <int>\n1 1930        17.1          74\n2 1940        30.1          59\n3 1950        52.4          35\n4 1960        45.7          31\n5 1970        38.6          46\n6 1980        29.5          78\n7 1990        25.7          99\n8 2000        12.5         220\n9 2010        10.3         306\n\n\nThis is a fantastic starting point. Intuitively, this makes a lot of sense. There‚Äôs far more competition for the #1 spot in the last 20 years which is driving down the longevity. Compare the 50‚Äôs to the 2010‚Äôs and the trend is hard to miss. This table is only for the books that made it to the #1 position. But how about the rest of the other books? A visual representation draws the same conclusion more elegantly.\n\n\nVisualising Longevity\nHat-tip to a few outstanding viz I came across while researching the NYT theme. Bob Rudis‚Äô Supreme Annotations and Rahul Sangole‚Äôs Visualizing Correlations\n\n\nCode\n#loading fonts that resemble the NYT viz\n#inspired by https://rud.is/b/2016/03/16/supreme-annotations/\n\nlibrary(showtext)\nshowtext_auto()\nfont_add(family = \"Open Sans\", \n         regular = \"OpenSans-CondLight.ttf\", \n         italic = \"OpenSans-CondLightItalic.ttf\", \n         bold = \"OpenSans-CondBold.ttf\")\n\n#changing facet labels as shown here \n#https://ggplot2.tidyverse.org/reference/as_labeller.html\nfacet_labels <- as_labeller(c(`1930`= \"1930 to 1939\",\n                              `1940`= \"1940 to 1949\",\n                              `1950`= \"1950 to 1959\",\n                              `1960`= \"1960 to 1969\",\n                              `1970`= \"1970 to 1979\",\n                              `1980`= \"1980 to 1989\",\n                              `1990`= \"1990 to 1999\",\n                              `2000`= \"2000 to 2009\",\n                              `2010`= \"2010 to 2019\"))\n\n\n#annotations for individual facet as discussed here https://stackoverflow.com/a/11889798/7938068\nannot_x <- data.frame(debut_rank = 5, \n                      total_weeks = 111,\n                      lab = \"Each dot\\n is a book\",\n                      decade = 1940)\n\ngraph1 <- data %>% \n  filter(best_rank==1,year<2020) %>% \n  mutate(decade = factor(10*year %/% 10)) %>% \n  ggplot(aes(x = debut_rank, y  = total_weeks))+\n  geom_point(aes(color = decade, group = debut_rank))+\n  facet_grid(~decade ,labeller = facet_labels)\n\ngraph1 <- graph1+\n  theme_minimal(base_family = \"Open Sans\")+\n  scale_color_brewer(palette = \"Paired\")+\n  labs(title = \"Longevity of NYT bestsellers has been decreasing\", \n       subtitle = \"Analysis of books that reached highest of #1 on the NYT chart tells us that starting from the 1950s, the bestsellers have reduced their longevity - or time spent on the chart.\\nFor instance, the top ranked books released in the 50s spent around 52 weeks on the chart while in contrast by the 2010s, they only spent 10 weeks.\",\n       caption = \"TidyTuesday Week 19, 2022\\n Prepared by D.S.Ramakant Raju (www.ds-ramakant.com)\",\n       x = \"Rank of title on debut week\",\n       y = \"Number of weeks on the bestsellers list\")+\n  theme(panel.border = element_rect(color = \"#2b2b2b\", \n                                    fill = NA), #borders for each facet panel\n        legend.position = \"none\", #removing legend\n        strip.text = element_text(face = \"italic\"),\n        plot.title = element_text(size = 14, face = \"bold\"),\n        panel.grid.major.x = element_line(linetype = \"dotted\", \n                                          color = \"black\"),\n        panel.grid.minor.x = element_blank(),\n        panel.grid.minor.y = element_blank(),\n        plot.caption = element_text(size = 12),\n        plot.subtitle = element_text(size = 12)) +\n  scale_y_continuous(breaks = seq(from = 25, to = 175, by = 25))+\n  #annotations by default is applied to all facets\n  #for individual facet annotations, check https://stackoverflow.com/a/11889798/7938068\n  geom_text(data = annot_x, \n            aes(x = debut_rank, y = total_weeks, \n                family = \"Open Sans\", alpha = 0.8,\n                hjust = -0.2, vjust = -0.2),\n            label = annot_x$lab\n            )\n\n\n\nprint(graph1)\n\n\n\n\n\n\n\nVisualising Seasonality\nNow lets look at seasonality - is there any trend as far as the launch month is concerned? For the sake of analysis, I‚Äôve truncated the analysis period to 2010 onwards to keep it more relevant and exlcude irrelvant historical data.\n\n\nCode\ngraph2 <- data %>% \n  filter(best_rank<11, year> 2010) %>% \n  mutate(month = month(first_week, label = T),\n         stage = case_when(year<=2015 ~ \"2011-2015\",\n                           year> 2015 ~ \"2016-2020\",\n                           T ~ \"x\")) %>% \n  group_by(stage,year,month) %>% \n  summarise(n = n_distinct(title)) %>% \n  mutate(all_titles = ave(n, year, FUN = sum),\n         pct = n/all_titles) %>%  \n  ggplot(aes(x = month, y = pct, group = 1))+\n  geom_point(size = 2, \n             alpha = 0.5, position = \"jitter\")+\n  geom_smooth(se = T) \n\n\ngraph2 <- graph2+\n  theme_minimal(base_family = \"Open Sans\")+\n  scale_color_brewer(palette = \"Paired\")+\n  labs(title = \"Monthly seasonality of books that featured in the top 10 of NYT Bestsellers list (2010-2021)\", \n       subtitle = \"Books launched in Summer (Apr-May) or Fall (Sep-Oct) were more likely to make it feature in the top 10\",\n       caption = \"TidyTuesday Week 19, 2022\\n Prepared by D.S.Ramakant Raju, www.ds-ramakant.com\",\n       x = \"Months (2010-2019)\",\n       y = \"%age of books launched within that year\")+\n  scale_y_continuous(labels = label_percent(accuracy = 1),\n                     breaks = seq(from = 0, to = 0.2, by= 0.05), \n                     limits = c(0,0.15))+\n  theme(axis.line.x = element_line(color = \"grey\"),\n        panel.grid.minor.y = element_blank())\n\ngraph2\n\n\n\n\n\nThis is a fairly straightforward and replicable analysis. If you‚Äôre a #TidyTuesday fan please feel free to share your work in the comments below"
  },
  {
    "objectID": "posts/2022-10-07-day-1-of-50daysofkaggle/index.en.html",
    "href": "posts/2022-10-07-day-1-of-50daysofkaggle/index.en.html",
    "title": "day 1 of #50daysofkaggle",
    "section": "",
    "text": "Introducing my own personal sprint training ‚Äú50 Days of Kaggle‚Äù\nThe task is simple:\nI‚Äôd want to use this blog to journal my progress. Hopefully by 26th Nov‚Äô22, I‚Äôd have improved from where I‚Äôm starting out.\nSo what do we have for Day 1?"
  },
  {
    "objectID": "posts/2022-10-07-day-1-of-50daysofkaggle/index.en.html#reading-the-data",
    "href": "posts/2022-10-07-day-1-of-50daysofkaggle/index.en.html#reading-the-data",
    "title": "day 1 of #50daysofkaggle",
    "section": "Reading the data",
    "text": "Reading the data\nFirst things first, import libraries\n\n\nCode\nimport requests\nimport numpy as np\nimport pandas as pd\nimport kaggle\n\nkaggle.api.authenticate()\n\n\nNote to self: below command did not work\n\n\nCode\n#kaggle.api.dataset_download_files(\"titanic\", path = \".\", unzip = True)\n\n\nHowever, this one does as per this link https://www.kaggle.com/general/138914\n\n\nCode\nkaggle.api.competition_download_files(\"titanic\", path = \".\")\n\n\nThis pulls the .zip file in the local folder. because this is a zip file, we need package called zipfile(note to self: don‚Äôt forget the console command reticulate::py_install(\"zipfile\"))\nhttps://stackoverflow.com/a/56786517/7938068\nReading and checking the first rows of train\n\n\nCode\nimport zipfile\n\nzf = zipfile.ZipFile(\"titanic.zip\")\ntrain = pd.read_csv(zf.open(\"train.csv\"))\ntest = pd.read_csv(zf.open(\"test.csv\"))\ntrain.head()\n\n\n\n\n\n\n  \n    \n      \n      PassengerId\n      Survived\n      Pclass\n      Name\n      Sex\n      Age\n      SibSp\n      Parch\n      Ticket\n      Fare\n      Cabin\n      Embarked\n    \n  \n  \n    \n      0\n      1\n      0\n      3\n      Braund, Mr. Owen Harris\n      male\n      22.0\n      1\n      0\n      A/5 21171\n      7.2500\n      NaN\n      S\n    \n    \n      1\n      2\n      1\n      1\n      Cumings, Mrs. John Bradley (Florence Briggs Th...\n      female\n      38.0\n      1\n      0\n      PC 17599\n      71.2833\n      C85\n      C\n    \n    \n      2\n      3\n      1\n      3\n      Heikkinen, Miss. Laina\n      female\n      26.0\n      0\n      0\n      STON/O2. 3101282\n      7.9250\n      NaN\n      S\n    \n    \n      3\n      4\n      1\n      1\n      Futrelle, Mrs. Jacques Heath (Lily May Peel)\n      female\n      35.0\n      1\n      0\n      113803\n      53.1000\n      C123\n      S\n    \n    \n      4\n      5\n      0\n      3\n      Allen, Mr. William Henry\n      male\n      35.0\n      0\n      0\n      373450\n      8.0500\n      NaN\n      S\n    \n  \n\n\n\n\nChecking the first rows of test\n\n\nCode\ntest.head()\n\n\n\n\n\n\n  \n    \n      \n      PassengerId\n      Pclass\n      Name\n      Sex\n      Age\n      SibSp\n      Parch\n      Ticket\n      Fare\n      Cabin\n      Embarked\n    \n  \n  \n    \n      0\n      892\n      3\n      Kelly, Mr. James\n      male\n      34.5\n      0\n      0\n      330911\n      7.8292\n      NaN\n      Q\n    \n    \n      1\n      893\n      3\n      Wilkes, Mrs. James (Ellen Needs)\n      female\n      47.0\n      1\n      0\n      363272\n      7.0000\n      NaN\n      S\n    \n    \n      2\n      894\n      2\n      Myles, Mr. Thomas Francis\n      male\n      62.0\n      0\n      0\n      240276\n      9.6875\n      NaN\n      Q\n    \n    \n      3\n      895\n      3\n      Wirz, Mr. Albert\n      male\n      27.0\n      0\n      0\n      315154\n      8.6625\n      NaN\n      S\n    \n    \n      4\n      896\n      3\n      Hirvonen, Mrs. Alexander (Helga E Lindqvist)\n      female\n      22.0\n      1\n      1\n      3101298\n      12.2875\n      NaN\n      S\n    \n  \n\n\n\n\nThis took me a whole day to figure out. End of Day1 ü§∑‚Äç‚ôÇÔ∏è"
  },
  {
    "objectID": "posts/2022-10-10-day-4-of-50daysofkaggle/index.en.html",
    "href": "posts/2022-10-10-day-4-of-50daysofkaggle/index.en.html",
    "title": "Day 4 of #50daysofkaggle",
    "section": "",
    "text": "Previously I managed to download the titanic zip file using the kaggle api and extract two datasets train and test .\n\n\n\n\nCode\nimport requests\nimport numpy as np\nimport pandas as pd\nimport kaggle \nimport zipfile \n\nkaggle.api.authenticate()\n\nkaggle.api.competition_download_files(\"titanic\", path = \".\")\n\nzf = zipfile.ZipFile(\"titanic.zip\")\ntrain = pd.read_csv(zf.open(\"train.csv\"))\ntest = pd.read_csv(zf.open(\"test.csv\"))\n\n\n\n\n\nLets see what we have here in the train data\n\n\nCode\ntrain.head()\n\n\n\n\n\n\n  \n    \n      \n      PassengerId\n      Survived\n      Pclass\n      Name\n      Sex\n      Age\n      SibSp\n      Parch\n      Ticket\n      Fare\n      Cabin\n      Embarked\n    \n  \n  \n    \n      0\n      1\n      0\n      3\n      Braund, Mr. Owen Harris\n      male\n      22.0\n      1\n      0\n      A/5 21171\n      7.2500\n      NaN\n      S\n    \n    \n      1\n      2\n      1\n      1\n      Cumings, Mrs. John Bradley (Florence Briggs Th...\n      female\n      38.0\n      1\n      0\n      PC 17599\n      71.2833\n      C85\n      C\n    \n    \n      2\n      3\n      1\n      3\n      Heikkinen, Miss. Laina\n      female\n      26.0\n      0\n      0\n      STON/O2. 3101282\n      7.9250\n      NaN\n      S\n    \n    \n      3\n      4\n      1\n      1\n      Futrelle, Mrs. Jacques Heath (Lily May Peel)\n      female\n      35.0\n      1\n      0\n      113803\n      53.1000\n      C123\n      S\n    \n    \n      4\n      5\n      0\n      3\n      Allen, Mr. William Henry\n      male\n      35.0\n      0\n      0\n      373450\n      8.0500\n      NaN\n      S\n    \n  \n\n\n\n\nChecking more details on train columns.\n\n\nCode\ntrain.info()\n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.7+ KB\n\n\nPassengerID is the unique identifier for each row while Survived is the column to be predicted. Finding only the numeric columns and dropping the above two (ref - this link)\n\n\nCode\nnum_col = train.select_dtypes(include=np.number).columns.tolist()\n\ndel num_col[0:2] #.remove() can remove only 1 item. so for more than 1, use for loop \nselect_col = num_col\n\n\nAmong the string columns, only Sex and Embarked are relevant for our analysis. Ref - selecting columns by intersection\n\n\nCode\nstr_col= [\"Sex\", \"Embarked\", \"Survived\"]\nstr_col\n\n\nselect_col.extend(str_col)\nselect_col\n\ntrain_eda= train[train.columns.intersection(select_col)]\n\ntrain_eda.info()\n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 891 entries, 0 to 890\nData columns (total 8 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Survived  891 non-null    int64  \n 1   Pclass    891 non-null    int64  \n 2   Sex       891 non-null    object \n 3   Age       714 non-null    float64\n 4   SibSp     891 non-null    int64  \n 5   Parch     891 non-null    int64  \n 6   Fare      891 non-null    float64\n 7   Embarked  889 non-null    object \ndtypes: float64(2), int64(4), object(2)\nmemory usage: 55.8+ KB\n\n\n\n\n\nSeems like the older folks were luckier than the younger ones\n\n\nCode\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.catplot(data = train_eda, x = \"Age\", y = \"Fare\", hue = \"Survived\")\n            \nplt.show()\n\n\n\n\n\nDistinction between Class 1 and Class 3 is clear - poorer folks in Class 3 were younger (mean being just under 30 years) than the richer folks in Class 1\n\n\nCode\nplt.clf()\nsns.boxplot(data = train_eda, y = \"Age\", x = \"Pclass\", hue = \"Survived\")\nplt.show()\n\n\n\n\n\nBelow graph shows us that among the survivors, there were a lot more women than men survived the disaster.\n\n\nCode\nplt.clf()\nplot = sns.FacetGrid(data = train_eda, col = \"Survived\", hue = \"Sex\", col_wrap = 2)\nplot.map(sns.scatterplot, \"Age\", \"Fare\")\nplot.axes[-1].legend()\nplt.show()\n\n\n<Figure size 672x480 with 0 Axes>\n\n\n\n\n\nWe continue to notice the clearer skew towards Class 1 (richer) compared to Class 3 (poorer)\n\n\nCode\nplt.clf()\nplot = sns.FacetGrid(data = train_eda, col = \"Survived\", hue = \"Pclass\", col_wrap = 2)\nplot.map(sns.scatterplot, \"Age\", \"Fare\")\nplot.axes[-1].legend()\nplt.show()\n\n\n<Figure size 672x480 with 0 Axes>\n\n\n\n\n\n\n\nCode\nplt.clf()\nplot = sns.FacetGrid(data = train_eda, col = \"Survived\", hue = \"SibSp\", col_wrap = 2)\nplot.map(sns.scatterplot, \"Age\", \"Fare\")\nplot.axes[-1].legend()\nplt.show()\n\n\n<Figure size 672x480 with 0 Axes>\n\n\n\n\n\n\n\n\n\n\nCode\ntrain_df = pd.get_dummies(train_eda, columns = [\"Sex\", \"Embarked\"])\n\ntrain_df.info()\n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 891 entries, 0 to 890\nData columns (total 11 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   Survived    891 non-null    int64  \n 1   Pclass      891 non-null    int64  \n 2   Age         714 non-null    float64\n 3   SibSp       891 non-null    int64  \n 4   Parch       891 non-null    int64  \n 5   Fare        891 non-null    float64\n 6   Sex_female  891 non-null    uint8  \n 7   Sex_male    891 non-null    uint8  \n 8   Embarked_C  891 non-null    uint8  \n 9   Embarked_Q  891 non-null    uint8  \n 10  Embarked_S  891 non-null    uint8  \ndtypes: float64(2), int64(4), uint8(5)\nmemory usage: 46.2 KB\n\n\nAnd day 2 comes to an endü§∑‚Äç‚ôÇÔ∏è"
  }
]