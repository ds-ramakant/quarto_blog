[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Null Hypothesis",
    "section": "",
    "text": "Hi there üëãüèº\nMy name is Ramakant and I‚Äôve always had a natural curiosity towards consumer, data and technology.\nFive words to describe myself are ‚Äì Restless, Resourceful, Passionate, Grounded and Empathetic.\nI‚Äôm an independent marketing consultant based out of Mumbai helping brands and companies with revenue-led marketing efforts.\nWelcome to my personal blog of random musings on marketing & other stuff.\nYou will find more about me on this page."
  },
  {
    "objectID": "index.html#get-in-touch",
    "href": "index.html#get-in-touch",
    "title": "The Null Hypothesis",
    "section": "Get In Touch ‚úçÔ∏è",
    "text": "Get In Touch ‚úçÔ∏è\nFeel free to mail me at d.s.ramakant@gmail.com or check out my Linkedin profile.\nIf you‚Äôd like to leave comments, please click the vertical bar on the right üëâüèº"
  },
  {
    "objectID": "index.html#blog-posts",
    "href": "index.html#blog-posts",
    "title": "The Null Hypothesis",
    "section": "Blog Posts",
    "text": "Blog Posts"
  },
  {
    "objectID": "posts/2022-10-10-day-4-of-50daysofkaggle/index.en.html",
    "href": "posts/2022-10-10-day-4-of-50daysofkaggle/index.en.html",
    "title": "Day 4 of #50daysofkaggle",
    "section": "",
    "text": "Previously I managed to download the titanic zip file using the kaggle api and extract two datasets train and test .\n\n\n\n\nCode\nimport requests\nimport numpy as np\nimport pandas as pd\nimport kaggle \nimport zipfile \n\nkaggle.api.authenticate()\n\nkaggle.api.competition_download_files(\"titanic\", path = \".\")\n\nzf = zipfile.ZipFile(\"titanic.zip\")\ntrain = pd.read_csv(zf.open(\"train.csv\"))\ntest = pd.read_csv(zf.open(\"test.csv\"))\n\n\n\n\n\nLets see what we have here in the train data\n\n\nCode\ntrain.head()\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\n\n\n\nChecking more details on train columns.\n\n\nCode\ntrain.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.7+ KB\n\n\nPassengerID is the unique identifier for each row while Survived is the column to be predicted. Finding only the numeric columns and dropping the above two (ref - this link)\n\n\nCode\nnum_col = train.select_dtypes(include=np.number).columns.tolist()\n\ndel num_col[0:2] #.remove() can remove only 1 item. so for more than 1, use for loop \nselect_col = num_col\n\n\nAmong the string columns, only Sex and Embarked are relevant for our analysis. Ref - selecting columns by intersection\n\n\nCode\nstr_col= [\"Sex\", \"Embarked\", \"Survived\"]\nstr_col\n\n\nselect_col.extend(str_col)\nselect_col\n\ntrain_eda= train[train.columns.intersection(select_col)]\n\ntrain_eda.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 8 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Survived  891 non-null    int64  \n 1   Pclass    891 non-null    int64  \n 2   Sex       891 non-null    object \n 3   Age       714 non-null    float64\n 4   SibSp     891 non-null    int64  \n 5   Parch     891 non-null    int64  \n 6   Fare      891 non-null    float64\n 7   Embarked  889 non-null    object \ndtypes: float64(2), int64(4), object(2)\nmemory usage: 55.8+ KB\n\n\n\n\n\nSeems like the older folks were luckier than the younger ones\n\n\nCode\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.catplot(data = train_eda, x = \"Age\", y = \"Fare\", hue = \"Survived\")\n            \nplt.show()\n\n\n\n\n\n\n\n\n\nDistinction between Class 1 and Class 3 is clear - poorer folks in Class 3 were younger (mean being just under 30 years) than the richer folks in Class 1\n\n\nCode\nplt.clf()\nsns.boxplot(data = train_eda, y = \"Age\", x = \"Pclass\", hue = \"Survived\")\nplt.show()\n\n\n\n\n\n\n\n\n\nBelow graph shows us that among the survivors, there were a lot more women than men survived the disaster.\n\n\nCode\nplt.clf()\nplot = sns.FacetGrid(data = train_eda, col = \"Survived\", hue = \"Sex\", col_wrap = 2)\nplot.map(sns.scatterplot, \"Age\", \"Fare\")\nplot.axes[-1].legend()\nplt.show()\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\nWe continue to notice the clearer skew towards Class 1 (richer) compared to Class 3 (poorer)\n\n\nCode\nplt.clf()\nplot = sns.FacetGrid(data = train_eda, col = \"Survived\", hue = \"Pclass\", col_wrap = 2)\nplot.map(sns.scatterplot, \"Age\", \"Fare\")\nplot.axes[-1].legend()\nplt.show()\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\nCode\nplt.clf()\nplot = sns.FacetGrid(data = train_eda, col = \"Survived\", hue = \"SibSp\", col_wrap = 2)\nplot.map(sns.scatterplot, \"Age\", \"Fare\")\nplot.axes[-1].legend()\nplt.show()\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ntrain_df = pd.get_dummies(train_eda, columns = [\"Sex\", \"Embarked\"])\n\ntrain_df.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 11 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   Survived    891 non-null    int64  \n 1   Pclass      891 non-null    int64  \n 2   Age         714 non-null    float64\n 3   SibSp       891 non-null    int64  \n 4   Parch       891 non-null    int64  \n 5   Fare        891 non-null    float64\n 6   Sex_female  891 non-null    uint8  \n 7   Sex_male    891 non-null    uint8  \n 8   Embarked_C  891 non-null    uint8  \n 9   Embarked_Q  891 non-null    uint8  \n 10  Embarked_S  891 non-null    uint8  \ndtypes: float64(2), int64(4), uint8(5)\nmemory usage: 46.2 KB\n\n\nAnd day 2 comes to an endü§∑‚Äç‚ôÇÔ∏è"
  },
  {
    "objectID": "posts/2022-10-10-day-4-of-50daysofkaggle/index.en.html#importing-libraries-and-reading-the-data",
    "href": "posts/2022-10-10-day-4-of-50daysofkaggle/index.en.html#importing-libraries-and-reading-the-data",
    "title": "Day 4 of #50daysofkaggle",
    "section": "",
    "text": "Code\nimport requests\nimport numpy as np\nimport pandas as pd\nimport kaggle \nimport zipfile \n\nkaggle.api.authenticate()\n\nkaggle.api.competition_download_files(\"titanic\", path = \".\")\n\nzf = zipfile.ZipFile(\"titanic.zip\")\ntrain = pd.read_csv(zf.open(\"train.csv\"))\ntest = pd.read_csv(zf.open(\"test.csv\"))"
  },
  {
    "objectID": "posts/2022-10-10-day-4-of-50daysofkaggle/index.en.html#rearranging-train-dataset",
    "href": "posts/2022-10-10-day-4-of-50daysofkaggle/index.en.html#rearranging-train-dataset",
    "title": "Day 4 of #50daysofkaggle",
    "section": "",
    "text": "Lets see what we have here in the train data\n\n\nCode\ntrain.head()\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\n\n\n\nChecking more details on train columns.\n\n\nCode\ntrain.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.7+ KB\n\n\nPassengerID is the unique identifier for each row while Survived is the column to be predicted. Finding only the numeric columns and dropping the above two (ref - this link)\n\n\nCode\nnum_col = train.select_dtypes(include=np.number).columns.tolist()\n\ndel num_col[0:2] #.remove() can remove only 1 item. so for more than 1, use for loop \nselect_col = num_col\n\n\nAmong the string columns, only Sex and Embarked are relevant for our analysis. Ref - selecting columns by intersection\n\n\nCode\nstr_col= [\"Sex\", \"Embarked\", \"Survived\"]\nstr_col\n\n\nselect_col.extend(str_col)\nselect_col\n\ntrain_eda= train[train.columns.intersection(select_col)]\n\ntrain_eda.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 8 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Survived  891 non-null    int64  \n 1   Pclass    891 non-null    int64  \n 2   Sex       891 non-null    object \n 3   Age       714 non-null    float64\n 4   SibSp     891 non-null    int64  \n 5   Parch     891 non-null    int64  \n 6   Fare      891 non-null    float64\n 7   Embarked  889 non-null    object \ndtypes: float64(2), int64(4), object(2)\nmemory usage: 55.8+ KB"
  },
  {
    "objectID": "posts/2022-10-10-day-4-of-50daysofkaggle/index.en.html#eda",
    "href": "posts/2022-10-10-day-4-of-50daysofkaggle/index.en.html#eda",
    "title": "Day 4 of #50daysofkaggle",
    "section": "",
    "text": "Seems like the older folks were luckier than the younger ones\n\n\nCode\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.catplot(data = train_eda, x = \"Age\", y = \"Fare\", hue = \"Survived\")\n            \nplt.show()\n\n\n\n\n\n\n\n\n\nDistinction between Class 1 and Class 3 is clear - poorer folks in Class 3 were younger (mean being just under 30 years) than the richer folks in Class 1\n\n\nCode\nplt.clf()\nsns.boxplot(data = train_eda, y = \"Age\", x = \"Pclass\", hue = \"Survived\")\nplt.show()\n\n\n\n\n\n\n\n\n\nBelow graph shows us that among the survivors, there were a lot more women than men survived the disaster.\n\n\nCode\nplt.clf()\nplot = sns.FacetGrid(data = train_eda, col = \"Survived\", hue = \"Sex\", col_wrap = 2)\nplot.map(sns.scatterplot, \"Age\", \"Fare\")\nplot.axes[-1].legend()\nplt.show()\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\nWe continue to notice the clearer skew towards Class 1 (richer) compared to Class 3 (poorer)\n\n\nCode\nplt.clf()\nplot = sns.FacetGrid(data = train_eda, col = \"Survived\", hue = \"Pclass\", col_wrap = 2)\nplot.map(sns.scatterplot, \"Age\", \"Fare\")\nplot.axes[-1].legend()\nplt.show()\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\nCode\nplt.clf()\nplot = sns.FacetGrid(data = train_eda, col = \"Survived\", hue = \"SibSp\", col_wrap = 2)\nplot.map(sns.scatterplot, \"Age\", \"Fare\")\nplot.axes[-1].legend()\nplt.show()\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;"
  },
  {
    "objectID": "posts/2022-10-10-day-4-of-50daysofkaggle/index.en.html#roughspace",
    "href": "posts/2022-10-10-day-4-of-50daysofkaggle/index.en.html#roughspace",
    "title": "Day 4 of #50daysofkaggle",
    "section": "",
    "text": "Code\ntrain_df = pd.get_dummies(train_eda, columns = [\"Sex\", \"Embarked\"])\n\ntrain_df.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 11 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   Survived    891 non-null    int64  \n 1   Pclass      891 non-null    int64  \n 2   Age         714 non-null    float64\n 3   SibSp       891 non-null    int64  \n 4   Parch       891 non-null    int64  \n 5   Fare        891 non-null    float64\n 6   Sex_female  891 non-null    uint8  \n 7   Sex_male    891 non-null    uint8  \n 8   Embarked_C  891 non-null    uint8  \n 9   Embarked_Q  891 non-null    uint8  \n 10  Embarked_S  891 non-null    uint8  \ndtypes: float64(2), int64(4), uint8(5)\nmemory usage: 46.2 KB\n\n\nAnd day 2 comes to an endü§∑‚Äç‚ôÇÔ∏è"
  },
  {
    "objectID": "posts/2022-10-07-day-1-of-50daysofkaggle/index.en.html",
    "href": "posts/2022-10-07-day-1-of-50daysofkaggle/index.en.html",
    "title": "day 1 of #50daysofkaggle",
    "section": "",
    "text": "Introducing my own personal sprint training ‚Äú50 Days of Kaggle‚Äù\nThe task is simple:\nI‚Äôd want to use this blog to journal my progress. Hopefully by 26th Nov‚Äô22, I‚Äôd have improved from where I‚Äôm starting out.\nSo what do we have for Day 1?"
  },
  {
    "objectID": "posts/2022-10-07-day-1-of-50daysofkaggle/index.en.html#reading-the-data",
    "href": "posts/2022-10-07-day-1-of-50daysofkaggle/index.en.html#reading-the-data",
    "title": "day 1 of #50daysofkaggle",
    "section": "Reading the data",
    "text": "Reading the data\nFirst things first, import libraries\n\n\nCode\nimport requests\nimport numpy as np\nimport pandas as pd\nimport kaggle\n\nkaggle.api.authenticate()\n\n\nNote to self: below command did not work\n\n\nCode\n#kaggle.api.dataset_download_files(\"titanic\", path = \".\", unzip = True)\n\n\nHowever, this one does as per this link https://www.kaggle.com/general/138914\n\n\nCode\nkaggle.api.competition_download_files(\"titanic\", path = \".\")\n\n\nThis pulls the .zip file in the local folder. because this is a zip file, we need package called zipfile(note to self: don‚Äôt forget the console command reticulate::py_install(\"zipfile\"))\nhttps://stackoverflow.com/a/56786517/7938068\nReading and checking the first rows of train\n\n\nCode\nimport zipfile\n\nzf = zipfile.ZipFile(\"titanic.zip\")\ntrain = pd.read_csv(zf.open(\"train.csv\"))\ntest = pd.read_csv(zf.open(\"test.csv\"))\ntrain.head()\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\n\n\n\nChecking the first rows of test\n\n\nCode\ntest.head()\n\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n892\n3\nKelly, Mr. James\nmale\n34.5\n0\n0\n330911\n7.8292\nNaN\nQ\n\n\n1\n893\n3\nWilkes, Mrs. James (Ellen Needs)\nfemale\n47.0\n1\n0\n363272\n7.0000\nNaN\nS\n\n\n2\n894\n2\nMyles, Mr. Thomas Francis\nmale\n62.0\n0\n0\n240276\n9.6875\nNaN\nQ\n\n\n3\n895\n3\nWirz, Mr. Albert\nmale\n27.0\n0\n0\n315154\n8.6625\nNaN\nS\n\n\n4\n896\n3\nHirvonen, Mrs. Alexander (Helga E Lindqvist)\nfemale\n22.0\n1\n1\n3101298\n12.2875\nNaN\nS\n\n\n\n\n\n\n\nThis took me a whole day to figure out. End of Day1 ü§∑‚Äç‚ôÇÔ∏è"
  },
  {
    "objectID": "posts/2023-02-20-day-11-of-50daysofkaggle/index.html",
    "href": "posts/2023-02-20-day-11-of-50daysofkaggle/index.html",
    "title": "Day 11 of #50daysofkaggle",
    "section": "",
    "text": "Till now I practiced creating classification predictions on the Titanic dataset using KNN, DT and SVM algorithms. As per Kaggle, my submission got a score of 77%. Now I‚Äôm going to try these approaches in R.\nsteps to do : data reading &gt; cleaning &gt; replacing NA &gt; splitting &gt; model using Decision Trees&gt; comparing results"
  },
  {
    "objectID": "posts/2023-02-20-day-11-of-50daysofkaggle/index.html#splitting-the-data",
    "href": "posts/2023-02-20-day-11-of-50daysofkaggle/index.html#splitting-the-data",
    "title": "Day 11 of #50daysofkaggle",
    "section": "Splitting the data",
    "text": "Splitting the data\nSplitting the data into train & test\n\n\nCode\ndf_split &lt;- initial_split(df_n, prop = 0.8)\ntrain &lt;- training(df_split)\ntest &lt;- testing(df_split)\n\ndf_split\n\n\n&lt;Training/Testing/Total&gt;\n&lt;712/179/891&gt;\n\n\ncreating the recipe\n\n\nCode\ndt_recipe &lt;- recipe(Survived ~ ., data = df_n) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors()) %&gt;% \n  #replacing NA values in Age with median Age\n  step_mutate_at(Age, fn = ~ replace_na(Age, median(Age, na.rm = T))) %&gt;% \n  #updating the role of the PassengerId to exclude from analysis\n  update_role(PassengerId, new_role = \"id_variable\")\n\ndt_recipe\n\n\nAnother way to view the recipe using tidy() function\n\n\nCode\ntidy(dt_recipe)\n\n\n# A tibble: 3 √ó 6\n  number operation type      trained skip  id             \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;          \n1      1 step      dummy     FALSE   FALSE dummy_jD1sy    \n2      2 step      normalize FALSE   FALSE normalize_CvfCk\n3      3 step      mutate_at FALSE   FALSE mutate_at_xAmJj"
  },
  {
    "objectID": "posts/2023-02-20-day-11-of-50daysofkaggle/index.html#model-creation",
    "href": "posts/2023-02-20-day-11-of-50daysofkaggle/index.html#model-creation",
    "title": "Day 11 of #50daysofkaggle",
    "section": "Model Creation",
    "text": "Model Creation\nDeclaring a model dt_model as a Decision Tree with depth as 3 and engine rpart\n\n\nCode\ndt_model &lt;- decision_tree(mode = \"classification\", tree_depth = 3) %&gt;% \n  set_engine(\"rpart\")\ndt_model %&gt;% translate()\n\n\nDecision Tree Model Specification (classification)\n\nMain Arguments:\n  tree_depth = 3\n\nComputational engine: rpart \n\nModel fit template:\nrpart::rpart(formula = missing_arg(), data = missing_arg(), weights = missing_arg(), \n    maxdepth = 3)"
  },
  {
    "objectID": "posts/2023-02-20-day-11-of-50daysofkaggle/index.html#workflow-creation",
    "href": "posts/2023-02-20-day-11-of-50daysofkaggle/index.html#workflow-creation",
    "title": "Day 11 of #50daysofkaggle",
    "section": "Workflow creation",
    "text": "Workflow creation\nWorkflow = recipe + model\n\n\nCode\ndt_wf &lt;- workflow() %&gt;%\n  add_model(dt_model) %&gt;% \n  add_recipe(dt_recipe)"
  },
  {
    "objectID": "posts/2023-02-20-day-11-of-50daysofkaggle/index.html#predicting-on-test-data",
    "href": "posts/2023-02-20-day-11-of-50daysofkaggle/index.html#predicting-on-test-data",
    "title": "Day 11 of #50daysofkaggle",
    "section": "Predicting on test data",
    "text": "Predicting on test data\nFitting the dt_wf workflow with model created on train data to predict the test data\n\n\nCode\nset.seed(2023)\ndt_predict &lt;- predict(fit(dt_wf, data = train), test)\nhead(dt_predict)\n\n\n# A tibble: 6 √ó 1\n  .pred_class\n  &lt;fct&gt;      \n1 0          \n2 0          \n3 0          \n4 1          \n5 0          \n6 0          \n\n\nCreating a new tibble called preidcted_table by binding the predicted values .pred_class to the test data\n\n\nCode\npredicted_table &lt;- bind_cols(test, dt_predict) %&gt;% \n  rename(dt_yhat = .pred_class) %&gt;% \n  select(Survived, dt_yhat) \nhead(predicted_table)\n\n\n# A tibble: 6 √ó 2\n  Survived dt_yhat\n  &lt;fct&gt;    &lt;fct&gt;  \n1 0        0      \n2 0        0      \n3 0        0      \n4 0        1      \n5 0        0      \n6 0        0"
  },
  {
    "objectID": "posts/2023-03-04-day-13-of-50daysofkaggle/index.html",
    "href": "posts/2023-03-04-day-13-of-50daysofkaggle/index.html",
    "title": "Day 13 of #50daysofKaggle",
    "section": "",
    "text": "Where am I right now?\nSo ‚Ä¶ what is #50daysofKaggle?\nAttempting a kaggle competition has been one of the longest running aspiration for me. Exactly 6 years ago in March ‚Äô17, I created my profile. But I knew I had to start learning to code. So I just kept learning‚Ä¶ and learning‚Ä¶ and learning‚Ä¶ without any real application. As with any new project, I think I may have spent way too much time in planning and worrying about stuff instead of actually ‚Ä¶ doing stuff!\nMany false starts later, it was in the winter of 2022 I had to take that hard decision.\n#50daysofkaggle was my own personal challenge to check-in daily here to ensure that I‚Äôm giving at least 50 consecutive days to my worst fears. My first kaggle post was on 7th Oct.¬†Its roughly 150 days since then. Here are my thoughts so far:\n\nThis blogging is super helpful in not only reviewing code but also for revising theory thru ISLR labs. Offline I‚Äôm maintaining my own notes but this blog is all about application and it has given me more perspective than I intended at the start. Totally the right thing I should be investing my time into.\nCode-switching between R & Python is super helpful to learning from best of both worlds. I may have effectively eliminated the ‚Äúfear of coding‚Äù that held me back forever now.\nthese posts are taking longer to write than I thought. Seriously‚Ä¶ its been around 13 posts in the last 26 weeks. So that‚Äôs 1 post every two weeks. gulp! Reasons are obvious\n\nlearning to code is one of the biggest things slowing me down\nbusy with job-hunt\nholiday season in Dec‚Äô22 (was away from keyboard for almost an entire month)\n\nTill now the numbering of the posts & the days are largely immaterial. It should broadly be read as ‚Äúpost #13 out of 50 that I committed myself‚Äù. I keep skipping the count of days because each post actually takes me 2-3 days to finish. Going forward, I‚Äôm going change it to imply ‚Äúblog post number‚Äù instead of ‚Äúday number‚Äù. And this one definitely counts because its really important to what I want to keep doing\n\n\n\nWhere do I want to go?\nNow that I‚Äôve got that wee bit experiential learning, 7 posts have been made about my kaggle journey, so now I‚Äôm aiming for 43 more. I‚Äôm listing down a bunch of things that I want to achieve through #50daysofkaggle. topics that I definitely want to cover in order of priority:\n\nkeep the focus on Marketing Analytics & Marketing datasets (Hands-on DS for Marketing, Hwang).\nShiny App in R\nPlotly/ Dash App in Python\nNeed to highlight usecases to share with professional network\nsci-kit learn in python\nDeep learning algorithms\nusing coding for applying quant research (Marketing Research 7e, Malhotra/Dash)\nTidyTuesday participation?\ntime-series approach for content consumption data\n\n\n\nHow long will it take?\nNot anytime soon. Even if I make a post every 2 days (without holidays) starting today I‚Äôll be done by 31st May. At 3 days for each post it will 13th July. Incidentally, by August my financial runway is going to end. I‚Äôll at least have a blog. hurrah!üòÖ\n\n\nHow do I get there?\nThe only real answer is to keep practicing. Persevereance is key so the deadline makes it more important.\n\nspeeden up the EDA, feature engineering by picking datasets that comes from my domain experience. in other words, stick to strengths and don‚Äôt jump into areas I‚Äôve got no idea about.\ngetting to XGBoost was one of my goals with revising DT & Resampling methods\nEfficiency in model building should be over-ridden with explaining the business impact. Need to pull myself away to the 20,000 feet view for each dataset.\nUse communities like LetsCodeTogether on WA, KaggleNoobs+R4DS on Slack and Rstats on Twitter\nTarget 1 or 2 important real-world applications that will be worth showcasing at a job interview.\n\nOther issues I‚Äôll have to address:\n\nConsulting assignments that are coming up\nJob hunts (as of today, there‚Äôs only one process in hand)\n\nAnyways, that‚Äôs enough for today.\nNow my goal is in sight. I must proceed! üèÉ‚Äç‚ôÇÔ∏è"
  },
  {
    "objectID": "posts/2023-02-17-migrating-to-quarto/index.html",
    "href": "posts/2023-02-17-migrating-to-quarto/index.html",
    "title": "Migrating to Quarto",
    "section": "",
    "text": "The blog now takes a new look. Finally bought myself a ticket on Quarto hype-train and must say it seems quite interesting. Hopefully there‚Äôll be a separate blog with the nuts and bolts details about how I got this far. For now let me attempt to address why I did this.\nPython code using blogdown/Hugo was not performing consistently. I‚Äôve not been able to duplicate the errors, but they would happen once in a while. Visualisations not rendering in HTML, random crashes and occasional kernel errors.\nTo be fair, the issue could not have been due to just the blogdown framework. Part of my research led me to read up about Quarto that was introduced in early 2022. At that time, I figured with my non-existent tech skills, I‚Äôd be better off focussing on what I started with.\nBy 13th Feb, I had pretty much exhausted all my interview processes. So while I was just sitting tight waiting for that one company to revert, I figured this Quarto migration might be a good project to warm-up towards blogging. That turned out smoother than I imagined.\n\n\nVery early thoughts about what I‚Äôm seeing:\n\nLooks far more neater\nno glitches as far as code is concerned\nDeploying and customisation is super smooth. Documentation has it all covered.\nLoving the new file structure which makes it so much more easier to keep track of changes\nUser comments now appear on the right hand side instead of page bottom\nstill have to get used to the inline CSS/ HTML commands. This was frustrating in the beginning.\n\n\n\n\nCouple of tracks that I‚Äôd like to double down on:\n\nSolving exercises in Introduction to Statistical Learning in R v2 (PDF)\nSolving exercises in ‚ÄúHands-on Data Science for Marketing‚Äù, Packt (Hwang) (Kindle edition)\nMarketing Analytics projects using R, Python\n\nMarket Mix Modelling using Kaggle\nbuilding plotly and shiny dashboards\n\nrevise and blog about Quant research concepts from ‚ÄúMarketing Research‚Äù, Pearson 7e (Malhotra/Dash)\nTarget for daily check-ins all thru Feb\nIf above is completed, then proceed to ‚ÄúPython ML: ML & Deep Learning with Python, sci-kit and TensorFlow 2‚Äù, Packt (Raschka/Mirjalili) (PDF)\nResearch & academic articles in the field of digital marketing/ marketing analytics??ü§î\nExploring discussions on using NLP in the field of qualitative research\n\nAs always, please share feedback and inputs using comments tab on the right üëâüèº"
  },
  {
    "objectID": "posts/2023-02-17-migrating-to-quarto/index.html#initial-thoughts",
    "href": "posts/2023-02-17-migrating-to-quarto/index.html#initial-thoughts",
    "title": "Migrating to Quarto",
    "section": "",
    "text": "Very early thoughts about what I‚Äôm seeing:\n\nLooks far more neater\nno glitches as far as code is concerned\nDeploying and customisation is super smooth. Documentation has it all covered.\nLoving the new file structure which makes it so much more easier to keep track of changes\nUser comments now appear on the right hand side instead of page bottom\nstill have to get used to the inline CSS/ HTML commands. This was frustrating in the beginning."
  },
  {
    "objectID": "posts/2023-02-17-migrating-to-quarto/index.html#next-steps",
    "href": "posts/2023-02-17-migrating-to-quarto/index.html#next-steps",
    "title": "Migrating to Quarto",
    "section": "",
    "text": "Couple of tracks that I‚Äôd like to double down on:\n\nSolving exercises in Introduction to Statistical Learning in R v2 (PDF)\nSolving exercises in ‚ÄúHands-on Data Science for Marketing‚Äù, Packt (Hwang) (Kindle edition)\nMarketing Analytics projects using R, Python\n\nMarket Mix Modelling using Kaggle\nbuilding plotly and shiny dashboards\n\nrevise and blog about Quant research concepts from ‚ÄúMarketing Research‚Äù, Pearson 7e (Malhotra/Dash)\nTarget for daily check-ins all thru Feb\nIf above is completed, then proceed to ‚ÄúPython ML: ML & Deep Learning with Python, sci-kit and TensorFlow 2‚Äù, Packt (Raschka/Mirjalili) (PDF)\nResearch & academic articles in the field of digital marketing/ marketing analytics??ü§î\nExploring discussions on using NLP in the field of qualitative research\n\nAs always, please share feedback and inputs using comments tab on the right üëâüèº"
  },
  {
    "objectID": "posts/2022-10-16-day-10-of-50daysofkaggle/index.en.html",
    "href": "posts/2022-10-16-day-10-of-50daysofkaggle/index.en.html",
    "title": "Day 10 of #50daysofkaggle",
    "section": "",
    "text": "Part of an ongoing series to familiarise working on kaggle\nProgress till date:\n\nDownload titanic dataset and assign to train & test\nRearranging the data\nEDA (including plots and finding survival rate using .groupby())\nModelling\nData preparation - one-hot encoding the Sex, Pclass & Embarked columns - appending these to the numerical columns - normalising the data - splitting between train into X_train, y_train, X_test, y_test\nApplying KNN algo\n\nfinding the right K based on accuracy. (best at K = 7)\nCalculating the accuracy based on test\n\nApplying Decision Trees algo\n\nwith criterion = entropy and max_depth = 3\nsligthly better accuracy in prediction than KNN\n\n\nTo do today: - classification using Support Vector Machines algo\n\n\nReading and printing the top 5 rows\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport zipfile\n\n#importing the zipfile already saved in the other folder. \nzf = zipfile.ZipFile(\"../2022-10-12-day-6-of-50daysofkaggle/titanic.zip\")\ntrain = pd.read_csv(zf.open(\"train.csv\"))\ntest = pd.read_csv(zf.open(\"test.csv\"))\n\n#Selecting only the numerical columns\nnum_col = train.select_dtypes(include=np.number).columns.tolist()\n\n#deslecting passenger ID and 'Survived' \ndel num_col[0:2] #.remove() can remove only 1 item. so for more than 1, use for loop \nselect_col = num_col\n\n#remaining columns\nstr_col= [\"Sex\", \"Embarked\", \"Survived\"]\n#Adding more elements into a list using `extend` and not `append`\nselect_col.extend(str_col)\n\ntrain_eda= train[train.columns.intersection(select_col)]\ntrain_eda\n\n\n\n\n\n\n\n\n\nSurvived\nPclass\nSex\nAge\nSibSp\nParch\nFare\nEmbarked\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\nS\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nC\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\nS\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\nS\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500\nS\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n0\n2\nmale\n27.0\n0\n0\n13.0000\nS\n\n\n887\n1\n1\nfemale\n19.0\n0\n0\n30.0000\nS\n\n\n888\n0\n3\nfemale\nNaN\n1\n2\n23.4500\nS\n\n\n889\n1\n1\nmale\n26.0\n0\n0\n30.0000\nC\n\n\n890\n0\n3\nmale\n32.0\n0\n0\n7.7500\nQ\n\n\n\n\n891 rows √ó 8 columns\n\n\n\n\n\n\nChecking all na values in the existing dataset.\n\n\nCode\ntrain_eda.isna().sum().sort_values()\n\n\nSurvived      0\nPclass        0\nSex           0\nSibSp         0\nParch         0\nFare          0\nEmbarked      2\nAge         177\ndtype: int64\n\n\nReplacing empty cells with median age (28)\n\n\nCode\nmedian_age = train_eda.Age.median() #28\ntrain_eda.loc[train_eda.Age.isna(), \"Age\"] = median_age #.loc returns the view and doesn't throw warning msg\ntrain_eda.isna().sum().sort_values()\n\n\nSurvived    0\nPclass      0\nSex         0\nAge         0\nSibSp       0\nParch       0\nFare        0\nEmbarked    2\ndtype: int64\n\n\n\n\n\nSeperating X & y\n\n\nCode\ntrain_eda = train_eda.dropna(axis = 0) #removing all rows with NA\n\nX = train_eda[[\"Age\", \"SibSp\", \"Parch\", \"Fare\"]]\nX = pd.concat([X,pd.get_dummies(data = train_eda[[\"Sex\", \"Embarked\", \"Pclass\"]], columns = [\"Sex\", \"Embarked\", \"Pclass\"])], axis = 1)\n\ny = train_eda[\"Survived\"].values\n\n\n\n\nTransform X and printing the first 5 datapoints\n\n\nCode\nfrom sklearn import preprocessing\n\nX= preprocessing.StandardScaler().fit(X).transform(X)\nX[0:5]\n\n\narray([[-0.56367407,  0.43135024, -0.47432585, -0.50023975, -0.73534203,\n         0.73534203, -0.48271079, -0.30794088,  0.61679395, -0.56306042,\n        -0.51087465,  0.90032807],\n       [ 0.66921696,  0.43135024, -0.47432585,  0.78894661,  1.35991138,\n        -1.35991138,  2.07163382, -0.30794088, -1.62128697,  1.77600834,\n        -0.51087465, -1.11070624],\n       [-0.25545131, -0.47519908, -0.47432585, -0.48664993,  1.35991138,\n        -1.35991138, -0.48271079, -0.30794088,  0.61679395, -0.56306042,\n        -0.51087465,  0.90032807],\n       [ 0.43804989,  0.43135024, -0.47432585,  0.42286111,  1.35991138,\n        -1.35991138, -0.48271079, -0.30794088,  0.61679395,  1.77600834,\n        -0.51087465, -1.11070624],\n       [ 0.43804989, -0.47519908, -0.47432585, -0.4841333 , -0.73534203,\n         0.73534203, -0.48271079, -0.30794088,  0.61679395, -0.56306042,\n        -0.51087465,  0.90032807]])\n\n\n\n\n\nSplitting into test & train data and comparing the dimensions.\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\nprint ('Train set\\t :', X_train.shape,  y_train.shape,\n'\\nTest set\\t :', X_test.shape,  y_test.shape)\n\n\nTrain set    : (711, 12) (711,) \nTest set     : (178, 12) (178,)\n\n\n\n\n\n\nLets check the classification results using SVM. First 10 are as follows:\n\n\nCode\nfrom sklearn import svm\nclf = svm.SVC(kernel='rbf')\nclf.fit(X_train, y_train) \n\nyhat_svm = clf.predict(X_test)\n\nprint(\"First 10 actual\\t\\t:\", y_test[0:10],\"\\nFirst 10 predicted\\t:\", yhat_svm[0:10])\n\n\nFirst 10 actual     : [1 1 0 1 1 1 0 0 0 0] \nFirst 10 predicted  : [0 1 0 1 1 0 0 0 0 0]\n\n\n\n\n\n\nCode\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport itertools\n\n# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_test, yhat_svm)\nnp.set_printoptions(precision=2)\n\nprint (classification_report(y_test, yhat_svm))\n\n\n              precision    recall  f1-score   support\n\n           0       0.78      0.95      0.85       105\n           1       0.90      0.60      0.72        73\n\n    accuracy                           0.81       178\n   macro avg       0.84      0.78      0.79       178\nweighted avg       0.83      0.81      0.80       178\n\n\n\n\n\n\n\n\nCode\nfrom sklearn import metrics\n\nprint(\"SVM Accuracy\\t:\", metrics.accuracy_score(y_test, yhat_svm),\"\\nRMSE\\t\\t\\t:\", metrics.mean_squared_error(y_test,yhat_svm),\"\\nNormalised RMSE\\t:\", metrics.mean_squared_error(y_test,yhat_svm)/np.std(y_test))\n\n\nSVM Accuracy    : 0.8089887640449438 \nRMSE            : 0.19101123595505617 \nNormalised RMSE : 0.38834957789472496\n\n\nAchieved 81% accuracy using SVM with RMSE of 0.1911. This is is not as good as Decision Trees which resulted in RMSE of 0.168\nTherefore after 10 days of struggle, I have come to the conclusion that Decision Trees is a good classification algorithm for the Titanic dataset."
  },
  {
    "objectID": "posts/2022-10-16-day-10-of-50daysofkaggle/index.en.html#reading-the-data",
    "href": "posts/2022-10-16-day-10-of-50daysofkaggle/index.en.html#reading-the-data",
    "title": "Day 10 of #50daysofkaggle",
    "section": "",
    "text": "Reading and printing the top 5 rows\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport zipfile\n\n#importing the zipfile already saved in the other folder. \nzf = zipfile.ZipFile(\"../2022-10-12-day-6-of-50daysofkaggle/titanic.zip\")\ntrain = pd.read_csv(zf.open(\"train.csv\"))\ntest = pd.read_csv(zf.open(\"test.csv\"))\n\n#Selecting only the numerical columns\nnum_col = train.select_dtypes(include=np.number).columns.tolist()\n\n#deslecting passenger ID and 'Survived' \ndel num_col[0:2] #.remove() can remove only 1 item. so for more than 1, use for loop \nselect_col = num_col\n\n#remaining columns\nstr_col= [\"Sex\", \"Embarked\", \"Survived\"]\n#Adding more elements into a list using `extend` and not `append`\nselect_col.extend(str_col)\n\ntrain_eda= train[train.columns.intersection(select_col)]\ntrain_eda\n\n\n\n\n\n\n\n\n\nSurvived\nPclass\nSex\nAge\nSibSp\nParch\nFare\nEmbarked\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\nS\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nC\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\nS\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\nS\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500\nS\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n0\n2\nmale\n27.0\n0\n0\n13.0000\nS\n\n\n887\n1\n1\nfemale\n19.0\n0\n0\n30.0000\nS\n\n\n888\n0\n3\nfemale\nNaN\n1\n2\n23.4500\nS\n\n\n889\n1\n1\nmale\n26.0\n0\n0\n30.0000\nC\n\n\n890\n0\n3\nmale\n32.0\n0\n0\n7.7500\nQ\n\n\n\n\n891 rows √ó 8 columns"
  },
  {
    "objectID": "posts/2022-10-16-day-10-of-50daysofkaggle/index.en.html#cleaning-up-the-data",
    "href": "posts/2022-10-16-day-10-of-50daysofkaggle/index.en.html#cleaning-up-the-data",
    "title": "Day 10 of #50daysofkaggle",
    "section": "",
    "text": "Checking all na values in the existing dataset.\n\n\nCode\ntrain_eda.isna().sum().sort_values()\n\n\nSurvived      0\nPclass        0\nSex           0\nSibSp         0\nParch         0\nFare          0\nEmbarked      2\nAge         177\ndtype: int64\n\n\nReplacing empty cells with median age (28)\n\n\nCode\nmedian_age = train_eda.Age.median() #28\ntrain_eda.loc[train_eda.Age.isna(), \"Age\"] = median_age #.loc returns the view and doesn't throw warning msg\ntrain_eda.isna().sum().sort_values()\n\n\nSurvived    0\nPclass      0\nSex         0\nAge         0\nSibSp       0\nParch       0\nFare        0\nEmbarked    2\ndtype: int64"
  },
  {
    "objectID": "posts/2022-10-16-day-10-of-50daysofkaggle/index.en.html#model-building",
    "href": "posts/2022-10-16-day-10-of-50daysofkaggle/index.en.html#model-building",
    "title": "Day 10 of #50daysofkaggle",
    "section": "",
    "text": "Seperating X & y\n\n\nCode\ntrain_eda = train_eda.dropna(axis = 0) #removing all rows with NA\n\nX = train_eda[[\"Age\", \"SibSp\", \"Parch\", \"Fare\"]]\nX = pd.concat([X,pd.get_dummies(data = train_eda[[\"Sex\", \"Embarked\", \"Pclass\"]], columns = [\"Sex\", \"Embarked\", \"Pclass\"])], axis = 1)\n\ny = train_eda[\"Survived\"].values\n\n\n\n\nTransform X and printing the first 5 datapoints\n\n\nCode\nfrom sklearn import preprocessing\n\nX= preprocessing.StandardScaler().fit(X).transform(X)\nX[0:5]\n\n\narray([[-0.56367407,  0.43135024, -0.47432585, -0.50023975, -0.73534203,\n         0.73534203, -0.48271079, -0.30794088,  0.61679395, -0.56306042,\n        -0.51087465,  0.90032807],\n       [ 0.66921696,  0.43135024, -0.47432585,  0.78894661,  1.35991138,\n        -1.35991138,  2.07163382, -0.30794088, -1.62128697,  1.77600834,\n        -0.51087465, -1.11070624],\n       [-0.25545131, -0.47519908, -0.47432585, -0.48664993,  1.35991138,\n        -1.35991138, -0.48271079, -0.30794088,  0.61679395, -0.56306042,\n        -0.51087465,  0.90032807],\n       [ 0.43804989,  0.43135024, -0.47432585,  0.42286111,  1.35991138,\n        -1.35991138, -0.48271079, -0.30794088,  0.61679395,  1.77600834,\n        -0.51087465, -1.11070624],\n       [ 0.43804989, -0.47519908, -0.47432585, -0.4841333 , -0.73534203,\n         0.73534203, -0.48271079, -0.30794088,  0.61679395, -0.56306042,\n        -0.51087465,  0.90032807]])\n\n\n\n\n\nSplitting into test & train data and comparing the dimensions.\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\nprint ('Train set\\t :', X_train.shape,  y_train.shape,\n'\\nTest set\\t :', X_test.shape,  y_test.shape)\n\n\nTrain set    : (711, 12) (711,) \nTest set     : (178, 12) (178,)"
  },
  {
    "objectID": "posts/2022-10-16-day-10-of-50daysofkaggle/index.en.html#support-vector-machines",
    "href": "posts/2022-10-16-day-10-of-50daysofkaggle/index.en.html#support-vector-machines",
    "title": "Day 10 of #50daysofkaggle",
    "section": "",
    "text": "Lets check the classification results using SVM. First 10 are as follows:\n\n\nCode\nfrom sklearn import svm\nclf = svm.SVC(kernel='rbf')\nclf.fit(X_train, y_train) \n\nyhat_svm = clf.predict(X_test)\n\nprint(\"First 10 actual\\t\\t:\", y_test[0:10],\"\\nFirst 10 predicted\\t:\", yhat_svm[0:10])\n\n\nFirst 10 actual     : [1 1 0 1 1 1 0 0 0 0] \nFirst 10 predicted  : [0 1 0 1 1 0 0 0 0 0]\n\n\n\n\n\n\nCode\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport itertools\n\n# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_test, yhat_svm)\nnp.set_printoptions(precision=2)\n\nprint (classification_report(y_test, yhat_svm))\n\n\n              precision    recall  f1-score   support\n\n           0       0.78      0.95      0.85       105\n           1       0.90      0.60      0.72        73\n\n    accuracy                           0.81       178\n   macro avg       0.84      0.78      0.79       178\nweighted avg       0.83      0.81      0.80       178\n\n\n\n\n\n\n\n\nCode\nfrom sklearn import metrics\n\nprint(\"SVM Accuracy\\t:\", metrics.accuracy_score(y_test, yhat_svm),\"\\nRMSE\\t\\t\\t:\", metrics.mean_squared_error(y_test,yhat_svm),\"\\nNormalised RMSE\\t:\", metrics.mean_squared_error(y_test,yhat_svm)/np.std(y_test))\n\n\nSVM Accuracy    : 0.8089887640449438 \nRMSE            : 0.19101123595505617 \nNormalised RMSE : 0.38834957789472496\n\n\nAchieved 81% accuracy using SVM with RMSE of 0.1911. This is is not as good as Decision Trees which resulted in RMSE of 0.168\nTherefore after 10 days of struggle, I have come to the conclusion that Decision Trees is a good classification algorithm for the Titanic dataset."
  },
  {
    "objectID": "posts/2024-11-04-intangiblebenefits/index.html",
    "href": "posts/2024-11-04-intangiblebenefits/index.html",
    "title": "Consumers Purchase Emotionally and Defend Rationally",
    "section": "",
    "text": "Once in a while, you encounter a nugget of wisdom that immediately puts in place a lot of experiential knowledge. While reading Chris Goward‚Äôs awesome book ‚ÄúYou Should Test That‚Äù I was struck by the this passage.\n\nWhat struck me the most were the words - ‚Äúdefends ‚Ä¶ rationally‚Äù. Why is the customer defending anything? Isn‚Äôt the purchase made out of pure free-will? Did anyone coerce the customer to make the purchase?\nThe answer lies in a Harvard Business School professor‚Äôs Gerald Zaltman‚Äôs book ‚ÄúHow Customer‚Äôs Think‚Äù. Almost 95% of the the time, customers have decided subconciously that they need to buy a product. This is relayed to the concious mind as an emotion. The concious mind now searches for rational reasons therefore completing this loop and allowing the customer to feel safe and secure.\nTherefore when the customer has to re-visit their purchase action either by way of speaking about it to others or justify it themselves, it is these rational markers that recalled. However, the genesis has almost always been an emotional one.\nHow does all this matter for conversion rate optimisation? For a marketeer, the key takeaway is to lace the purchase journey with emotional as well as rational markers. Begin first by identifying the tangible and intangible reasons for purchase.\n\n\nSimply put, what is the brand‚Äôs Reasons To Buy that sets it apart from competition? List down all the features of the product/service that can be used to describe it.\nThe second tangible reason are incentives and offers. Important to highlight that along with ‚ÄúAudience Targetting‚Äù and ‚ÄúCreative‚Äù, the ‚ÄúOffer‚Äù stands out as a major lever in digital marketing campaigns. Price offers are hooks that can get customers into the funnel but one should test for the best combination not impact revenues. Some of the incentives and offers include:\n\nPremiums\nDiscounts\nCredits\nBuy One, Get one\nBundling\nFree Trial\nFree Upgrade\nExclusive offer\nFestive offer\nLoyalty programs etc\n\n\n\n\nThere are several conversion tactics to increase trust:\n\nProfessional reviews: screenshots of established trade figures with positive brand mentions\nMedia Mentions: Nothing like some old-school media mentions to add credibility\nAwards & honors\nThought Leadership: displaying articles, white papers, books, blogs and other content marekting assets is a good way to build positive perception\nCase Studies: previous wins and client reports describing how the service/ product has solved other customer problems\nSocial proof: showing number of customers especially if its big enough adds trust\nTestimonials: age-old device to influence new customers. Showing consensus opinions of other people around them.\nClient logos\nCustomer ratings and reviews\nSocial media: this one is buried at the end but isn‚Äôt it the most obvious one !\n\nWhich one of these resonated with you and which didn‚Äôt? Let us talk more the comments on the right‚û°Ô∏è"
  },
  {
    "objectID": "posts/2024-11-04-intangiblebenefits/index.html#tangible-reasons",
    "href": "posts/2024-11-04-intangiblebenefits/index.html#tangible-reasons",
    "title": "Consumers Purchase Emotionally and Defend Rationally",
    "section": "",
    "text": "Simply put, what is the brand‚Äôs Reasons To Buy that sets it apart from competition? List down all the features of the product/service that can be used to describe it.\nThe second tangible reason are incentives and offers. Important to highlight that along with ‚ÄúAudience Targetting‚Äù and ‚ÄúCreative‚Äù, the ‚ÄúOffer‚Äù stands out as a major lever in digital marketing campaigns. Price offers are hooks that can get customers into the funnel but one should test for the best combination not impact revenues. Some of the incentives and offers include:\n\nPremiums\nDiscounts\nCredits\nBuy One, Get one\nBundling\nFree Trial\nFree Upgrade\nExclusive offer\nFestive offer\nLoyalty programs etc"
  },
  {
    "objectID": "posts/2024-11-04-intangiblebenefits/index.html#intangible-reasons",
    "href": "posts/2024-11-04-intangiblebenefits/index.html#intangible-reasons",
    "title": "Consumers Purchase Emotionally and Defend Rationally",
    "section": "",
    "text": "There are several conversion tactics to increase trust:\n\nProfessional reviews: screenshots of established trade figures with positive brand mentions\nMedia Mentions: Nothing like some old-school media mentions to add credibility\nAwards & honors\nThought Leadership: displaying articles, white papers, books, blogs and other content marekting assets is a good way to build positive perception\nCase Studies: previous wins and client reports describing how the service/ product has solved other customer problems\nSocial proof: showing number of customers especially if its big enough adds trust\nTestimonials: age-old device to influence new customers. Showing consensus opinions of other people around them.\nClient logos\nCustomer ratings and reviews\nSocial media: this one is buried at the end but isn‚Äôt it the most obvious one !\n\nWhich one of these resonated with you and which didn‚Äôt? Let us talk more the comments on the right‚û°Ô∏è"
  },
  {
    "objectID": "posts/2022-10-12-day-6-of-50daysofkaggle/index.en.html",
    "href": "posts/2022-10-12-day-6-of-50daysofkaggle/index.en.html",
    "title": "Day 6 of #50daysofkaggle",
    "section": "",
    "text": "Progress till date:\n\nDownload titanic dataset and assign to train & test\nRearranging the data\nEDA\n\nTo do today:\n\nwrite function to find share of survivors by each variable\nattempt to create model\n\n\n\nLoading the data using kaggle library and examining the top rows of relevant columns.\n\n\nCode\nimport requests\nimport numpy as np\nimport pandas as pd\nimport kaggle \nimport zipfile \n\nkaggle.api.authenticate()\n\nkaggle.api.competition_download_files(\"titanic\", path = \".\")\n\nzf = zipfile.ZipFile(\"titanic.zip\")\ntrain = pd.read_csv(zf.open(\"train.csv\"))\ntest = pd.read_csv(zf.open(\"test.csv\"))\n\n#Selecting only the numerical columns\nnum_col = train.select_dtypes(include=np.number).columns.tolist()\n\n#deslecting passenger ID and 'Survived' \ndel num_col[0:2] #.remove() can remove only 1 item. so for more than 1, use for loop \nselect_col = num_col\n\n#remaining columns\nstr_col= [\"Sex\", \"Embarked\", \"Survived\"]\n\n#Adding more elements into a list using `extend` and not `append`\nselect_col.extend(str_col)\n\ntrain_eda= train[train.columns.intersection(select_col)]\ntrain_eda.head()\n\n\n\n\n\n\n\n\n\nSurvived\nPclass\nSex\nAge\nSibSp\nParch\nFare\nEmbarked\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\nS\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nC\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\nS\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\nS\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500\nS\n\n\n\n\n\n\n\nHow many columns with na values?\n\n\nCode\ntrain_eda.isna().sum().sort_values()\n\n\nSurvived      0\nPclass        0\nSex           0\nSibSp         0\nParch         0\nFare          0\nEmbarked      2\nAge         177\ndtype: int64\n\n\nAlmost 177 entries in the Age column have no value. Calculating the median age of remaining data.\n\n\nCode\ntrain_eda[\"Age\"].median() #28\n\n\n28.0\n\n\nReplacing these with the median age (28) instead of removing them.\n\n\nCode\ntrain_eda[\"Age\"].fillna(value = train_eda[\"Age\"].median(), inplace = True)\ntrain_eda.isna().sum().sort_values()\n\n\nC:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_5312\\1076914416.py:1: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\nSurvived    0\nPclass      0\nSex         0\nAge         0\nSibSp       0\nParch       0\nFare        0\nEmbarked    2\ndtype: int64\n\n\nToday I want to calculate the survival rate of each of these attributes (Pclass, Sex, Embarked).\n\n\nCode\ndf_copy2 = pd.DataFrame(columns = {\"category\", \"col\", \"survive_rate\"})\n\nfor t in [\"Pclass\", \"Sex\", \"Embarked\"]:\n  df_copy = train_eda.groupby([t])[\"Survived\"].mean().reset_index()\n  df_copy[\"category\"] = t\n  #trying to create a `tidy` version of the data \n  df_copy.rename(columns = {t: \"col\", \"Survived\": \"survive_rate\"}, errors = \"raise\", inplace = True)\n  df_copy = df_copy[[\"category\", \"col\", \"survive_rate\"]]\n  df_copy2= pd.concat([df_copy2, df_copy], ignore_index = True)\n\n\n#final table in a tidy format that can be used to create graphs. but that i'm keeping for later\ndf_copy2[[\"category\", \"col\", \"survive_rate\"]]\n\n\n\n\n\n\n\n\n\ncategory\ncol\nsurvive_rate\n\n\n\n\n0\nPclass\n1\n0.62963\n\n\n1\nPclass\n2\n0.472826\n\n\n2\nPclass\n3\n0.242363\n\n\n3\nSex\nfemale\n0.742038\n\n\n4\nSex\nmale\n0.188908\n\n\n5\nEmbarked\nC\n0.553571\n\n\n6\nEmbarked\nQ\n0.38961\n\n\n7\nEmbarked\nS\n0.336957\n\n\n\n\n\n\n\nWith this, its pretty clear that among the sex category, males had the least likelihood of surviving with 19%. The richer class 1 managed a 63% chance of survival while only 24% of the lower class 3 survived. Finally those that embarked from Cherbourg had a higher survival rate 55% compared to Southampton at 34%.\n\n\n\nSeperating the X & y. Here‚Äôs the first 5 rows of X\n\n\nCode\ntrain_eda.isna().sum().sort_values()\ntrain_eda = train_eda.dropna(axis = 0) #removing all rows with NA\n\nX = train_eda[[\"Age\", \"SibSp\", \"Parch\", \"Fare\"]]\n\nX = pd.concat([X,pd.get_dummies(data = train_eda[[\"Sex\", \"Embarked\", \"Pclass\"]], columns = [\"Sex\", \"Embarked\", \"Pclass\"])], axis = 1)\n\nX.head()\n\n\n\n\n\n\n\n\n\nAge\nSibSp\nParch\nFare\nSex_female\nSex_male\nEmbarked_C\nEmbarked_Q\nEmbarked_S\nPclass_1\nPclass_2\nPclass_3\n\n\n\n\n0\n22.0\n1\n0\n7.2500\n0\n1\n0\n0\n1\n0\n0\n1\n\n\n1\n38.0\n1\n0\n71.2833\n1\n0\n1\n0\n0\n1\n0\n0\n\n\n2\n26.0\n0\n0\n7.9250\n1\n0\n0\n0\n1\n0\n0\n1\n\n\n3\n35.0\n1\n0\n53.1000\n1\n0\n0\n0\n1\n1\n0\n0\n\n\n4\n35.0\n0\n0\n8.0500\n0\n1\n0\n0\n1\n0\n0\n1\n\n\n\n\n\n\n\nFirst 5 rows of y\n\n\nCode\ny = train_eda[\"Survived\"].values\ny[0:5]\n\n\narray([0, 1, 1, 1, 0], dtype=int64)\n\n\nChecking dimensions of y & X\n\n\nCode\nlen(y) #889 after filling up the NA. previously 712\nX.shape #(889, 12)\n\n\n(889, 12)\n\n\n\n\nTransform X and printing the first 5 datapoints\n\n\nCode\nfrom sklearn import preprocessing\n\nX= preprocessing.StandardScaler().fit(X).transform(X)\nX[0:5]\n\n\narray([[-0.56367407,  0.43135024, -0.47432585, -0.50023975, -0.73534203,\n         0.73534203, -0.48271079, -0.30794088,  0.61679395, -0.56306042,\n        -0.51087465,  0.90032807],\n       [ 0.66921696,  0.43135024, -0.47432585,  0.78894661,  1.35991138,\n        -1.35991138,  2.07163382, -0.30794088, -1.62128697,  1.77600834,\n        -0.51087465, -1.11070624],\n       [-0.25545131, -0.47519908, -0.47432585, -0.48664993,  1.35991138,\n        -1.35991138, -0.48271079, -0.30794088,  0.61679395, -0.56306042,\n        -0.51087465,  0.90032807],\n       [ 0.43804989,  0.43135024, -0.47432585,  0.42286111,  1.35991138,\n        -1.35991138, -0.48271079, -0.30794088,  0.61679395,  1.77600834,\n        -0.51087465, -1.11070624],\n       [ 0.43804989, -0.47519908, -0.47432585, -0.4841333 , -0.73534203,\n         0.73534203, -0.48271079, -0.30794088,  0.61679395, -0.56306042,\n        -0.51087465,  0.90032807]])\n\n\n\n\n\nSplitting into test & train data and comparing the dimensions.\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\nprint ('Train set\\t :', X_train.shape,  y_train.shape,\n'\\nTest set\\t :', X_test.shape,  y_test.shape)\n\n\nTrain set    : (711, 12) (711,) \nTest set     : (178, 12) (178,)\n\n\n\n\n\nUsing KNN at k = 4\n\n\nCode\nfrom sklearn.neighbors import KNeighborsClassifier\nk = 4\nneighbours = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)\nneighbours\n\n\nKNeighborsClassifier(n_neighbors=4)\n\n\n\n\n\n\nCode\nyhat1 = neighbours.predict(X_test)\nyhat1[0:5]\n\n\narray([0, 1, 0, 0, 1], dtype=int64)\n\n\nCalculating the accuracy at k = 4\n\n\nCode\nfrom sklearn import metrics\n\nprint(\"Train set Accuracy \\t:\", metrics.accuracy_score(y_train, neighbours.predict(X_train)), \"\\nTest set Accuracy \\t:\", metrics.accuracy_score(y_test, yhat1))\n\n\nTrain set Accuracy  : 0.8509142053445851 \nTest set Accuracy   : 0.7584269662921348\n\n\n(without replacing na values, the previous test accuracy was 78%)\n\n\n\n\n\nCode\nfrom sklearn import metrics\n\nKs = 10\nmean_acc = np.zeros((Ks-1))\nstd_acc = np.zeros((Ks-1))\n\nfor n in range(1,Ks):\n    #Train Model and Predict  \n    neigh = KNeighborsClassifier(n_neighbors = n).fit(X_train,y_train)\n    yhat=neigh.predict(X_test)\n    mean_acc[n-1] = metrics.accuracy_score(y_test, yhat)\n    std_acc[n-1]=np.std(yhat==y_test)/np.sqrt(yhat.shape[0])\n\nmean_acc\n\n\narray([0.78651685, 0.76404494, 0.7752809 , 0.75842697, 0.78089888,\n       0.78651685, 0.80337079, 0.7752809 , 0.78089888])\n\n\nGlad that IBM coursera assignments came in handy! Now visualising the accuracy across each K\n\n\nCode\nimport matplotlib.pyplot as plt\n\nplt.plot(range(1,Ks),mean_acc,'g')\nplt.fill_between(range(1,Ks),mean_acc - 1 * std_acc,mean_acc + 1 * std_acc, alpha=0.10)\nplt.fill_between(range(1,Ks),mean_acc - 3 * std_acc,mean_acc + 3 * std_acc, alpha=0.10,color=\"green\")\nplt.legend(('Accuracy ', '+/- 1xstd','+/- 3xstd'))\nplt.ylabel('Accuracy ')\nplt.xlabel('Number of Neighbors (K)')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nLooks like accuracy of KNN is best at 7 neighbours. previously without replacing NA the accuracy was highest at k = 5\n\n\n\n\n\nCode\nk = 7\n\nneighbours_7 = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)\nyhat = neighbours_7.predict(X_test)\n\nprint(\"Train set Accuracy \\t:\", metrics.accuracy_score(y_train, neighbours_7.predict(X_train)),\"\\nTest set Accuracy \\t:\", metrics.accuracy_score(y_test, yhat),\"\\nRMSE \\t\\t\\t:\",metrics.mean_squared_error(y_test, yhat),\"\\nNormalised RMSE\\t\\t:\",metrics.mean_squared_error(y_test, yhat)/np.std(y_test))\n\n\nTrain set Accuracy  : 0.8509142053445851 \nTest set Accuracy   : 0.8033707865168539 \nRMSE            : 0.19662921348314608 \nNormalised RMSE     : 0.3997716243033934\n\n\nWe find that Test accuracy is around 80% for KNN1 with RMSE of 0.197 and Normalised RMSE of 40%2. formula for NRMSE here"
  },
  {
    "objectID": "posts/2022-10-12-day-6-of-50daysofkaggle/index.en.html#reading-the-data",
    "href": "posts/2022-10-12-day-6-of-50daysofkaggle/index.en.html#reading-the-data",
    "title": "Day 6 of #50daysofkaggle",
    "section": "",
    "text": "Loading the data using kaggle library and examining the top rows of relevant columns.\n\n\nCode\nimport requests\nimport numpy as np\nimport pandas as pd\nimport kaggle \nimport zipfile \n\nkaggle.api.authenticate()\n\nkaggle.api.competition_download_files(\"titanic\", path = \".\")\n\nzf = zipfile.ZipFile(\"titanic.zip\")\ntrain = pd.read_csv(zf.open(\"train.csv\"))\ntest = pd.read_csv(zf.open(\"test.csv\"))\n\n#Selecting only the numerical columns\nnum_col = train.select_dtypes(include=np.number).columns.tolist()\n\n#deslecting passenger ID and 'Survived' \ndel num_col[0:2] #.remove() can remove only 1 item. so for more than 1, use for loop \nselect_col = num_col\n\n#remaining columns\nstr_col= [\"Sex\", \"Embarked\", \"Survived\"]\n\n#Adding more elements into a list using `extend` and not `append`\nselect_col.extend(str_col)\n\ntrain_eda= train[train.columns.intersection(select_col)]\ntrain_eda.head()\n\n\n\n\n\n\n\n\n\nSurvived\nPclass\nSex\nAge\nSibSp\nParch\nFare\nEmbarked\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\nS\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nC\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\nS\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\nS\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500\nS\n\n\n\n\n\n\n\nHow many columns with na values?\n\n\nCode\ntrain_eda.isna().sum().sort_values()\n\n\nSurvived      0\nPclass        0\nSex           0\nSibSp         0\nParch         0\nFare          0\nEmbarked      2\nAge         177\ndtype: int64\n\n\nAlmost 177 entries in the Age column have no value. Calculating the median age of remaining data.\n\n\nCode\ntrain_eda[\"Age\"].median() #28\n\n\n28.0\n\n\nReplacing these with the median age (28) instead of removing them.\n\n\nCode\ntrain_eda[\"Age\"].fillna(value = train_eda[\"Age\"].median(), inplace = True)\ntrain_eda.isna().sum().sort_values()\n\n\nC:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_5312\\1076914416.py:1: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\nSurvived    0\nPclass      0\nSex         0\nAge         0\nSibSp       0\nParch       0\nFare        0\nEmbarked    2\ndtype: int64\n\n\nToday I want to calculate the survival rate of each of these attributes (Pclass, Sex, Embarked).\n\n\nCode\ndf_copy2 = pd.DataFrame(columns = {\"category\", \"col\", \"survive_rate\"})\n\nfor t in [\"Pclass\", \"Sex\", \"Embarked\"]:\n  df_copy = train_eda.groupby([t])[\"Survived\"].mean().reset_index()\n  df_copy[\"category\"] = t\n  #trying to create a `tidy` version of the data \n  df_copy.rename(columns = {t: \"col\", \"Survived\": \"survive_rate\"}, errors = \"raise\", inplace = True)\n  df_copy = df_copy[[\"category\", \"col\", \"survive_rate\"]]\n  df_copy2= pd.concat([df_copy2, df_copy], ignore_index = True)\n\n\n#final table in a tidy format that can be used to create graphs. but that i'm keeping for later\ndf_copy2[[\"category\", \"col\", \"survive_rate\"]]\n\n\n\n\n\n\n\n\n\ncategory\ncol\nsurvive_rate\n\n\n\n\n0\nPclass\n1\n0.62963\n\n\n1\nPclass\n2\n0.472826\n\n\n2\nPclass\n3\n0.242363\n\n\n3\nSex\nfemale\n0.742038\n\n\n4\nSex\nmale\n0.188908\n\n\n5\nEmbarked\nC\n0.553571\n\n\n6\nEmbarked\nQ\n0.38961\n\n\n7\nEmbarked\nS\n0.336957\n\n\n\n\n\n\n\nWith this, its pretty clear that among the sex category, males had the least likelihood of surviving with 19%. The richer class 1 managed a 63% chance of survival while only 24% of the lower class 3 survived. Finally those that embarked from Cherbourg had a higher survival rate 55% compared to Southampton at 34%."
  },
  {
    "objectID": "posts/2022-10-12-day-6-of-50daysofkaggle/index.en.html#model-building",
    "href": "posts/2022-10-12-day-6-of-50daysofkaggle/index.en.html#model-building",
    "title": "Day 6 of #50daysofkaggle",
    "section": "",
    "text": "Seperating the X & y. Here‚Äôs the first 5 rows of X\n\n\nCode\ntrain_eda.isna().sum().sort_values()\ntrain_eda = train_eda.dropna(axis = 0) #removing all rows with NA\n\nX = train_eda[[\"Age\", \"SibSp\", \"Parch\", \"Fare\"]]\n\nX = pd.concat([X,pd.get_dummies(data = train_eda[[\"Sex\", \"Embarked\", \"Pclass\"]], columns = [\"Sex\", \"Embarked\", \"Pclass\"])], axis = 1)\n\nX.head()\n\n\n\n\n\n\n\n\n\nAge\nSibSp\nParch\nFare\nSex_female\nSex_male\nEmbarked_C\nEmbarked_Q\nEmbarked_S\nPclass_1\nPclass_2\nPclass_3\n\n\n\n\n0\n22.0\n1\n0\n7.2500\n0\n1\n0\n0\n1\n0\n0\n1\n\n\n1\n38.0\n1\n0\n71.2833\n1\n0\n1\n0\n0\n1\n0\n0\n\n\n2\n26.0\n0\n0\n7.9250\n1\n0\n0\n0\n1\n0\n0\n1\n\n\n3\n35.0\n1\n0\n53.1000\n1\n0\n0\n0\n1\n1\n0\n0\n\n\n4\n35.0\n0\n0\n8.0500\n0\n1\n0\n0\n1\n0\n0\n1\n\n\n\n\n\n\n\nFirst 5 rows of y\n\n\nCode\ny = train_eda[\"Survived\"].values\ny[0:5]\n\n\narray([0, 1, 1, 1, 0], dtype=int64)\n\n\nChecking dimensions of y & X\n\n\nCode\nlen(y) #889 after filling up the NA. previously 712\nX.shape #(889, 12)\n\n\n(889, 12)\n\n\n\n\nTransform X and printing the first 5 datapoints\n\n\nCode\nfrom sklearn import preprocessing\n\nX= preprocessing.StandardScaler().fit(X).transform(X)\nX[0:5]\n\n\narray([[-0.56367407,  0.43135024, -0.47432585, -0.50023975, -0.73534203,\n         0.73534203, -0.48271079, -0.30794088,  0.61679395, -0.56306042,\n        -0.51087465,  0.90032807],\n       [ 0.66921696,  0.43135024, -0.47432585,  0.78894661,  1.35991138,\n        -1.35991138,  2.07163382, -0.30794088, -1.62128697,  1.77600834,\n        -0.51087465, -1.11070624],\n       [-0.25545131, -0.47519908, -0.47432585, -0.48664993,  1.35991138,\n        -1.35991138, -0.48271079, -0.30794088,  0.61679395, -0.56306042,\n        -0.51087465,  0.90032807],\n       [ 0.43804989,  0.43135024, -0.47432585,  0.42286111,  1.35991138,\n        -1.35991138, -0.48271079, -0.30794088,  0.61679395,  1.77600834,\n        -0.51087465, -1.11070624],\n       [ 0.43804989, -0.47519908, -0.47432585, -0.4841333 , -0.73534203,\n         0.73534203, -0.48271079, -0.30794088,  0.61679395, -0.56306042,\n        -0.51087465,  0.90032807]])\n\n\n\n\n\nSplitting into test & train data and comparing the dimensions.\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\nprint ('Train set\\t :', X_train.shape,  y_train.shape,\n'\\nTest set\\t :', X_test.shape,  y_test.shape)\n\n\nTrain set    : (711, 12) (711,) \nTest set     : (178, 12) (178,)\n\n\n\n\n\nUsing KNN at k = 4\n\n\nCode\nfrom sklearn.neighbors import KNeighborsClassifier\nk = 4\nneighbours = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)\nneighbours\n\n\nKNeighborsClassifier(n_neighbors=4)\n\n\n\n\n\n\nCode\nyhat1 = neighbours.predict(X_test)\nyhat1[0:5]\n\n\narray([0, 1, 0, 0, 1], dtype=int64)\n\n\nCalculating the accuracy at k = 4\n\n\nCode\nfrom sklearn import metrics\n\nprint(\"Train set Accuracy \\t:\", metrics.accuracy_score(y_train, neighbours.predict(X_train)), \"\\nTest set Accuracy \\t:\", metrics.accuracy_score(y_test, yhat1))\n\n\nTrain set Accuracy  : 0.8509142053445851 \nTest set Accuracy   : 0.7584269662921348\n\n\n(without replacing na values, the previous test accuracy was 78%)\n\n\n\n\n\nCode\nfrom sklearn import metrics\n\nKs = 10\nmean_acc = np.zeros((Ks-1))\nstd_acc = np.zeros((Ks-1))\n\nfor n in range(1,Ks):\n    #Train Model and Predict  \n    neigh = KNeighborsClassifier(n_neighbors = n).fit(X_train,y_train)\n    yhat=neigh.predict(X_test)\n    mean_acc[n-1] = metrics.accuracy_score(y_test, yhat)\n    std_acc[n-1]=np.std(yhat==y_test)/np.sqrt(yhat.shape[0])\n\nmean_acc\n\n\narray([0.78651685, 0.76404494, 0.7752809 , 0.75842697, 0.78089888,\n       0.78651685, 0.80337079, 0.7752809 , 0.78089888])\n\n\nGlad that IBM coursera assignments came in handy! Now visualising the accuracy across each K\n\n\nCode\nimport matplotlib.pyplot as plt\n\nplt.plot(range(1,Ks),mean_acc,'g')\nplt.fill_between(range(1,Ks),mean_acc - 1 * std_acc,mean_acc + 1 * std_acc, alpha=0.10)\nplt.fill_between(range(1,Ks),mean_acc - 3 * std_acc,mean_acc + 3 * std_acc, alpha=0.10,color=\"green\")\nplt.legend(('Accuracy ', '+/- 1xstd','+/- 3xstd'))\nplt.ylabel('Accuracy ')\nplt.xlabel('Number of Neighbors (K)')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nLooks like accuracy of KNN is best at 7 neighbours. previously without replacing NA the accuracy was highest at k = 5\n\n\n\n\n\nCode\nk = 7\n\nneighbours_7 = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)\nyhat = neighbours_7.predict(X_test)\n\nprint(\"Train set Accuracy \\t:\", metrics.accuracy_score(y_train, neighbours_7.predict(X_train)),\"\\nTest set Accuracy \\t:\", metrics.accuracy_score(y_test, yhat),\"\\nRMSE \\t\\t\\t:\",metrics.mean_squared_error(y_test, yhat),\"\\nNormalised RMSE\\t\\t:\",metrics.mean_squared_error(y_test, yhat)/np.std(y_test))\n\n\nTrain set Accuracy  : 0.8509142053445851 \nTest set Accuracy   : 0.8033707865168539 \nRMSE            : 0.19662921348314608 \nNormalised RMSE     : 0.3997716243033934\n\n\nWe find that Test accuracy is around 80% for KNN1 with RMSE of 0.197 and Normalised RMSE of 40%2. formula for NRMSE here"
  },
  {
    "objectID": "posts/2022-10-12-day-6-of-50daysofkaggle/index.en.html#footnotes",
    "href": "posts/2022-10-12-day-6-of-50daysofkaggle/index.en.html#footnotes",
    "title": "Day 6 of #50daysofkaggle",
    "section": "Footnotes",
    "text": "Footnotes\n\n\npretty much the same as previous attempt before replacing NA‚Ü©Ô∏é\nactually NRMSE is not needed as all models are of the same scale. This is used typically for model comparisons across log, decimal etc scales‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2023-03-02-ISLRv2-Decision-Trees/index.html",
    "href": "posts/2023-03-02-ISLRv2-Decision-Trees/index.html",
    "title": "ISLR Lab - Decision Trees",
    "section": "",
    "text": "All the code here is derived from the legendary book ISRL 2nd edition‚Äôs chapter 8 ‚ÄúDecision Trees‚Äù. Its sometimes a wonder how elegant the base R language can be. The ISRL lab rarely mentions tidyverse syntax but yet manages to make the code so easy to read. The more you learn!ü§ì"
  },
  {
    "objectID": "posts/2023-03-02-ISLRv2-Decision-Trees/index.html#splitting-and-fitting-the-model",
    "href": "posts/2023-03-02-ISLRv2-Decision-Trees/index.html#splitting-and-fitting-the-model",
    "title": "ISLR Lab - Decision Trees",
    "section": "Splitting and fitting the model",
    "text": "Splitting and fitting the model\n\n\nCode\nset.seed(2)\ntrain &lt;- sample(1:nrow(Carseats), 200)\nCarseats.test &lt;- Carseats[-train,]\nHigh.test &lt;- High[-train]\ntree.carseats &lt;- tree(High ~ .-Sales, data = Carseats, \n                      subset = train)\n\n\nchecking the top few rows of predicted columns\n\n\nCode\ntree.predict &lt;- predict(tree.carseats, Carseats.test, \n                        type = \"class\") #type is needed to declare classification model\nhead(tree.predict)\n\n\n[1] Yes No  No  Yes No  No \nLevels: No Yes\n\n\nComparing predicted with actual values\n\n\nCode\ntable(tree.predict, High.test)\n\n\n            High.test\ntree.predict  No Yes\n         No  104  33\n         Yes  13  50\n\n\nWhat‚Äôs the accuracy?\n\n\nCode\n(104+50)/200\n\n\n[1] 0.77\n\n\n77% Accuracy"
  },
  {
    "objectID": "posts/2023-03-02-ISLRv2-Decision-Trees/index.html#pruning-the-tree-for-improved-classification",
    "href": "posts/2023-03-02-ISLRv2-Decision-Trees/index.html#pruning-the-tree-for-improved-classification",
    "title": "ISLR Lab - Decision Trees",
    "section": "Pruning the tree for improved classification",
    "text": "Pruning the tree for improved classification\nTo improve the accuracy, lets attempt to prune the tree. For this cv.tree() function is used to determine the optimal level of tree complexity. Here the FUN argument is taken as prune.misclass to indicate that the cross-validation and tree pruning should be guided by the classification error instead of the default deviance.\n\n\nCode\nset.seed(7)\ncv.carseats &lt;- cv.tree(tree.carseats, FUN = prune.misclass)\nnames(cv.carseats)\n\n\n[1] \"size\"   \"dev\"    \"k\"      \"method\"\n\n\nNote to self:\n\nk is the regularisation parameter \\(\\alpha\\) (alpha)\nsize is # of terminal nodes for each tree\ndev is the number of cross-validation errors\n\n\n\nCode\ncv.carseats\n\n\n$size\n[1] 21 19 14  9  8  5  3  2  1\n\n$dev\n[1] 75 75 75 74 82 83 83 85 82\n\n$k\n[1] -Inf  0.0  1.0  1.4  2.0  3.0  4.0  9.0 18.0\n\n$method\n[1] \"misclass\"\n\nattr(,\"class\")\n[1] \"prune\"         \"tree.sequence\"\n\n\nVisualising the tree. The classification error is least (74) at size  = 9\n\n\nCode\npar(mfrow = c(1,2))\nplot(cv.carseats$size, cv.carseats$dev, type = \"b\")\nplot(cv.carseats$k, cv.carseats$dev, type = \"b\")\n\n\n\n\n\nRelation between Deviance with tree size & regularisation parameter\n\n\n\n\nUsing the prune.misclass() function to prune the tree to the 9-node specification.\n\n\nCode\nprune.carseats= prune.misclass(tree.carseats, best = 9)\nplot(prune.carseats)\ntext(prune.carseats, pretty = 0)\n\n\n\n\n\n\n\n\n\nChecking the accuracy in the good-old fashioned way (its really that simple!)ü§ì\n\n\nCode\nprune.tree.pred &lt;- predict(prune.carseats, Carseats.test, type = \"class\")\ntable(prune.tree.pred, High.test)\n\n\n               High.test\nprune.tree.pred No Yes\n            No  97  25\n            Yes 20  58\n\n\nSo what‚Äôs the accuracy?\n\n\nCode\n(97+58)/200\n\n\n[1] 0.775\n\n\n77.5% which is slightly better than the non-pruned tree. Not bad."
  },
  {
    "objectID": "posts/2023-03-02-ISLRv2-Decision-Trees/index.html#key-takeaways",
    "href": "posts/2023-03-02-ISLRv2-Decision-Trees/index.html#key-takeaways",
    "title": "ISLR Lab - Decision Trees",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nWithout tuning the model, the default DT algo creates a tree with 27 nodes\ndeviance measured as a result of changing the number of nodes indicates the best DT of 9 nodes.\nThe code needed to write this is surprisingly simple. However, the tidymodels interface allows for managing the resulting output and models in a more structured way."
  },
  {
    "objectID": "posts/2023-03-02-ISLRv2-Decision-Trees/index.html#making-the-predictions",
    "href": "posts/2023-03-02-ISLRv2-Decision-Trees/index.html#making-the-predictions",
    "title": "ISLR Lab - Decision Trees",
    "section": "Making the predictions",
    "text": "Making the predictions\n\n\nCode\nyhat &lt;- predict(tree.boston, newdata = Boston[-train.boston,])\ntest.boston &lt;- Boston[-train.boston,\"medv\"]\n\nplot(yhat, test.boston)\nabline(0,1, col = \"red\")\n\n\n\n\n\nThis plot visualises the Predicted v/s Actuals for Boston test data\n\n\n\n\nMean Square Error is defined as \\[MSE = \\frac{1}{n} \\sum_{i=1}^{n}(y_i - \\hat{y_i})^2\\]\n\n\nCode\nmean((yhat - test.boston)^2)\n\n\n[1] 35.28688\n\n\nRMSE which uses the same units as the output variable is:\n\n\nCode\n(mean((yhat - test.boston)^2))^0.5\n\n\n[1] 5.940276"
  },
  {
    "objectID": "posts/2023-03-02-ISLRv2-Decision-Trees/index.html#key-takeaways-1",
    "href": "posts/2023-03-02-ISLRv2-Decision-Trees/index.html#key-takeaways-1",
    "title": "ISLR Lab - Decision Trees",
    "section": "Key Takeaways:",
    "text": "Key Takeaways:\nAs the SD is the same units as the outcome variable, we can say that this model leads to predictions which on an average are within ¬±$5940 of the true median home value. Can we do better? Let‚Äôs keep digging"
  },
  {
    "objectID": "posts/2023-03-02-ISLRv2-Decision-Trees/index.html#bagging",
    "href": "posts/2023-03-02-ISLRv2-Decision-Trees/index.html#bagging",
    "title": "ISLR Lab - Decision Trees",
    "section": "Bagging",
    "text": "Bagging\n\n\nimportance parameter here will compute and return the importance measures of each predictor variable. Importance measures provide a way to assess the relative importance of each predictor variable in the random forest model, based on the decrease in accuracy that occurs when that variable is excluded from the model. This increases the runtime significantly on large datasets\n\n\nCode\nlibrary(randomForest)\nset.seed(1)\nbag.boston &lt;- randomForest(medv ~ . , data = Boston, \n                           subset = train.boston, \n                           mtry = 12, # m = p\n                           importance = T)\nbag.boston\n\n\n\nCall:\n randomForest(formula = medv ~ ., data = Boston, mtry = 12, importance = T,      subset = train.boston) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 12\n\n          Mean of squared residuals: 11.40162\n                    % Var explained: 85.17\n\n\n\n\nCode\nyhat.bag &lt;- predict(bag.boston, newdata = Boston[-train.boston, ])\nplot(yhat.bag, test.boston)\nabline(0,1,col = \"red\")\n\n\n\n\n\nPredicted v/s Actuals for Boston test data using Bagging\n\n\n\n\nWhat‚Äôs the accuracy here? Checking the MSE\n\n\nCode\nmean((yhat.bag - test.boston)^2)\n\n\n[1] 23.41916\n\n\nAnd square root of MSE or RMSE is:\n\n\nCode\n(mean((yhat.bag - test.boston)^2))^0.5\n\n\n[1] 4.839335\n\n\nThat‚Äôs $4839 which is better than $ 5940 derived from the 7-node decision tree discussed in Key Takeaways. Moving to Random Forest now."
  },
  {
    "objectID": "posts/2023-03-02-ISLRv2-Decision-Trees/index.html#random-forest",
    "href": "posts/2023-03-02-ISLRv2-Decision-Trees/index.html#random-forest",
    "title": "ISLR Lab - Decision Trees",
    "section": "Random Forest",
    "text": "Random Forest\nIts the same code, but we alter the number of predicted variables to \\(m= 6\\) which is the mtry parameter\n\n\nDefault settings for randomForest()\nfor regression analysis, \\(m = p/3\\)\nfor classification analysis, \\(m = \\sqrt p\\)\n\n\nCode\nset.seed(1)\nrf.boston &lt;- randomForest(medv ~ . , data = Boston, \n                           subset = train.boston, \n                           mtry = 6, # m = p/2\n                           importance = T)\nrf.boston\n\n\n\nCall:\n randomForest(formula = medv ~ ., data = Boston, mtry = 6, importance = T,      subset = train.boston) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 6\n\n          Mean of squared residuals: 10.09466\n                    % Var explained: 86.87\n\n\n\n\nCode\nyhat.rf &lt;- predict(rf.boston, newdata = Boston[-train.boston, ])\nplot(yhat.rf, test.boston)\nabline(0,1,col = \"red\")\n\n\n\n\n\nPredicted v/s Actuals for Boston test data using RandomForest\n\n\n\n\nWhat‚Äôs the MSE here?\n\n\nCode\nmean((yhat.rf - test.boston)^2)\n\n\n[1] 20.06644\n\n\n.. and therefore RMSE is:\n\n\nCode\nmean((yhat.rf - test.boston)^2)^0.5\n\n\n[1] 4.479558\n\n\nThat‚Äôs ¬±$4480 from the mean predicted values - which is better than $4839 by using the Bagging method.\nBefore moving ahead, we can also check the importance() function to determine key predictors\n\n\nCode\nimportance(rf.boston)\n\n\n          %IncMSE IncNodePurity\ncrim    19.435587    1070.42307\nzn       3.091630      82.19257\nindus    6.140529     590.09536\nchas     1.370310      36.70356\nnox     13.263466     859.97091\nrm      35.094741    8270.33906\nage     15.144821     634.31220\ndis      9.163776     684.87953\nrad      4.793720      83.18719\ntax      4.410714     292.20949\nptratio  8.612780     902.20190\nlstat   28.725343    5813.04833\n\n\nWhat are these columns?\n\n%IncMSE: Avg decrease in accuracy of predictions on out-of-bag samples when given variable is calculated\nIncNodePurity:Total decrease in node purity that results from split on that variable averaged over all trees.\n\nin regression trees, the node impurity measured by the training Residual Sum of Squares(RSS)\nin classification trees, it is the deviance\n\n\n\n\nCode\nvarImpPlot(rf.boston)\n\n\n\n\n\nPredicted v/s Actuals for Boston test data using Bagging\n\n\n\n\nThis shows that the two most important variables are rm (average number of rooms per dwelling) and lstat (lower status of the population in %)"
  },
  {
    "objectID": "posts/2023-03-02-ISLRv2-Decision-Trees/index.html#partial-dependence-plots",
    "href": "posts/2023-03-02-ISLRv2-Decision-Trees/index.html#partial-dependence-plots",
    "title": "ISLR Lab - Decision Trees",
    "section": "partial dependence plots",
    "text": "partial dependence plots\nBy plotting the partial dependence of rm and lstat on outcome variable, we see that\n\nrm has a direct relation viz.¬†more the number of rooms, higher the price increases\nlstat has an inverse relation viz.¬†higher the lower stata in the neighbourhood, lower the price\n\n\n\nCode\npar(mfrow = c(1,2))\nplot(boost.boston, i = \"rm\")\n\n\n\n\n\n\n\n\n\nCode\nplot(boost.boston, i = \"lstat\")"
  },
  {
    "objectID": "posts/2023-03-02-ISLRv2-Decision-Trees/index.html#predictions",
    "href": "posts/2023-03-02-ISLRv2-Decision-Trees/index.html#predictions",
    "title": "ISLR Lab - Decision Trees",
    "section": "Predictions",
    "text": "Predictions\n\n\nCode\nyhat.boost &lt;- predict(boost.boston, newdata = Boston[-train.boston,], \n                      n.trees = 5000)\n\n#| fig-cap: \"Predicted v/s Actuals for Boston test data using Boosted RF\"\n#| fig-cap-location: top\nplot(yhat.boost, test.boston)\nabline(0,1,col = \"red\")\n\n\n\n\n\n\n\n\n\nFigure looks so much better. testing the accuracy now. starting with the MSE\n\n\nCode\nmean((yhat.boost - test.boston)^2)\n\n\n[1] 12.98195\n\n\nWow.. that‚Äôs significantly lower. How about the RMSE?\n\n\nCode\nmean((yhat.boost - test.boston)^2)^0.5\n\n\n[1] 3.603047\n\n\nAmazing. This means our predicted value on an average is ¬±$3603 from the actual which is a signifcant improvement from the RMSE calculated by Random Forest ¬±$4480"
  },
  {
    "objectID": "posts/2023-03-02-ISLRv2-Decision-Trees/index.html#adding-regularisation-parameter-ùõå",
    "href": "posts/2023-03-02-ISLRv2-Decision-Trees/index.html#adding-regularisation-parameter-ùõå",
    "title": "ISLR Lab - Decision Trees",
    "section": "Adding regularisation parameter ùõå",
    "text": "Adding regularisation parameter ùõå\nAlso referred as the shrinkage parameter, the default value is 0.001 but we will change this to 0.01\n\n\nCode\nset.seed(1)\nboost.boston2 &lt;- gbm(medv ~ ., data = Boston[train,], \n                    distribution = \"gaussian\",\n                    n.trees = 5000,\n                    interaction.depth = 4, \n                    shrinkage = 0.01)\nyhat.boost2 &lt;- predict(boost.boston2, newdata = Boston[-train.boston,], \n                      n.trees = 5000)\n\n\nThe resulting MSE therefore is calculated as:\n\n\nCode\nmean((yhat.boost2 - test.boston)^2)^0.5\n\n\n[1] 3.593472\n\n\nNow we‚Äôve got it even lower at ¬±$3593"
  },
  {
    "objectID": "posts/2024-10-14-hiringquestions/index.html",
    "href": "posts/2024-10-14-hiringquestions/index.html",
    "title": "Interview tips for Growth marketeers",
    "section": "",
    "text": "Interview tips for Growth marketeers\nHow do you assess skills of an incoming candidate specifically in the field of marketing? And what should the interviewee prepare in advance? Sharing a few tips I‚Äôve picked over the years.\nApart from a plethora of others, here are some mandatory questions that creep into the interview rounds at one point or another. Would be glad to know what works for u as well in the comments.\nüó£ Before the candidate starts to talk, I try setting the context of the job with regards to the category and the company. Then leave the floor open for them to understand their journey and curiosity towards the job profile.\nüëÇ Listening for: Expression of interest towards the profile. Most times the homework is visiting the homepage - which is meant for customers and not prospective employees. Therefore any action apart from this is welcome (eg. visiting Linkedin, reading articles about the company, researching about founders/ CXO, speaking to ex-employees are all positive signs). Speaking to customers is an absolute win in my view!\nüó£ tell me about a few other growth companies/ personalities/ resources that you admire? Any other companies doing similar work in the category?\nüëÇ Listening for: industry knowledge. Cannot understate how important it is to stay up-to-date in the field of marketing (especially digital / growth). If you think growth is all about setting paid ads, then you‚Äôre displaying your very limited knowledge base. Keep reading and soaking in information about all aspects of the marketing funnel. At the very least - please talk about how you‚Äôre working on AARRR frameworks (not expanding here. google for yourself and thank me later!)\nüó£ Tell me about a non-obvious growth experiment you ran. What were the results? What did you learn, and how did you use what you learned?\nüëÇ Listening for: understanding how a candidate is able to think qualitatively and quantitatively about growth problems. Asking for a non-obvious example can help weed out candidates who simply follow best practices without thinking critically.\nüó£ Tell me about an A/B test that completely failed. How did the failure lead to a win?\nüëÇ Listening for: candidate‚Äôs ability to analyse failures. it shows if the interviewee has ability to identify unexpected insights and translate those insights into actionable steps that can lead to positive outcomes. This is my fav question because strong growth marketers always seek out lessons from their failures as well.\nLet us talk more the comments on the right‚û°Ô∏è"
  },
  {
    "objectID": "posts/2024-09-23-mondaymotivation/index.html",
    "href": "posts/2024-09-23-mondaymotivation/index.html",
    "title": "Monday Motivation",
    "section": "",
    "text": "Monday Motivation: Answer Their Call\nToday‚Äôs Monday Motivation post is a call to action. Events of last week left me disturbed and I reflected on my own behaviour.\nA Faustian bargain is a tragic deal where more is lost than gained. A young magician named Faust makes a deal with the devil to unlock great magic but ultimately sells his soul in return. Office going folks will find this tale oddly familiar. While we slog those long hours in office, we silently mourn the loss of time - away from our dreams, our family, our loved ones who need us more.\n\nThe past 1.5 years where I‚Äôve been away from family made me realise a lot of mistakes that I made. And in the past 1.5 years, I‚Äôve tried to correct myself by doing the one thing that I guess I should done all along - answer the call. Whenever my family rang in I‚Äôd answer the call. And I urged my team-members to do the same. If it was an important meeting, at the very least I‚Äôd message back and call back later.\nAnswer Their Call.\nYour company will only check if you‚Äôve logged in on time. It will not check if you‚Äôve had dinner or reached home on time. Only your family will.\nAnswer Their Call.\nNo matter what the situation, I urge you all to #AnswerTheirCall.¬†Talk to your team and colleagues about this more often.\nAnswer Their Call.\nTell yourself its ok to prioritise this for a few minutes. Give your brain more signals to correct the work life balance. We surround ourselves 15 hours a day for years together that our brains have been conditioned to behave obediently.\nAnswer Their Call.\nLet us talk more the comments on the right‚û°Ô∏è"
  },
  {
    "objectID": "posts/2022-10-14-day-8-of-kaggle/index.en.html",
    "href": "posts/2022-10-14-day-8-of-kaggle/index.en.html",
    "title": "Day 8 of #50daysofkaggle",
    "section": "",
    "text": "Progress till date:\n\nDownload titanic dataset and assign to train & test\nRearranging the data\nEDA (including plots and finding survival rate using .groupby())\nModelling\nData preparation - one-hot encoding the Sex, Pclass & Embarked columns - appending these to the numerical columns - normalising the data - splitting between train into X_train, y_train, X_test, y_test\nApplying KNN algo\n\nfinding the right K based on accuracy. (best at K = 7)\nCalculating the accuracy based on test\n\n\nTo do today: - Perform Decision Tree classification\n\n\nReading and printing the top 5 rows\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport zipfile\n\n\n#importing the zipfile already saved in the other folder. \nzf = zipfile.ZipFile(\"../2022-10-12-day-6-of-50daysofkaggle/titanic.zip\")\ntrain = pd.read_csv(zf.open(\"train.csv\"))\ntest = pd.read_csv(zf.open(\"test.csv\"))\n\n#Selecting only the numerical columns\nnum_col = train.select_dtypes(include=np.number).columns.tolist()\n\n#deslecting passenger ID and 'Survived' \ndel num_col[0:2] #.remove() can remove only 1 item. so for more than 1, use for loop \nselect_col = num_col\n\n#remaining columns\nstr_col= [\"Sex\", \"Embarked\", \"Survived\"]\n\n\n#Adding more elements into a list using `extend` and not `append`\nselect_col.extend(str_col)\n\ntrain_eda= train[train.columns.intersection(select_col)]\ntrain_eda.head()\n\n\n\n\n\n\n\n\n\nSurvived\nPclass\nSex\nAge\nSibSp\nParch\nFare\nEmbarked\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\nS\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nC\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\nS\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\nS\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500\nS\n\n\n\n\n\n\n\n\n\n\nChecking all na values in the existing dataset\n\n\nCode\ntrain_eda.isna().sum().sort_values()\n\n\nSurvived      0\nPclass        0\nSex           0\nSibSp         0\nParch         0\nFare          0\nEmbarked      2\nAge         177\ndtype: int64\n\n\nFinding the mean\n\n\nCode\ntrain_eda[\"Age\"].median()\n\n\n28.0\n\n\nReplacing na cells with the mean\n\n\nCode\ntrain_eda[\"Age\"].fillna(value = train_eda[\"Age\"].median(), inplace = True)\ntrain_eda.isna().sum().sort_values()\n\n\nC:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_9908\\1076914416.py:1: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\nSurvived    0\nPclass      0\nSex         0\nAge         0\nSibSp       0\nParch       0\nFare        0\nEmbarked    2\ndtype: int64\n\n\nSidenote: Was getting a wierd warning (SettingWithCopyWarning) while using .fillna() to replace na with the median values. Turns out there‚Äôs a between calling a view or a copy. One way of avoiding this error is to use train_eda.loc[:,\"Age\"] instead of train_eda[\"Age\"]. This is because .loc returns the view (original) while using subsets. Elegant explanation here. Below code will not throw up a warning.\n\n\nCode\nxx = train_eda.copy()\nxx.loc[:,\"Age\"].fillna(value = xx.Age.median(), inplace = True)\nxx.isna().sum()\n\n\nSurvived    0\nPclass      0\nSex         0\nAge         0\nSibSp       0\nParch       0\nFare        0\nEmbarked    2\ndtype: int64\n\n\n\n\n\nSeperating X & y. Here‚Äôs the first 5 rows of X\n\n\nCode\ntrain_eda = train_eda.dropna(axis = 0) #removing all rows with NA\n\nX = train_eda[[\"Age\", \"SibSp\", \"Parch\", \"Fare\"]]\nX = pd.concat([X,pd.get_dummies(data = train_eda[[\"Sex\", \"Embarked\", \"Pclass\"]], columns = [\"Sex\", \"Embarked\", \"Pclass\"])], axis = 1)\n\nX.head()\n\n\n\n\n\n\n\n\n\nAge\nSibSp\nParch\nFare\nSex_female\nSex_male\nEmbarked_C\nEmbarked_Q\nEmbarked_S\nPclass_1\nPclass_2\nPclass_3\n\n\n\n\n0\n22.0\n1\n0\n7.2500\n0\n1\n0\n0\n1\n0\n0\n1\n\n\n1\n38.0\n1\n0\n71.2833\n1\n0\n1\n0\n0\n1\n0\n0\n\n\n2\n26.0\n0\n0\n7.9250\n1\n0\n0\n0\n1\n0\n0\n1\n\n\n3\n35.0\n1\n0\n53.1000\n1\n0\n0\n0\n1\n1\n0\n0\n\n\n4\n35.0\n0\n0\n8.0500\n0\n1\n0\n0\n1\n0\n0\n1\n\n\n\n\n\n\n\nHere‚Äôs the first 5 rows of y\n\n\nCode\ny = train_eda[\"Survived\"].values\ny[0:5]\n\n\narray([0, 1, 1, 1, 0], dtype=int64)\n\n\ncomparing the shapes of X and y\n\n\nCode\nlen(y) #889 after filling up the NA. previously 712\nX.shape #(889, 12)\n\n\n(889, 12)\n\n\n\n\nStandardising and printing the first 5 datapoints.\n\n\nCode\nfrom sklearn import preprocessing\n\nX= preprocessing.StandardScaler().fit(X).transform(X)\nX[0:5]\n\n\narray([[-0.56367407,  0.43135024, -0.47432585, -0.50023975, -0.73534203,\n         0.73534203, -0.48271079, -0.30794088,  0.61679395, -0.56306042,\n        -0.51087465,  0.90032807],\n       [ 0.66921696,  0.43135024, -0.47432585,  0.78894661,  1.35991138,\n        -1.35991138,  2.07163382, -0.30794088, -1.62128697,  1.77600834,\n        -0.51087465, -1.11070624],\n       [-0.25545131, -0.47519908, -0.47432585, -0.48664993,  1.35991138,\n        -1.35991138, -0.48271079, -0.30794088,  0.61679395, -0.56306042,\n        -0.51087465,  0.90032807],\n       [ 0.43804989,  0.43135024, -0.47432585,  0.42286111,  1.35991138,\n        -1.35991138, -0.48271079, -0.30794088,  0.61679395,  1.77600834,\n        -0.51087465, -1.11070624],\n       [ 0.43804989, -0.47519908, -0.47432585, -0.4841333 , -0.73534203,\n         0.73534203, -0.48271079, -0.30794088,  0.61679395, -0.56306042,\n        -0.51087465,  0.90032807]])\n\n\n\n\n\nSplitting into test & train data and comparing the dimensions.\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\nprint ('Train set\\t :', X_train.shape,  y_train.shape,\n'\\nTest set\\t :', X_test.shape,  y_test.shape)\n\n\nTrain set    : (711, 12) (711,) \nTest set     : (178, 12) (178,)\n\n\n\n\n\nLets check the classification results using Decision trees. First 10 are as follows:\n\n\nCode\nfrom sklearn.tree import DecisionTreeClassifier\n\nDtree = DecisionTreeClassifier(criterion = \"entropy\", max_depth = 3)\nDtree.fit(X_train,y_train)\ny_test_hat = Dtree.predict(X_test)\nprint(\"First 10 actual\\t\\t:\", y_test[0:10],\"\\nFirst 10 predicted\\t:\", y_test_hat[0:10])\n\n\nFirst 10 actual     : [1 1 0 1 1 1 0 0 0 0] \nFirst 10 predicted  : [1 1 0 1 1 0 0 0 0 0]\n\n\n\n\n\nCalculating accuracy using Decision Tree classification for y_test\n\n\nCode\nfrom sklearn import metrics\n\nprint(\"Decision Tree Accuracy\\t:\", metrics.accuracy_score(y_test, y_test_hat),\"\\nRMSE\\t\\t\\t:\", metrics.mean_squared_error(y_test,y_test_hat),\"\\nNormalised RMSE\\t\\t:\", metrics.mean_squared_error(y_test,y_test_hat)/np.std(y_test))\n\n\nDecision Tree Accuracy  : 0.8314606741573034 \nRMSE            : 0.16853932584269662 \nNormalised RMSE     : 0.34266139226005143\n\n\nNot bad. We find that Test accuracy is around 83% for Decision Trees and RMSE of 0.168\n\n\n\nHere‚Äôs a neat little trick to see how the DT actually thinks.\n\n\nCode\nfrom sklearn import tree\nimport matplotlib.pyplot as plt\n\nplt.clf()\ntree.plot_tree(Dtree)\nplt.show()"
  },
  {
    "objectID": "posts/2022-10-14-day-8-of-kaggle/index.en.html#loading-the-data",
    "href": "posts/2022-10-14-day-8-of-kaggle/index.en.html#loading-the-data",
    "title": "Day 8 of #50daysofkaggle",
    "section": "",
    "text": "Reading and printing the top 5 rows\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport zipfile\n\n\n#importing the zipfile already saved in the other folder. \nzf = zipfile.ZipFile(\"../2022-10-12-day-6-of-50daysofkaggle/titanic.zip\")\ntrain = pd.read_csv(zf.open(\"train.csv\"))\ntest = pd.read_csv(zf.open(\"test.csv\"))\n\n#Selecting only the numerical columns\nnum_col = train.select_dtypes(include=np.number).columns.tolist()\n\n#deslecting passenger ID and 'Survived' \ndel num_col[0:2] #.remove() can remove only 1 item. so for more than 1, use for loop \nselect_col = num_col\n\n#remaining columns\nstr_col= [\"Sex\", \"Embarked\", \"Survived\"]\n\n\n#Adding more elements into a list using `extend` and not `append`\nselect_col.extend(str_col)\n\ntrain_eda= train[train.columns.intersection(select_col)]\ntrain_eda.head()\n\n\n\n\n\n\n\n\n\nSurvived\nPclass\nSex\nAge\nSibSp\nParch\nFare\nEmbarked\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\nS\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nC\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\nS\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\nS\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500\nS"
  },
  {
    "objectID": "posts/2022-10-14-day-8-of-kaggle/index.en.html#cleaning-up-the-data",
    "href": "posts/2022-10-14-day-8-of-kaggle/index.en.html#cleaning-up-the-data",
    "title": "Day 8 of #50daysofkaggle",
    "section": "",
    "text": "Checking all na values in the existing dataset\n\n\nCode\ntrain_eda.isna().sum().sort_values()\n\n\nSurvived      0\nPclass        0\nSex           0\nSibSp         0\nParch         0\nFare          0\nEmbarked      2\nAge         177\ndtype: int64\n\n\nFinding the mean\n\n\nCode\ntrain_eda[\"Age\"].median()\n\n\n28.0\n\n\nReplacing na cells with the mean\n\n\nCode\ntrain_eda[\"Age\"].fillna(value = train_eda[\"Age\"].median(), inplace = True)\ntrain_eda.isna().sum().sort_values()\n\n\nC:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_9908\\1076914416.py:1: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\nSurvived    0\nPclass      0\nSex         0\nAge         0\nSibSp       0\nParch       0\nFare        0\nEmbarked    2\ndtype: int64\n\n\nSidenote: Was getting a wierd warning (SettingWithCopyWarning) while using .fillna() to replace na with the median values. Turns out there‚Äôs a between calling a view or a copy. One way of avoiding this error is to use train_eda.loc[:,\"Age\"] instead of train_eda[\"Age\"]. This is because .loc returns the view (original) while using subsets. Elegant explanation here. Below code will not throw up a warning.\n\n\nCode\nxx = train_eda.copy()\nxx.loc[:,\"Age\"].fillna(value = xx.Age.median(), inplace = True)\nxx.isna().sum()\n\n\nSurvived    0\nPclass      0\nSex         0\nAge         0\nSibSp       0\nParch       0\nFare        0\nEmbarked    2\ndtype: int64"
  },
  {
    "objectID": "posts/2022-10-14-day-8-of-kaggle/index.en.html#model-building",
    "href": "posts/2022-10-14-day-8-of-kaggle/index.en.html#model-building",
    "title": "Day 8 of #50daysofkaggle",
    "section": "",
    "text": "Seperating X & y. Here‚Äôs the first 5 rows of X\n\n\nCode\ntrain_eda = train_eda.dropna(axis = 0) #removing all rows with NA\n\nX = train_eda[[\"Age\", \"SibSp\", \"Parch\", \"Fare\"]]\nX = pd.concat([X,pd.get_dummies(data = train_eda[[\"Sex\", \"Embarked\", \"Pclass\"]], columns = [\"Sex\", \"Embarked\", \"Pclass\"])], axis = 1)\n\nX.head()\n\n\n\n\n\n\n\n\n\nAge\nSibSp\nParch\nFare\nSex_female\nSex_male\nEmbarked_C\nEmbarked_Q\nEmbarked_S\nPclass_1\nPclass_2\nPclass_3\n\n\n\n\n0\n22.0\n1\n0\n7.2500\n0\n1\n0\n0\n1\n0\n0\n1\n\n\n1\n38.0\n1\n0\n71.2833\n1\n0\n1\n0\n0\n1\n0\n0\n\n\n2\n26.0\n0\n0\n7.9250\n1\n0\n0\n0\n1\n0\n0\n1\n\n\n3\n35.0\n1\n0\n53.1000\n1\n0\n0\n0\n1\n1\n0\n0\n\n\n4\n35.0\n0\n0\n8.0500\n0\n1\n0\n0\n1\n0\n0\n1\n\n\n\n\n\n\n\nHere‚Äôs the first 5 rows of y\n\n\nCode\ny = train_eda[\"Survived\"].values\ny[0:5]\n\n\narray([0, 1, 1, 1, 0], dtype=int64)\n\n\ncomparing the shapes of X and y\n\n\nCode\nlen(y) #889 after filling up the NA. previously 712\nX.shape #(889, 12)\n\n\n(889, 12)\n\n\n\n\nStandardising and printing the first 5 datapoints.\n\n\nCode\nfrom sklearn import preprocessing\n\nX= preprocessing.StandardScaler().fit(X).transform(X)\nX[0:5]\n\n\narray([[-0.56367407,  0.43135024, -0.47432585, -0.50023975, -0.73534203,\n         0.73534203, -0.48271079, -0.30794088,  0.61679395, -0.56306042,\n        -0.51087465,  0.90032807],\n       [ 0.66921696,  0.43135024, -0.47432585,  0.78894661,  1.35991138,\n        -1.35991138,  2.07163382, -0.30794088, -1.62128697,  1.77600834,\n        -0.51087465, -1.11070624],\n       [-0.25545131, -0.47519908, -0.47432585, -0.48664993,  1.35991138,\n        -1.35991138, -0.48271079, -0.30794088,  0.61679395, -0.56306042,\n        -0.51087465,  0.90032807],\n       [ 0.43804989,  0.43135024, -0.47432585,  0.42286111,  1.35991138,\n        -1.35991138, -0.48271079, -0.30794088,  0.61679395,  1.77600834,\n        -0.51087465, -1.11070624],\n       [ 0.43804989, -0.47519908, -0.47432585, -0.4841333 , -0.73534203,\n         0.73534203, -0.48271079, -0.30794088,  0.61679395, -0.56306042,\n        -0.51087465,  0.90032807]])\n\n\n\n\n\nSplitting into test & train data and comparing the dimensions.\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\nprint ('Train set\\t :', X_train.shape,  y_train.shape,\n'\\nTest set\\t :', X_test.shape,  y_test.shape)\n\n\nTrain set    : (711, 12) (711,) \nTest set     : (178, 12) (178,)\n\n\n\n\n\nLets check the classification results using Decision trees. First 10 are as follows:\n\n\nCode\nfrom sklearn.tree import DecisionTreeClassifier\n\nDtree = DecisionTreeClassifier(criterion = \"entropy\", max_depth = 3)\nDtree.fit(X_train,y_train)\ny_test_hat = Dtree.predict(X_test)\nprint(\"First 10 actual\\t\\t:\", y_test[0:10],\"\\nFirst 10 predicted\\t:\", y_test_hat[0:10])\n\n\nFirst 10 actual     : [1 1 0 1 1 1 0 0 0 0] \nFirst 10 predicted  : [1 1 0 1 1 0 0 0 0 0]\n\n\n\n\n\nCalculating accuracy using Decision Tree classification for y_test\n\n\nCode\nfrom sklearn import metrics\n\nprint(\"Decision Tree Accuracy\\t:\", metrics.accuracy_score(y_test, y_test_hat),\"\\nRMSE\\t\\t\\t:\", metrics.mean_squared_error(y_test,y_test_hat),\"\\nNormalised RMSE\\t\\t:\", metrics.mean_squared_error(y_test,y_test_hat)/np.std(y_test))\n\n\nDecision Tree Accuracy  : 0.8314606741573034 \nRMSE            : 0.16853932584269662 \nNormalised RMSE     : 0.34266139226005143\n\n\nNot bad. We find that Test accuracy is around 83% for Decision Trees and RMSE of 0.168\n\n\n\nHere‚Äôs a neat little trick to see how the DT actually thinks.\n\n\nCode\nfrom sklearn import tree\nimport matplotlib.pyplot as plt\n\nplt.clf()\ntree.plot_tree(Dtree)\nplt.show()"
  },
  {
    "objectID": "posts/2022-04-05-what-a-wonderful-wordle/index.en.html",
    "href": "posts/2022-04-05-what-a-wonderful-wordle/index.en.html",
    "title": "What a wonderful ‚Äòwordle‚Äô!",
    "section": "",
    "text": "Who ever thought that a bunch of black and green boxes would bring out the logophile in us all? With friends and family groups sharing their progress, I find this to be an entertaining mind-puzzle to kickstart the day.\nAnd I was not alone in my quest for 5 letter words. Wordle has tickled the fascination of many in the data science community. I found Arthur Holtz‚Äôs lucid breakdown of the Wordle dataset quite interesting. Of course, there is 3B1B‚Äôs incredibly detailed videos on applying Information Theory to this 6-by-5 grid. (original video as well as the follow-up errata)\nOthers have simulated the wordle game (like here) or even solved it for you (like this blog). I‚Äôve read at least one blog post that has an academic take on the matter.\nFortunately for the reader, none of the above will be attempted by me. My inspiration comes from Gerry Chng‚Äôs Frequency Analysis Approach where I‚Äôve tried to understand the most commonly occuring letters in the official word list by position by considering a ranking mechanism"
  },
  {
    "objectID": "posts/2022-04-05-what-a-wonderful-wordle/index.en.html#what-is-a-wordle",
    "href": "posts/2022-04-05-what-a-wonderful-wordle/index.en.html#what-is-a-wordle",
    "title": "What a wonderful ‚Äòwordle‚Äô!",
    "section": "What is a wordle?",
    "text": "What is a wordle?\nThe game rules are fairly simple:\n\nYou need to guess a 5-letter word. One new word is given every day\nYou are given 6 guesses\nAfter every guess, each square is coded by a color\n\nGREY: chosen letter is not in the word\nYELLOW: chosen letter is in the word by wrong position\nGREEN: chosen letter is in the word and in the correct position\n\nRepetition of letters is allowed\n\nThat‚Äôs it!\nIn my opinion, one of the reasons for the game going viral is the way the results are shared. You‚Äôve possibly seen something like this floating around:\n\n\n\nSample world share\n\n\n‚Ä¶And if your family too has been bitten hard by the Wordle bug, then you would be familiar with group messages like this!\n\n\n\nWorld share in whatsapp"
  },
  {
    "objectID": "posts/2022-04-05-what-a-wonderful-wordle/index.en.html#frequency-analysis",
    "href": "posts/2022-04-05-what-a-wonderful-wordle/index.en.html#frequency-analysis",
    "title": "What a wonderful ‚Äòwordle‚Äô!",
    "section": "Frequency analysis",
    "text": "Frequency analysis\nArthur Hotlz‚Äôs blog is a good place to start for extracting and loading the Official Wordle list. After parsing and cleaning the data, here‚Äôs all the words broken down into a single rectangular dataframe word_list .\nUpdate 29th Jan ‚Äô23: NYT‚Äôs .js file is not retrieving any list for some reason. I‚Äôve referred to Arjun Vikram‚Äôs repo on dagshub\n\n\nCode\nknitr::opts_chunk$set(warning = FALSE, message = FALSE) \nsuppressMessages({ \nlibrary(httr)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(ggthemes)\nlibrary(scales)\nlibrary(tidyr)\nlibrary(tibble)\nlibrary(forcats)\nlibrary(knitr)\nlibrary(kableExtra)\ntheme_set(theme_light())\n})\n\nurl &lt;- \"https://www.nytimes.com/games/wordle/main.18637ca1.js\" #not working\nurl2 &lt;- \"https://dagshub.com/arjvik/wordle-wordlist/raw/e8d07d33a59a6b05f3b08bd827385604f89d89a0/answerlist.txt\"\nwordle_script_text &lt;- GET(url2) %&gt;% \n  content(as = \"text\", encoding = \"UTF-8\")\n# word_list = substr(\n#   wordle_script_text,\n#   # cigar is the first word\n#   str_locate(wordle_script_text, \"cigar\")[,\"start\"],\n#   # shave is the last word\n#   str_locate(wordle_script_text, \"shave\")[,\"end\"]) %&gt;%\n#   str_remove_all(\"\\\"\") %&gt;%\n#   str_split(\",\") %&gt;%\n#   data.frame() %&gt;%\n#   select(word = 1) %&gt;%\n#   mutate(word = toupper(word))\n\n\nwordle_list &lt;- str_split(wordle_script_text, \"\\n\")\n\nwordle_list &lt;- data.frame(wordle_list) \n\nwordle_list &lt;- rename(wordle_list, word = names(wordle_list)[1] ) %&gt;% mutate(word = toupper(word)) #renaming column to 'word'\n\ndim(wordle_list)\n\n\n[1] 2310    1\n\n\nCode\nhead(wordle_list)\n\n\n   word\n1 CIGAR\n2 REBUT\n3 SISSY\n4 HUMPH\n5 AWAKE\n6 BLUSH\n\n\nModification to the above is another dataframe with each of the characters separated into columns which we‚Äôll call position_word_list\nThe line select(-x) removes the empty column that is created due to the seperate() function\n\n\nCode\nposition_word_list &lt;- wordle_list %&gt;% \n  separate(word, \n           sep = \"\", \n           into = c(\"x\",\"p1\",\"p2\",\"p3\",\"p4\",\"p5\")) %&gt;% \n  select(-x)\nhead(position_word_list,10)\n\n\n   p1 p2 p3 p4 p5\n1   C  I  G  A  R\n2   R  E  B  U  T\n3   S  I  S  S  Y\n4   H  U  M  P  H\n5   A  W  A  K  E\n6   B  L  U  S  H\n7   F  O  C  A  L\n8   E  V  A  D  E\n9   N  A  V  A  L\n10  S  E  R  V  E\n\n\nNow onto some frequency analysis. Here‚Äôs a breakdown of all the letters in the wordle list sorted by number of occurrences stored in letter_list and creating a simple bar graph.\n\n\nCode\nletter_list &lt;- wordle_list %&gt;%\n  as.character() %&gt;%\n  str_split(\"\") %&gt;% \n  as.data.frame() %&gt;% \n  select(w_letter = 1) %&gt;% \n  filter(row_number()!=1) %&gt;%\n  filter(w_letter %in% LETTERS) %&gt;% \n  mutate(type = case_when(w_letter %in% c(\"A\",\"E\",\"I\",\"O\",\"U\") ~ \"vowel\",\n                          T ~ \"consonant\")) %&gt;% \n  group_by(w_letter, type) %&gt;% \n  summarise(freq = n()) %&gt;% \n  arrange(desc(freq))\n\nletter_list %&gt;% ungroup() %&gt;% \n  ggplot(aes(x = reorder(w_letter, -freq), y = freq))+\n  geom_col(aes(fill = type))+\n  scale_y_continuous(labels = comma)+\n  geom_text(aes(label = freq), \n            size = 3)+\n  labs(x = \"Letter\", y = \"Frequency\",\n       title = \"Frequency of letters in Official Wordle list\")\n\n\n\n\n\n\n\n\n\nThis is interesting. Now I‚Äôm curious to know the top words by each position. To do this, I created a single table called freq_table that provides me with the frequency of occurrences by position for each letter. To iterate this process across all the 5 places, I used a for loop. Output is generated via the kableExtra package which provides a neat scrollable window\n\n\nCode\n#declaring null table\nfreq_table &lt;- tibble(alpha = LETTERS)\n\nfor(i in 1:5){\n    test &lt;- position_word_list %&gt;% \n    select(all_of(i)) %&gt;%\n# group_by_at() used for column index ID\n    group_by_at(1) %&gt;% \n    summarise(f = n()) %&gt;% \n    arrange(desc(f)) %&gt;% \n#first column returns p1, p2.. etc and is standardised\n    rename(a = 1) \n\n#adding the freq values to a new dataframe\n    freq_table &lt;- freq_table %&gt;%\n    left_join(test, by = c(\"alpha\" = \"a\")) \n\n#renaming column name to reflect the position number\n    colnames(freq_table)[1+i] = paste0(\"p\",i)\n    rm(test)\n}\n#replacing NA with zero\nfreq_table[is.na(freq_table)] &lt;- 0 \n#output using kable's scrollable window \nkable(freq_table, \n      format = \"html\", \n      caption = \"Frequency Table\") %&gt;%\n    kable_styling() %&gt;%\n    scroll_box(width = \"70%\", height = \"300px\") %&gt;% \n  kable_classic()\n\n\n\n\nFrequency Table\n\n\nalpha\np1\np2\np3\np4\np5\n\n\n\n\nA\n140\n304\n306\n162\n63\n\n\nB\n173\n16\n56\n24\n11\n\n\nC\n198\n40\n56\n150\n31\n\n\nD\n111\n20\n75\n69\n118\n\n\nE\n72\n241\n177\n318\n422\n\n\nF\n135\n8\n25\n35\n26\n\n\nG\n115\n11\n67\n76\n41\n\n\nH\n69\n144\n9\n28\n137\n\n\nI\n34\n201\n266\n158\n11\n\n\nJ\n20\n2\n3\n2\n0\n\n\nK\n20\n10\n12\n55\n113\n\n\nL\n87\n200\n112\n162\n155\n\n\nM\n107\n38\n61\n68\n42\n\n\nN\n37\n87\n137\n182\n130\n\n\nO\n41\n279\n243\n132\n58\n\n\nP\n141\n61\n57\n50\n56\n\n\nQ\n23\n5\n1\n0\n0\n\n\nR\n105\n267\n163\n150\n212\n\n\nS\n365\n16\n80\n171\n36\n\n\nT\n149\n77\n111\n139\n253\n\n\nU\n33\n185\n165\n82\n1\n\n\nV\n43\n15\n49\n45\n0\n\n\nW\n82\n44\n26\n25\n17\n\n\nX\n0\n14\n12\n3\n8\n\n\nY\n6\n22\n29\n3\n364\n\n\nZ\n3\n2\n11\n20\n4\n\n\n\n\n\n\n\nThis table looks good. However, for my visualisation, I want to plot the top 10 letters in each position. For this, I‚Äôm going to use pivot_longer() to make it easier to generate the viz.\n\n\nCode\nfreq_table_long10 &lt;- freq_table %&gt;% \n  pivot_longer(cols = !alpha, names_to = \"position\", values_to = \"freq\") %&gt;% \n  select(position, alpha, freq) %&gt;% \n  arrange(position, -freq) %&gt;% \n  group_by(position) %&gt;% \n  slice_head(n = 10) %&gt;% ungroup\n\nkable(freq_table_long10, \n      format = \"html\", \n      caption = \"Top 10 letters within each position\") %&gt;%\n    kable_styling() %&gt;%\n    scroll_box(height = \"200px\") %&gt;% \n  kable_classic()\n\n\n\n\nTop 10 letters within each position\n\n\nposition\nalpha\nfreq\n\n\n\n\np1\nS\n365\n\n\np1\nC\n198\n\n\np1\nB\n173\n\n\np1\nT\n149\n\n\np1\nP\n141\n\n\np1\nA\n140\n\n\np1\nF\n135\n\n\np1\nG\n115\n\n\np1\nD\n111\n\n\np1\nM\n107\n\n\np2\nA\n304\n\n\np2\nO\n279\n\n\np2\nR\n267\n\n\np2\nE\n241\n\n\np2\nI\n201\n\n\np2\nL\n200\n\n\np2\nU\n185\n\n\np2\nH\n144\n\n\np2\nN\n87\n\n\np2\nT\n77\n\n\np3\nA\n306\n\n\np3\nI\n266\n\n\np3\nO\n243\n\n\np3\nE\n177\n\n\np3\nU\n165\n\n\np3\nR\n163\n\n\np3\nN\n137\n\n\np3\nL\n112\n\n\np3\nT\n111\n\n\np3\nS\n80\n\n\np4\nE\n318\n\n\np4\nN\n182\n\n\np4\nS\n171\n\n\np4\nA\n162\n\n\np4\nL\n162\n\n\np4\nI\n158\n\n\np4\nC\n150\n\n\np4\nR\n150\n\n\np4\nT\n139\n\n\np4\nO\n132\n\n\np5\nE\n422\n\n\np5\nY\n364\n\n\np5\nT\n253\n\n\np5\nR\n212\n\n\np5\nL\n155\n\n\np5\nH\n137\n\n\np5\nN\n130\n\n\np5\nD\n118\n\n\np5\nK\n113\n\n\np5\nA\n63\n\n\n\n\n\n\n\nSo we have the # of occurences in each position laid out in a tidy format in one long rectangular dataframe. Now sprinkling some magic courtesy ggplot\n\nSide note on reordering within facets\nI tried my best to understand why I was unable to sort within each facet in spite of using free_y. Apparently that‚Äôs a known issue and a workaround has been discussed by David Robinson, Julia Silger and Tyler Rinker. To achieve this, two more functions need to be created reorder_within and scale_y_reordered\n\n\nCode\nreorder_within &lt;- function(x, by, within, fun = mean, sep = \"___\", ...) {\n  new_x &lt;- paste(x, within, sep = sep)\n  stats::reorder(new_x, by, FUN = fun)\n}\n\nscale_y_reordered &lt;- function(..., sep = \"___\") {\n  reg &lt;- paste0(sep, \".+$\")\n  ggplot2::scale_y_discrete(labels = function(x) gsub(reg, \"\", x), ...)\n}\n\nfreq_table_long10 %&gt;% \n  mutate(type = case_when(alpha %in% c(\"A\",\"E\",\"I\",\"O\",\"U\") ~ \"vowel\",\n                          T ~ \"consonant\")) %&gt;% \n  ggplot(aes(y = reorder_within(alpha, freq, position), x = freq))+\n  geom_col(aes(fill = type))+\n  scale_y_reordered()+\n  facet_wrap(~position, \n             scales = \"free_y\", \n             ncol = 5)+\n  labs(x = \"Frequency\", y = \"Letter\",\n       title = \"Frequency of top 10 letters by position in Official Wordle list \",\n       caption = \"D.S.Ramakant Raju\\nwww.linkedin.com/in/dsramakant/\")\n\n\n\n\n\n\n\n\n\nAha! Things are starting to get more clearer. Highly common letters in the 1st position are S, C, B, T and P - notice there‚Äôs only 1 vowel (A) that occurs in the top 10. Vowels appear more frequently in the 2nd and 3rd positions. Last position has a higher occurrence of E, Y, T, R & L"
  },
  {
    "objectID": "posts/2022-04-05-what-a-wonderful-wordle/index.en.html#which-words-can-be-the-best-worlde-openers",
    "href": "posts/2022-04-05-what-a-wonderful-wordle/index.en.html#which-words-can-be-the-best-worlde-openers",
    "title": "What a wonderful ‚Äòwordle‚Äô!",
    "section": "Which words can be the best Worlde openers?",
    "text": "Which words can be the best Worlde openers?\nArmed with the above knowledge, we now can filter out the commonly occurring words. Also I use a naive method to rank these words basis the occurrence of the letters. For instance, in the picture above, the word S A I N T seems to be a valid word comprising of the top occurring letters.\nAdmittedly, I use a pretty crude method to determine the best openers. Known drawbacks of this methodology are:\n\nDoesn‚Äôt consider the future path of the word (number of steps to get to the right word)\nOnly considers the rank of the letters and not the actual probability of occurrence\n\nWith that out of the way, I was able to determine that there are 39 words that can be formed with the top 5 occurring letters in each position. I‚Äôve created a score that is determined by the rank of each letter within its position. For instance, S A I N T gets a score of 9 by summing up 1 (S in first position) + 1 (A in second position) + 2 (I in third) + 2 (N in fourth) + 3 (T in fifth). The lower the score, the higher the frequency of occurrences. Scroll below to read the rest of the words.\n\n\nCode\n#function to pick the top 5 letters\ntop5_selection &lt;- function(x)\n{x %&gt;% arrange(desc(x[2])) %&gt;% head(5) %&gt;% select(1)}\n#defining null table\nfinal_grid &lt;- tibble(ranking = 1:5)\n\nfor(i in 2:length(freq_table)){\n  t &lt;- top5_selection(select(freq_table,1,all_of(i)))\n  final_grid &lt;- cbind(final_grid,t)\n  colnames(final_grid)[i] = paste0(\"p\",i-1)\n}\ntopwords &lt;- position_word_list %&gt;% \nfilter(p1 %in% final_grid$p1,\n       p2 %in% final_grid$p2,\n       p3 %in% final_grid$p3,\n       p4 %in% final_grid$p4,\n       p5 %in% final_grid$p5) \n\n#finding consolidated score of each word\ntopwords %&lt;&gt;%\n  rowwise() %&gt;% \n  mutate(p1_rank = which(p1 == final_grid$p1),\n         p2_rank = which(p2 == final_grid$p2),\n         p3_rank = which(p3 == final_grid$p3),\n         p4_rank = which(p4 == final_grid$p4),\n         p5_rank = which(p5 == final_grid$p5))\n\ntopwords2 &lt;- topwords %&gt;% \n  transmute(word = paste0(p1,p2,p3,p4,p5),\n         score = sum(p1_rank, p2_rank,p3_rank, p4_rank, p5_rank)) %&gt;% \n  arrange(score)\n\nkable(topwords2, \n      format = \"html\",\n      caption = \"Top 39 words\") %&gt;%\n    kable_styling() %&gt;%\n    scroll_box(width = \"50%\", height = \"400px\") %&gt;% \n  kable_classic()\n\n\n\n\nTop 39 words\n\n\nword\nscore\n\n\n\n\nSAINT\n9\n\n\nCRANE\n9\n\n\nCOAST\n11\n\n\nBRINE\n11\n\n\nCEASE\n11\n\n\nCRONE\n11\n\n\nCAUSE\n12\n\n\nCRIER\n12\n\n\nBRINY\n12\n\n\nBOAST\n12\n\n\nTAINT\n12\n\n\nCRONY\n12\n\n\nTEASE\n13\n\n\nPOISE\n13\n\n\nTOAST\n13\n\n\nPAINT\n13\n\n\nBOOST\n14\n\n\nPOINT\n14\n\n\nCOUNT\n14\n\n\nPRONE\n14\n\n\nBEAST\n14\n\n\nPRINT\n15\n\n\nPAUSE\n15\n\n\nTAUNT\n15\n\n\nPROSE\n15\n\n\nCREST\n15\n\n\nCRUST\n16\n\n\nBRIAR\n16\n\n\nBOULE\n16\n\n\nPOESY\n16\n\n\nCRUEL\n16\n\n\nPRUNE\n16\n\n\nBRUNT\n16\n\n\nTRUER\n17\n\n\nTREAT\n18\n\n\nTRIAL\n18\n\n\nTRUST\n18\n\n\nTRULY\n19\n\n\nTROLL\n20\n\n\n\n\n\n\n\nThere we have it. My take on the best opening words.\nI‚Äôve used words such as SAINT, CRANE, COAST etc and they‚Äôve been reasonably useful to me.\nWhich are your favourite opening words? Please do leave a comment to let me know!"
  },
  {
    "objectID": "posts/2022-01-14-hello-world2/index.en.html",
    "href": "posts/2022-01-14-hello-world2/index.en.html",
    "title": "Hello World2",
    "section": "",
    "text": "A lot of this is courtesy of Apres Hill‚Äôs blog\nhttps://www.apreshill.com/blog/2020-12-new-year-new-blogdown/#step-1-create-repo"
  },
  {
    "objectID": "posts/2022-06-01-tidytuesday-nyt-bestsellers-list/index.en.html",
    "href": "posts/2022-06-01-tidytuesday-nyt-bestsellers-list/index.en.html",
    "title": "TidyTuesday NYT Bestsellers list",
    "section": "",
    "text": "‚ÄúLearn by practice!‚Äù is a maxim that every coder/analyst agrees upon. One of the admirable initiatives by the R/ RStudio community is Tidy Tuesday - every week a new dataset is released for enthusiasts to dig into. A few days back, an interesting dataset caught my eye - NYT‚Äôs Bestsellers List from 1930 to 2021. This one was particularly unique as it mirrored a lot of projects that I‚Äôve been doing on the OTT side as well. So I cracked my knuckles and jumped right in!"
  },
  {
    "objectID": "posts/2022-06-01-tidytuesday-nyt-bestsellers-list/index.en.html#tidy-tuesday-week-19-2022",
    "href": "posts/2022-06-01-tidytuesday-nyt-bestsellers-list/index.en.html#tidy-tuesday-week-19-2022",
    "title": "TidyTuesday NYT Bestsellers list",
    "section": "",
    "text": "‚ÄúLearn by practice!‚Äù is a maxim that every coder/analyst agrees upon. One of the admirable initiatives by the R/ RStudio community is Tidy Tuesday - every week a new dataset is released for enthusiasts to dig into. A few days back, an interesting dataset caught my eye - NYT‚Äôs Bestsellers List from 1930 to 2021. This one was particularly unique as it mirrored a lot of projects that I‚Äôve been doing on the OTT side as well. So I cracked my knuckles and jumped right in!"
  },
  {
    "objectID": "posts/2022-06-01-tidytuesday-nyt-bestsellers-list/index.en.html#objective",
    "href": "posts/2022-06-01-tidytuesday-nyt-bestsellers-list/index.en.html#objective",
    "title": "TidyTuesday NYT Bestsellers list",
    "section": "Objective",
    "text": "Objective\n\nUnderstanding longevity & seasonality of how books track on the NYT bestseller‚Äôs list\nDeeper understanding of using customizing themes and fonts on the ggplot package\n\n\nLoading the data\nStarting off by loading the data and the libraries\n\n\nCode\nknitr::opts_chunk$set(warning = FALSE, message = FALSE) \nsuppressMessages({\n  library(tidyverse)\n  library(scales)\n  library(knitr)\n  library(tidytuesdayR)\n  library(forcats)\n  library(lubridate)\n  library(RColorBrewer)\n})\n\ntt_raw &lt;- tt_load(\"2022-05-10\")\n\n\n--- Compiling #TidyTuesday Information for 2022-05-10 ----\n\n\n--- There are 2 files available ---\n\n\n--- Starting Download ---\n\n\n\n    Downloading file 1 of 2: `nyt_titles.tsv`\n    Downloading file 2 of 2: `nyt_full.tsv`\n\n\n--- Download complete ---\n\n\nCode\ndata &lt;- tt_raw$nyt_titles\n\n\nWhat‚Äôs in the Tidy Tuesday dataset?\n\n\nCode\nglimpse(data)\n\n\nRows: 7,431\nColumns: 8\n$ id          &lt;dbl&gt; 0, 1, 10, 100, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1‚Ä¶\n$ title       &lt;chr&gt; \"\\\"H\\\" IS FOR HOMICIDE\", \"\\\"I\\\" IS FOR INNOCENT\", \"''G'' I‚Ä¶\n$ author      &lt;chr&gt; \"Sue Grafton\", \"Sue Grafton\", \"Sue Grafton\", \"W. Bruce Cam‚Ä¶\n$ year        &lt;dbl&gt; 1991, 1992, 1990, 2012, 2006, 2016, 1985, 1994, 2002, 1999‚Ä¶\n$ total_weeks &lt;dbl&gt; 15, 11, 6, 1, 1, 3, 16, 5, 4, 1, 3, 2, 11, 6, 9, 8, 1, 1, ‚Ä¶\n$ first_week  &lt;date&gt; 1991-05-05, 1992-04-26, 1990-05-06, 2012-05-27, 2006-02-1‚Ä¶\n$ debut_rank  &lt;dbl&gt; 1, 14, 4, 3, 11, 1, 9, 7, 7, 12, 13, 5, 12, 2, 11, 13, 2, ‚Ä¶\n$ best_rank   &lt;dbl&gt; 2, 2, 8, 14, 14, 7, 2, 10, 12, 17, 13, 13, 8, 5, 5, 11, 4,‚Ä¶\n\n\n\n\nExploratory Data Analysis (sort of)\nQuick EDA tells us that there the number of books in the #1 spot each year during the 50s have been increasing while the number of weeks they‚Äôve spent on the NYT list has been decreasing. 2020-21 is excluded as I‚Äôm breaking up the period into decades for easy analysis\n\n\nCode\ndata %&gt;% \n  mutate(decade = factor(10*year %/% 10)) %&gt;% \n  filter(best_rank==1, year&lt;2020) %&gt;% \n  group_by(decade) %&gt;% \n  summarise(avg_weeks = mean(total_weeks),\n            no_of_rank1 = n_distinct(title))\n\n\n# A tibble: 9 √ó 3\n  decade avg_weeks no_of_rank1\n  &lt;fct&gt;      &lt;dbl&gt;       &lt;int&gt;\n1 1930        17.1          74\n2 1940        30.1          59\n3 1950        52.4          35\n4 1960        45.7          31\n5 1970        38.6          46\n6 1980        29.5          78\n7 1990        25.7          99\n8 2000        12.5         220\n9 2010        10.3         306\n\n\nThis is a fantastic starting point. Intuitively, this makes a lot of sense. There‚Äôs far more competition for the #1 spot in the last 20 years which is driving down the longevity. Compare the 50‚Äôs to the 2010‚Äôs and the trend is hard to miss. This table is only for the books that made it to the #1 position. But how about the rest of the other books? A visual representation draws the same conclusion more elegantly.\n\n\nVisualising Longevity\nHat-tip to a few outstanding viz I came across while researching the NYT theme. Bob Rudis‚Äô Supreme Annotations and Rahul Sangole‚Äôs Visualizing Correlations\n\n\nCode\n#loading fonts that resemble the NYT viz\n#inspired by https://rud.is/b/2016/03/16/supreme-annotations/\n\nlibrary(showtext)\nshowtext_auto()\nfont_add(family = \"Open Sans\", \n         regular = \"OpenSans-CondLight.ttf\", \n         italic = \"OpenSans-CondLightItalic.ttf\", \n         bold = \"OpenSans-CondBold.ttf\")\n\n#changing facet labels as shown here \n#https://ggplot2.tidyverse.org/reference/as_labeller.html\nfacet_labels &lt;- as_labeller(c(`1930`= \"1930 to 1939\",\n                              `1940`= \"1940 to 1949\",\n                              `1950`= \"1950 to 1959\",\n                              `1960`= \"1960 to 1969\",\n                              `1970`= \"1970 to 1979\",\n                              `1980`= \"1980 to 1989\",\n                              `1990`= \"1990 to 1999\",\n                              `2000`= \"2000 to 2009\",\n                              `2010`= \"2010 to 2019\"))\n\n\n#annotations for individual facet as discussed here https://stackoverflow.com/a/11889798/7938068\nannot_x &lt;- data.frame(debut_rank = 5, \n                      total_weeks = 111,\n                      lab = \"Each dot\\n is a book\",\n                      decade = 1940)\n\ngraph1 &lt;- data %&gt;% \n  filter(best_rank==1,year&lt;2020) %&gt;% \n  mutate(decade = factor(10*year %/% 10)) %&gt;% \n  ggplot(aes(x = debut_rank, y  = total_weeks))+\n  geom_point(aes(color = decade, group = debut_rank))+\n  facet_grid(~decade ,labeller = facet_labels)\n\ngraph1 &lt;- graph1+\n  theme_minimal(base_family = \"Open Sans\")+\n  scale_color_brewer(palette = \"Paired\")+\n  labs(title = \"Longevity of NYT bestsellers has been decreasing\", \n       subtitle = \"Analysis of books that reached highest of #1 on the NYT chart tells us that starting from the 1950s, the bestsellers have reduced their longevity - or time spent on the chart.\\nFor instance, the top ranked books released in the 50s spent around 52 weeks on the chart while in contrast by the 2010s, they only spent 10 weeks.\",\n       caption = \"TidyTuesday Week 19, 2022\\n Prepared by D.S.Ramakant Raju (www.ds-ramakant.com)\",\n       x = \"Rank of title on debut week\",\n       y = \"Number of weeks on the bestsellers list\")+\n  theme(panel.border = element_rect(color = \"#2b2b2b\", \n                                    fill = NA), #borders for each facet panel\n        legend.position = \"none\", #removing legend\n        strip.text = element_text(face = \"italic\"),\n        plot.title = element_text(size = 14, face = \"bold\"),\n        panel.grid.major.x = element_line(linetype = \"dotted\", \n                                          color = \"black\"),\n        panel.grid.minor.x = element_blank(),\n        panel.grid.minor.y = element_blank(),\n        plot.caption = element_text(size = 12),\n        plot.subtitle = element_text(size = 12)) +\n  scale_y_continuous(breaks = seq(from = 25, to = 175, by = 25))+\n  #annotations by default is applied to all facets\n  #for individual facet annotations, check https://stackoverflow.com/a/11889798/7938068\n  geom_text(data = annot_x, \n            aes(x = debut_rank, y = total_weeks, \n                family = \"Open Sans\", alpha = 0.8,\n                hjust = -0.2, vjust = -0.2),\n            label = annot_x$lab\n            )\n\n\n\nprint(graph1)\n\n\n\n\n\n\n\n\n\n\n\nVisualising Seasonality\nNow lets look at seasonality - is there any trend as far as the launch month is concerned? For the sake of analysis, I‚Äôve truncated the analysis period to 2010 onwards to keep it more relevant and exlcude irrelvant historical data.\n\n\nCode\ngraph2 &lt;- data %&gt;% \n  filter(best_rank&lt;11, year&gt; 2010) %&gt;% \n  mutate(month = month(first_week, label = T),\n         stage = case_when(year&lt;=2015 ~ \"2011-2015\",\n                           year&gt; 2015 ~ \"2016-2020\",\n                           T ~ \"x\")) %&gt;% \n  group_by(stage,year,month) %&gt;% \n  summarise(n = n_distinct(title)) %&gt;% \n  mutate(all_titles = ave(n, year, FUN = sum),\n         pct = n/all_titles) %&gt;%  \n  ggplot(aes(x = month, y = pct, group = 1))+\n  geom_point(size = 2, \n             alpha = 0.5, position = \"jitter\")+\n  geom_smooth(se = T) \n\n\ngraph2 &lt;- graph2+\n  theme_minimal(base_family = \"Open Sans\")+\n  scale_color_brewer(palette = \"Paired\")+\n  labs(title = \"Monthly seasonality of books that featured in the top 10 of NYT Bestsellers list (2010-2021)\", \n       subtitle = \"Books launched in Summer (Apr-May) or Fall (Sep-Oct) were more likely to make it feature in the top 10\",\n       caption = \"TidyTuesday Week 19, 2022\\n Prepared by D.S.Ramakant Raju, www.ds-ramakant.com\",\n       x = \"Months (2010-2019)\",\n       y = \"%age of books launched within that year\")+\n  scale_y_continuous(labels = label_percent(accuracy = 1),\n                     breaks = seq(from = 0, to = 0.2, by= 0.05), \n                     limits = c(0,0.15))+\n  theme(axis.line.x = element_line(color = \"grey\"),\n        panel.grid.minor.y = element_blank())\n\ngraph2\n\n\n\n\n\n\n\n\n\nThis is a fairly straightforward and replicable analysis. If you‚Äôre a #TidyTuesday fan please feel free to share your work in the comments below"
  },
  {
    "objectID": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html",
    "href": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html",
    "title": "Day 14 of #50daysofKaggle",
    "section": "",
    "text": "How hard is it to evaluate multiple models when working on a given data problem?\nIf you‚Äôre using the tidymodels package, the answer is surprisingly simple.\nToday‚Äôs post is an attempt to use the tidymodels framework to screen multiple models. Inspiration for this post comes from the Ch 15 of the Tidymodels with R textbook along with two more noteworthy blogs\nI‚Äôm skipping the EDA component as I‚Äôve covered it in the previous posts. Moving on to some boring (but very necessary) sections.\nAs always, please feel free to leave a comment using the side-barüëâüèº"
  },
  {
    "objectID": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html#cleaning-data",
    "href": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html#cleaning-data",
    "title": "Day 14 of #50daysofKaggle",
    "section": "Cleaning data",
    "text": "Cleaning data\nChecking for NA values in the full df tells us that Age column has 86 missing in test & 177 missing values in train while there‚Äôs 1 missing NA value in Fare. The 418 missing values in Survived in test are the ones we need to predict.\n\n\nCode\ntitanic_data %&gt;% \n  group_by(source) %&gt;% \n  summarise_all(~ sum(is.na(.)))\n\n\n# A tibble: 2 √ó 13\n  source Passe‚Ä¶¬π Survi‚Ä¶¬≤ Pclass  Name   Sex   Age SibSp Parch Ticket  Fare Cabin\n  &lt;chr&gt;    &lt;int&gt;   &lt;int&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1 test         0     418      0     0     0    86     0     0      0     1     0\n2 train        0       0      0     0     0   177     0     0      0     0     0\n# ‚Ä¶ with 1 more variable: Embarked &lt;int&gt;, and abbreviated variable names\n#   ¬π‚ÄãPassengerId, ¬≤‚ÄãSurvived\n\n\n\nJanitor::clean_names()\nNow i‚Äôve got a thing about keeping column names clean so summoning janitor with a magic wand:\n\n\nCode\ntitanic_data &lt;- titanic_data %&gt;% \n  mutate(family_count = SibSp+Parch+1) %&gt;% \n  janitor::clean_names()\nnames(titanic_data)\n\n\n [1] \"passenger_id\" \"survived\"     \"pclass\"       \"name\"         \"sex\"         \n [6] \"age\"          \"sib_sp\"       \"parch\"        \"ticket\"       \"fare\"        \n[11] \"cabin\"        \"embarked\"     \"source\"       \"family_count\"\n\n\nVoila. Everything now in lower case and snake case!\n\n\nImputing NA values in embarked\nNow under embarked there are two rows that don‚Äôt have NA but are blank. That‚Äôs a bit oddü§®\n\n\nCode\ntitanic_data %&gt;% \n  count(embarked, sort = T)\n\n\n  embarked   n\n1        S 914\n2        C 270\n3        Q 123\n4            2\n\n\nSince it is only 2 such rows, I‚Äôll be replacing the NA values with the most repeated embarked value. So now zero empty values in embarked\n\n\nCode\nmode_embarked &lt;- titanic_data %&gt;% \n  count(embarked, sort = T) %&gt;% \n  select(embarked) %&gt;% \n  head(1) %&gt;% \n#this beautiful func comes from the purrr package. equivalent of .[[1]]\n  pluck(1)\n\ntitanic_data &lt;- titanic_data %&gt;% \n  mutate(embarked = if_else(embarked == \"\", mode_embarked, embarked))\n\ntitanic_data %&gt;% \n  count(embarked, sort = T)\n\n\n  embarked   n\n1        S 916\n2        C 270\n3        Q 123\n\n\n\n\nImputing NA values in age\nThe age column has a bunch of missing values. My approach today is to substitute them with the median values when grouped by sex and class. Here‚Äôs a function written to impute within the test and train data accordingly.\nnote: Feature engineering functions can be addressed in the tidymodels framework at recipe stage.\n\n\nCode\nmedian_age_calc &lt;- function(df){\n  median_ages &lt;- df %&gt;% \n    group_by(sex, pclass) %&gt;% \n    summarise(age = median(age,na.rm = T))\n  df %&gt;%\n    mutate(age = case_when((sex ==\"male\" & pclass ==1 & is.na(age)) ~ median_ages$age[1],\n                           (sex ==\"male\" & pclass ==2 & is.na(age)) ~ median_ages$age[2],\n                           (sex ==\"male\" & pclass ==3 & is.na(age)) ~ median_ages$age[3],\n                           (sex ==\"female\" & pclass ==1 & is.na(age)) ~ median_ages$age[4],\n                           (sex ==\"female\" & pclass ==2 & is.na(age)) ~ median_ages$age[5],\n                           (sex ==\"female\" & pclass ==3 & is.na(age)) ~ median_ages$age[6],\n                           .default = age)\n    )\n}\n\ntitanic_data &lt;- titanic_data %&gt;% \n  median_age_calc() %&gt;% ungroup()\n\n\nAre there any na values in the titanic data now?\n\n\nCode\ntitanic_data %&gt;% \n  select(source, survived, sex, pclass, fare, age) %&gt;% \n  group_by(source) %&gt;% \n  summarise_all(~ sum(is.na(.)))\n\n\n# A tibble: 2 √ó 6\n  source survived   sex pclass  fare   age\n  &lt;chr&gt;     &lt;int&gt; &lt;int&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1 test        418     0      0     1     0\n2 train         0     0      0     0     0\n\n\nPhew. So age is covered but one NA in fare. What are the median age values now?\n\n\nCode\ntitanic_data %&gt;% \n  group_by(pclass, sex) %&gt;%\n  summarise(median_age = median(age))\n\n\n# A tibble: 6 √ó 3\n# Groups:   pclass [3]\n  pclass sex    median_age\n   &lt;int&gt; &lt;chr&gt;       &lt;dbl&gt;\n1      1 female       37.5\n2      1 male         37  \n3      2 female       28  \n4      2 male         28  \n5      3 female       25  \n6      3 male         22  \n\n\n\n\nImputing NA values for fare\nThere‚Äôs this one person (passenger_id = 1044, male and pclass = 3) who has a na value in fare from test data. Replacing it with the median value.\n\n\nCode\ntitanic_data %&gt;% \n  select(source, name, passenger_id, sex, age, pclass, fare) %&gt;% \n  filter(is.na(fare))\n\n\n  source               name passenger_id  sex  age pclass fare\n1   test Storey, Mr. Thomas         1044 male 60.5      3   NA\n\n\n\n\nCode\ntitanic_data %&gt;% \n  group_by(sex, pclass) %&gt;% \n  summarise(median_fare = median(fare, na.rm = T))\n\n\n# A tibble: 6 √ó 3\n# Groups:   sex [2]\n  sex    pclass median_fare\n  &lt;chr&gt;   &lt;int&gt;       &lt;dbl&gt;\n1 female      1       80.9 \n2 female      2       23   \n3 female      3       10.5 \n4 male        1       49.5 \n5 male        2       13   \n6 male        3        7.90\n\n\nReplacing in the main df\n\n\nCode\ntitanic_data &lt;- titanic_data %&gt;% \n  mutate(fare = if_else(is.na(fare), \n#from the above table. Urgh!! hard coding for that 1 guy!! how inelegant.\n                        7.8958, age))\n\n\nchecking if has been replaced.\n\n\nCode\ntitanic_data %&gt;% \n  select(source, name, passenger_id, sex, age, pclass, fare) %&gt;% \n  filter(is.na(fare))\n\n\n[1] source       name         passenger_id sex          age         \n[6] pclass       fare        \n&lt;0 rows&gt; (or 0-length row.names)"
  },
  {
    "objectID": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html#splitting-names",
    "href": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html#splitting-names",
    "title": "Day 14 of #50daysofKaggle",
    "section": "Splitting names",
    "text": "Splitting names\nWith absolutely no guilt, I confess that this took me almost an entire day to figure out. And am I glad to have done it. The best thing about the tidyverse approach is the onus on making readable data. For that I‚Äôm grateful to discover functions like seperate_wider_regex.\nEssentially, it is a delimiter that breaks up columns based on the string patterns. So neat!\n\n\nCode\nnames_with_splchar &lt;- regex(\"[A-Za-z]+[\\\\'\\\\-\\\\s]+[A-Za-z]+\")\nnames_with_3words &lt;- regex(\"[A-Za-z]+\\\\s[A-Za-z]+\\\\s[A-Za-z]+\")\nnames_with_1word &lt;- regex(\"[A-Za-z]+\") \nnames_with_2words &lt;- regex(\"[A-Za-z]+\\\\s+[A-Za-z]+\") # for 'the countess'\n\n\ntitanic_data &lt;- titanic_data %&gt;% \n  separate_wider_regex(\n    name, \n    patterns = c(\n#IMP: ordering of regex patterns changes the outcome\n      surname = str_c(c(names_with_splchar, \n                        names_with_3words,\n                        names_with_1word), \n                      collapse = \"|\"),    # picks the first word before comma\n      \", \",                               # the comma  \n#IMP: ordering of regex patterns changes the outcome\n      title = str_c(c(names_with_2words , # two words with special char in between like 'the countess'\n                      names_with_1word),  # one word such as Mr Miss Mrs etc\n                    collapse = \"|\"),      \n      \". \",                               # the dot\n      given_name = \".+\"),                 # picks anything else which occurs at least once\n    cols_remove = F                       # retains the original column    \n  ) \n\ntitanic_data %&gt;% \n  select(name, title, surname, given_name) %&gt;% \n  head(10)\n\n\n# A tibble: 10 √ó 4\n   name                                                title  surname   given_‚Ä¶¬π\n   &lt;chr&gt;                                               &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;   \n 1 Braund, Mr. Owen Harris                             Mr     Braund    Owen Ha‚Ä¶\n 2 Cumings, Mrs. John Bradley (Florence Briggs Thayer) Mrs    Cumings   John Br‚Ä¶\n 3 Heikkinen, Miss. Laina                              Miss   Heikkinen Laina   \n 4 Futrelle, Mrs. Jacques Heath (Lily May Peel)        Mrs    Futrelle  Jacques‚Ä¶\n 5 Allen, Mr. William Henry                            Mr     Allen     William‚Ä¶\n 6 Moran, Mr. James                                    Mr     Moran     James   \n 7 McCarthy, Mr. Timothy J                             Mr     McCarthy  Timothy‚Ä¶\n 8 Palsson, Master. Gosta Leonard                      Master Palsson   Gosta L‚Ä¶\n 9 Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)   Mrs    Johnson   Oscar W‚Ä¶\n10 Nasser, Mrs. Nicholas (Adele Achem)                 Mrs    Nasser    Nichola‚Ä¶\n# ‚Ä¶ with abbreviated variable name ¬π‚Äãgiven_name\n\n\nWhat is the break-up of titles now?\n\n\nCode\ntitanic_data %&gt;% \n  count(title, sort= T)\n\n\n# A tibble: 18 √ó 2\n   title            n\n   &lt;chr&gt;        &lt;int&gt;\n 1 Mr             757\n 2 Miss           260\n 3 Mrs            197\n 4 Master          61\n 5 Dr               8\n 6 Rev              8\n 7 Col              4\n 8 Major            2\n 9 Mlle             2\n10 Ms               2\n11 Capt             1\n12 Don              1\n13 Dona             1\n14 Jonkheer         1\n15 Lady             1\n16 Mme              1\n17 Sir              1\n18 the Countess     1"
  },
  {
    "objectID": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html#creating-a-custom-grouping",
    "href": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html#creating-a-custom-grouping",
    "title": "Day 14 of #50daysofKaggle",
    "section": "Creating a custom grouping",
    "text": "Creating a custom grouping\nThe ticket is going to be broken up into ticket_tail (the last character) and ticket_head (all but the last character). Then we merge surname and ticket_head to create a group_id\n\n\nCode\ntitanic_data &lt;- titanic_data %&gt;% \n  mutate(ticket_head = substr(ticket, 1, nchar(ticket)-1),\n         ticket_tail = substr(ticket, nchar(ticket), nchar(ticket)),\n         group_id = paste0(surname, \"_\", ticket_head)) \n\n\nCreating columns that indicate the number of people and other flags\n\n\nCode\ntitanic_data &lt;- titanic_data %&gt;% \n  add_count(group_id) %&gt;% rename(pax_in_group = n)\n\ntitanic_data &lt;- titanic_data %&gt;% \n  mutate(flag = case_when ((family_count==1 & pax_in_group==1) ~ \"1_solo\",\n                           family_count == pax_in_group ~ \"2_family_full\",\n                           !(family_count == pax_in_group) ~ \"3_clubbed\",\n                           .default = \"x\"))\n\ntitanic_data &lt;- titanic_data %&gt;% \n  add_count(ticket_head) %&gt;% \n  rename(pax_in_ticket_head = n)\n\n# how many instances of the same ticket having multiple groups? \ntitanic_data &lt;- titanic_data %&gt;% \n  group_by(ticket) %&gt;% \n  mutate(groups_in_ticket = n_distinct(group_id)) %&gt;% ungroup()\n\n# which tickets that have more than 1 groups in them? \n#     these passengers will have ticket_grouping precedence as they may include \n#     nannies, relatives & friends that don't share the same surname\nticket_with_multiple_groups &lt;- titanic_data %&gt;% \n  filter(!groups_in_ticket==1) %&gt;% \n  count(ticket, sort = T)\n\ntitanic_data &lt;- titanic_data %&gt;% \n  mutate(final_grouping = if_else(ticket %in% ticket_with_multiple_groups$ticket, \n                             ticket, group_id),\n         final_label = if_else(ticket %in% ticket_with_multiple_groups$ticket,\n                         \"4_ticket_grouping\", flag)) \n\n\nSince the code now has become a bit too long and I‚Äôm creating a checkpost here with a new df called titanic_data2\n\n\nCode\ntitanic_data2 &lt;- titanic_data %&gt;% \n  select(source, passenger_id, survived,  sex, age, fare, pclass, embarked, \n         family_count, pax_in_group, pax_in_ticket_head, groups_in_ticket,\n         final_grouping) %&gt;% \n  mutate(final_grouping = as_factor(final_grouping),\n         survived = as_factor(survived),\n         pclass = as_factor(pclass),\n         sex = as_factor(sex),\n         embarked = as_factor(embarked))\nglimpse(titanic_data2)\n\n\nRows: 1,309\nColumns: 13\n$ source             &lt;chr&gt; \"train\", \"train\", \"train\", \"train\", \"train\", \"train‚Ä¶\n$ passenger_id       &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, ‚Ä¶\n$ survived           &lt;fct&gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, ‚Ä¶\n$ sex                &lt;fct&gt; male, female, female, female, male, male, male, mal‚Ä¶\n$ age                &lt;dbl&gt; 22, 38, 26, 35, 35, 22, 54, 2, 27, 14, 4, 58, 20, 3‚Ä¶\n$ fare               &lt;dbl&gt; 22, 38, 26, 35, 35, 22, 54, 2, 27, 14, 4, 58, 20, 3‚Ä¶\n$ pclass             &lt;fct&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, ‚Ä¶\n$ embarked           &lt;fct&gt; S, C, S, S, S, Q, S, S, S, C, S, S, S, S, S, S, Q, ‚Ä¶\n$ family_count       &lt;dbl&gt; 2, 2, 1, 2, 1, 1, 1, 5, 3, 2, 3, 1, 1, 7, 1, 1, 6, ‚Ä¶\n$ pax_in_group       &lt;int&gt; 1, 2, 1, 2, 1, 1, 1, 5, 3, 2, 3, 1, 1, 7, 1, 1, 6, ‚Ä¶\n$ pax_in_ticket_head &lt;int&gt; 5, 14, 2, 8, 1, 1, 6, 5, 4, 4, 3, 14, 1, 22, 8, 1, ‚Ä¶\n$ groups_in_ticket   &lt;int&gt; 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ‚Ä¶\n$ final_grouping     &lt;fct&gt; Braund_A/5 2117, Cumings_PC 1759, Heikkinen_STON/O2‚Ä¶\n\n\nNow this is the part I don‚Äôt get. Why does Kaggle call it train and test when it can be easily told to be given_data and to_predict data?\n\n\nCode\nto_predict &lt;- titanic_data2 %&gt;% \n  filter(source == \"test\") %&gt;% select(-survived, -source)\ngiven_data &lt;- titanic_data2 %&gt;% \n  filter(source == \"train\") %&gt;% select(-source)\n\n\ngiven_data has all the necessary columns we need to train the algo on\n\n\nCode\nglimpse(given_data)\n\n\nRows: 891\nColumns: 12\n$ passenger_id       &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, ‚Ä¶\n$ survived           &lt;fct&gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, ‚Ä¶\n$ sex                &lt;fct&gt; male, female, female, female, male, male, male, mal‚Ä¶\n$ age                &lt;dbl&gt; 22, 38, 26, 35, 35, 22, 54, 2, 27, 14, 4, 58, 20, 3‚Ä¶\n$ fare               &lt;dbl&gt; 22, 38, 26, 35, 35, 22, 54, 2, 27, 14, 4, 58, 20, 3‚Ä¶\n$ pclass             &lt;fct&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, ‚Ä¶\n$ embarked           &lt;fct&gt; S, C, S, S, S, Q, S, S, S, C, S, S, S, S, S, S, Q, ‚Ä¶\n$ family_count       &lt;dbl&gt; 2, 2, 1, 2, 1, 1, 1, 5, 3, 2, 3, 1, 1, 7, 1, 1, 6, ‚Ä¶\n$ pax_in_group       &lt;int&gt; 1, 2, 1, 2, 1, 1, 1, 5, 3, 2, 3, 1, 1, 7, 1, 1, 6, ‚Ä¶\n$ pax_in_ticket_head &lt;int&gt; 5, 14, 2, 8, 1, 1, 6, 5, 4, 4, 3, 14, 1, 22, 8, 1, ‚Ä¶\n$ groups_in_ticket   &lt;int&gt; 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ‚Ä¶\n$ final_grouping     &lt;fct&gt; Braund_A/5 2117, Cumings_PC 1759, Heikkinen_STON/O2‚Ä¶\n\n\nto_predict dataframe has everything else except survived column that needs to be predicted.\n\n\nCode\nglimpse(to_predict)\n\n\nRows: 418\nColumns: 11\n$ passenger_id       &lt;int&gt; 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 9‚Ä¶\n$ sex                &lt;fct&gt; male, female, male, male, female, male, female, mal‚Ä¶\n$ age                &lt;dbl&gt; 34.5, 47.0, 62.0, 27.0, 22.0, 14.0, 30.0, 26.0, 18.‚Ä¶\n$ fare               &lt;dbl&gt; 34.5, 47.0, 62.0, 27.0, 22.0, 14.0, 30.0, 26.0, 18.‚Ä¶\n$ pclass             &lt;fct&gt; 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 1, 1, 2, 1, 2, 2, ‚Ä¶\n$ embarked           &lt;fct&gt; Q, S, Q, S, S, S, Q, S, C, S, S, S, S, S, S, C, Q, ‚Ä¶\n$ family_count       &lt;dbl&gt; 1, 2, 1, 1, 3, 1, 1, 3, 1, 3, 1, 1, 2, 2, 2, 2, 1, ‚Ä¶\n$ pax_in_group       &lt;int&gt; 1, 1, 1, 1, 2, 1, 1, 3, 1, 3, 1, 1, 2, 2, 2, 2, 1, ‚Ä¶\n$ pax_in_ticket_head &lt;int&gt; 3, 1, 1, 6, 11, 3, 3, 6, 16, 4, 10, 3, 2, 2, 2, 4, ‚Ä¶\n$ groups_in_ticket   &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, ‚Ä¶\n$ final_grouping     &lt;fct&gt; Kelly_33091, Wilkes_36327, Myles_24027, Wirz_31515,‚Ä¶\n\n\nPhew. finally done with the pre-processing. Thanks for sticking around. Here have a gif."
  },
  {
    "objectID": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html#step-1-creating-resampling-folds",
    "href": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html#step-1-creating-resampling-folds",
    "title": "Day 14 of #50daysofKaggle",
    "section": "Step 1: creating resampling folds",
    "text": "Step 1: creating resampling folds\n\n\nCode\nset.seed(2023)\ntitanic_folds &lt;- bootstraps(data = given_data, \n                            times = 15)\ntitanic_folds\n\n\n# Bootstrap sampling \n# A tibble: 15 √ó 2\n   splits            id         \n   &lt;list&gt;            &lt;chr&gt;      \n 1 &lt;split [891/318]&gt; Bootstrap01\n 2 &lt;split [891/313]&gt; Bootstrap02\n 3 &lt;split [891/340]&gt; Bootstrap03\n 4 &lt;split [891/313]&gt; Bootstrap04\n 5 &lt;split [891/326]&gt; Bootstrap05\n 6 &lt;split [891/342]&gt; Bootstrap06\n 7 &lt;split [891/316]&gt; Bootstrap07\n 8 &lt;split [891/323]&gt; Bootstrap08\n 9 &lt;split [891/338]&gt; Bootstrap09\n10 &lt;split [891/331]&gt; Bootstrap10\n11 &lt;split [891/319]&gt; Bootstrap11\n12 &lt;split [891/322]&gt; Bootstrap12\n13 &lt;split [891/340]&gt; Bootstrap13\n14 &lt;split [891/321]&gt; Bootstrap14\n15 &lt;split [891/320]&gt; Bootstrap15"
  },
  {
    "objectID": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html#step-2-recipe",
    "href": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html#step-2-recipe",
    "title": "Day 14 of #50daysofKaggle",
    "section": "Step 2: Recipe",
    "text": "Step 2: Recipe\nCreating the base_recipe object that has only 3 steps\n\n\nCode\nbase_recipe &lt;- recipe(survived ~ ., data = given_data) %&gt;% \n  update_role(passenger_id, new_role = \"id_variable\") %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors()) \nbase_recipe\n\n\nOr if one would like to see it in a tidy format.\n\n\nCode\ntidy(base_recipe)\n\n\n# A tibble: 2 √ó 6\n  number operation type      trained skip  id             \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;          \n1      1 step      dummy     FALSE   FALSE dummy_FSxXz    \n2      2 step      normalize FALSE   FALSE normalize_bu1Ti"
  },
  {
    "objectID": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html#step-3-model-definitions",
    "href": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html#step-3-model-definitions",
    "title": "Day 14 of #50daysofKaggle",
    "section": "Step 3: Model definitions",
    "text": "Step 3: Model definitions\nAll the four models are defined as:\n\n\nCode\n#logistic regression\nglm_model &lt;- logistic_reg() %&gt;% \n  set_engine(\"glm\") %&gt;% \n  set_mode(\"classification\")\n\n#random forest\nrf_model &lt;- rand_forest(trees = 1000) %&gt;% \n  set_engine(\"ranger\") %&gt;% \n  set_mode(\"classification\")\n\n#support vector machines\nsvm_model &lt;- svm_rbf() %&gt;% # rbf - radial based\n  set_engine(\"kernlab\") %&gt;% \n  set_mode(\"classification\")\n\n#decision tree\ndt_model &lt;- decision_tree(mode = \"classification\", \n                          tree_depth = 3) %&gt;% \n  set_engine(\"rpart\")"
  },
  {
    "objectID": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html#step-4-workflow-set",
    "href": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html#step-4-workflow-set",
    "title": "Day 14 of #50daysofKaggle",
    "section": "Step 4: Workflow set",
    "text": "Step 4: Workflow set\nJust as you would with a single model, the workflow combines the base recipe with the multiple models by using lists when declaring the object.\n\n\nCode\ntitanic_wf_set &lt;- workflow_set(\n  list(base_recipe),\n  list(glm_model, rf_model, svm_model, dt_model),\n  cross = T\n)\ntitanic_wf_set\n\n\n# A workflow set/tibble: 4 √ó 4\n  wflow_id             info             option    result    \n  &lt;chr&gt;                &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n1 recipe_logistic_reg  &lt;tibble [1 √ó 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n2 recipe_rand_forest   &lt;tibble [1 √ó 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n3 recipe_svm_rbf       &lt;tibble [1 √ó 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n4 recipe_decision_tree &lt;tibble [1 √ó 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n\n\nIn the table above, the result column shows a list[0] implying that it is currently empty. This is because till now, we‚Äôve only defined the workflows and models but are yet to pass the data though it.\nObligatory shout out to the phenom Julia Silge whose YT tutorials and blogs have been my learning books at each step. These steps are succinctly explained in her Tidy Tuesday post which I highly recommend."
  },
  {
    "objectID": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html#step-5-fitting-on-resampled-folds",
    "href": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html#step-5-fitting-on-resampled-folds",
    "title": "Day 14 of #50daysofKaggle",
    "section": "Step 5: Fitting on resampled folds",
    "text": "Step 5: Fitting on resampled folds\nSo this part of it needs to be dealt with care. Resampling may take a while‚Ä¶\n\n\nCode\nstart_time &lt;- Sys.time()\nset.seed(2023)\ndoParallel::registerDoParallel()\ntitanic_rs &lt;- workflow_map(\n  titanic_wf_set,\n  \"fit_resamples\",\n  resamples = titanic_folds\n)\nend_time &lt;- Sys.time()\n\n\n‚Ä¶ which is ‚Ä¶\n\n\nCode\nend_time - start_time\n\n\nTime difference of 10.71708 mins\n\n\nHmm‚Ä¶ So moving on. What does the workflow_set object look like now?\n\n\nCode\ntitanic_rs\n\n\n# A workflow set/tibble: 4 √ó 4\n  wflow_id             info             option    result   \n  &lt;chr&gt;                &lt;list&gt;           &lt;list&gt;    &lt;list&gt;   \n1 recipe_logistic_reg  &lt;tibble [1 √ó 4]&gt; &lt;opts[1]&gt; &lt;rsmp[+]&gt;\n2 recipe_rand_forest   &lt;tibble [1 √ó 4]&gt; &lt;opts[1]&gt; &lt;rsmp[+]&gt;\n3 recipe_svm_rbf       &lt;tibble [1 √ó 4]&gt; &lt;opts[1]&gt; &lt;rsmp[+]&gt;\n4 recipe_decision_tree &lt;tibble [1 √ó 4]&gt; &lt;opts[1]&gt; &lt;rsmp[+]&gt;"
  },
  {
    "objectID": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html#step-6-finding-the-winning-model",
    "href": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html#step-6-finding-the-winning-model",
    "title": "Day 14 of #50daysofKaggle",
    "section": "Step 6: Finding the winning model",
    "text": "Step 6: Finding the winning model\nThis is how the tidymodels package all fits in. If you‚Äôve got all the steps covered this far, the decision making shouldn‚Äôt take much effort\n\n\nCode\ncollect_metrics(titanic_rs)\n\n\n# A tibble: 8 √ó 9\n  wflow_id             .config preproc model .metric .esti‚Ä¶¬π  mean     n std_err\n  &lt;chr&gt;                &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1 recipe_logistic_reg  Prepro‚Ä¶ recipe  logi‚Ä¶ accura‚Ä¶ binary  0.675    15 0.0147 \n2 recipe_logistic_reg  Prepro‚Ä¶ recipe  logi‚Ä¶ roc_auc binary  0.695    15 0.0184 \n3 recipe_rand_forest   Prepro‚Ä¶ recipe  rand‚Ä¶ accura‚Ä¶ binary  0.824    15 0.00484\n4 recipe_rand_forest   Prepro‚Ä¶ recipe  rand‚Ä¶ roc_auc binary  0.872    15 0.00488\n5 recipe_svm_rbf       Prepro‚Ä¶ recipe  svm_‚Ä¶ accura‚Ä¶ binary  0.780    15 0.00706\n6 recipe_svm_rbf       Prepro‚Ä¶ recipe  svm_‚Ä¶ roc_auc binary  0.806    15 0.00704\n7 recipe_decision_tree Prepro‚Ä¶ recipe  deci‚Ä¶ accura‚Ä¶ binary  0.809    15 0.00476\n8 recipe_decision_tree Prepro‚Ä¶ recipe  deci‚Ä¶ roc_auc binary  0.811    15 0.00684\n# ‚Ä¶ with abbreviated variable name ¬π‚Äã.estimator\n\n\nSince we‚Äôre looking at classification, lets see which one of the models resulted in the best roc_auc?\n\n\nCode\ncollect_metrics(titanic_rs) %&gt;% \n  filter(.metric == \"roc_auc\") %&gt;% \n  arrange(desc(mean))\n\n\n# A tibble: 4 √ó 9\n  wflow_id             .config preproc model .metric .esti‚Ä¶¬π  mean     n std_err\n  &lt;chr&gt;                &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1 recipe_rand_forest   Prepro‚Ä¶ recipe  rand‚Ä¶ roc_auc binary  0.872    15 0.00488\n2 recipe_decision_tree Prepro‚Ä¶ recipe  deci‚Ä¶ roc_auc binary  0.811    15 0.00684\n3 recipe_svm_rbf       Prepro‚Ä¶ recipe  svm_‚Ä¶ roc_auc binary  0.806    15 0.00704\n4 recipe_logistic_reg  Prepro‚Ä¶ recipe  logi‚Ä¶ roc_auc binary  0.695    15 0.0184 \n# ‚Ä¶ with abbreviated variable name ¬π‚Äã.estimator\n\n\nLooks like the rand_forest is the winner!\n\nAnother wonderful way it all ties in together is the visualisation with a single line of code.\n\n\nCode\nautoplot(titanic_rs)\n\n\n\n\n\n\n\n\n\nPieces of code that just fit into each other feels like a spoon-full of ice cream!"
  },
  {
    "objectID": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html#step-7-8-fitting-the-winner-and-predictions",
    "href": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html#step-7-8-fitting-the-winner-and-predictions",
    "title": "Day 14 of #50daysofKaggle",
    "section": "Step 7 & 8: Fitting the winner and predictions",
    "text": "Step 7 & 8: Fitting the winner and predictions\nFrom the object titanic_rs we need to pick the winning model and fit to to_predict\n\n\nCode\nfinal_fit &lt;- extract_workflow(titanic_rs, \n                              \"recipe_rand_forest\") %&gt;% \n  fit(given_data)\n\nfinal_fit\n\n\n‚ïê‚ïê Workflow [trained] ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nPreprocessor: Recipe\nModel: rand_forest()\n\n‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n2 Recipe Steps\n\n‚Ä¢ step_dummy()\n‚Ä¢ step_normalize()\n\n‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, num.trees = ~1000,      num.threads = 1, verbose = FALSE, seed = sample.int(10^5,          1), probability = TRUE) \n\nType:                             Probability estimation \nNumber of trees:                  1000 \nSample size:                      891 \nNumber of independent variables:  908 \nMtry:                             30 \nTarget node size:                 10 \nVariable importance mode:         none \nSplitrule:                        gini \nOOB prediction error (Brier s.):  0.1386875 \n\n\nCreating a new dataframe for predictions\n\n\nCode\nfinal_predictions &lt;- predict(object = final_fit, \n                             new_data = to_predict)\n\nfinal_predictions &lt;- final_predictions %&gt;% \n  rename(Survived = .pred_class) %&gt;% \n  bind_cols(PassengerId = to_predict$passenger_id)\nhead(final_predictions)\n\n\n# A tibble: 6 √ó 2\n  Survived PassengerId\n  &lt;fct&gt;          &lt;int&gt;\n1 0                892\n2 0                893\n3 0                894\n4 0                895\n5 0                896\n6 0                897"
  },
  {
    "objectID": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html#step-9-submit-to-kaggle",
    "href": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html#step-9-submit-to-kaggle",
    "title": "Day 14 of #50daysofKaggle",
    "section": "Step 9: Submit to Kaggle",
    "text": "Step 9: Submit to Kaggle\nConverting the final_predictions to csv and uploading to kaggle\n\n\nCode\nwrite.csv(final_predictions, row.names = F, \n          file = \"submissions.csv\")\n\n\nUploaded it to Kaggle and behold‚Ä¶\n\nNot bad !!! that‚Äôs like‚Ä¶ top 12% percentile. Woohoo!\n\n\n\n.. And so did you, dear reader!!"
  },
  {
    "objectID": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html#peeking-under-the-hood",
    "href": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html#peeking-under-the-hood",
    "title": "Day 14 of #50daysofKaggle",
    "section": "Peeking under the hood",
    "text": "Peeking under the hood\nI tried to replicate the same errorbar plot generated by the function. For me, this was possibly one of the bigger conceptual lessons about tidymodels framework. Here‚Äôs how I did it:\nThere are 15 bootsample folds that were generated. The workflow_set maps the fit_resamples function on the workflow. This implies that each of the 4 models have generated 2 pairs of metrics (accuracy & roc_auc) for each of the 15 resamples.\nThe collect_metrics function generates the mean vale of each metric allowing us to chose the best one.\nThe manner in which all of this is done within a single object is sublime. Information is stored in tibbles within tibbles!\nFor instance, let us look at the class of titanic_rs\n\n\nCode\nclass(titanic_rs)\n\n\n[1] \"workflow_set\" \"tbl_df\"       \"tbl\"          \"data.frame\"  \n\n\nIt is a tibble with 4 columns out of which results is a list object consisting of more tibbles.\n\n\nCode\ntitanic_rs\n\n\n# A workflow set/tibble: 4 √ó 4\n  wflow_id             info             option    result   \n  &lt;chr&gt;                &lt;list&gt;           &lt;list&gt;    &lt;list&gt;   \n1 recipe_logistic_reg  &lt;tibble [1 √ó 4]&gt; &lt;opts[1]&gt; &lt;rsmp[+]&gt;\n2 recipe_rand_forest   &lt;tibble [1 √ó 4]&gt; &lt;opts[1]&gt; &lt;rsmp[+]&gt;\n3 recipe_svm_rbf       &lt;tibble [1 √ó 4]&gt; &lt;opts[1]&gt; &lt;rsmp[+]&gt;\n4 recipe_decision_tree &lt;tibble [1 √ó 4]&gt; &lt;opts[1]&gt; &lt;rsmp[+]&gt;\n\n\nWhat does the first tibble in the results list look like? This corresponds to the logistic regression model as shown in the table above.\n\n\nCode\ntitanic_rs$result[[1]]\n\n\n# Resampling results\n# Bootstrap sampling \n# A tibble: 15 √ó 4\n   splits            id          .metrics         .notes          \n   &lt;list&gt;            &lt;chr&gt;       &lt;list&gt;           &lt;list&gt;          \n 1 &lt;split [891/318]&gt; Bootstrap01 &lt;tibble [2 √ó 4]&gt; &lt;tibble [3 √ó 3]&gt;\n 2 &lt;split [891/313]&gt; Bootstrap02 &lt;tibble [2 √ó 4]&gt; &lt;tibble [3 √ó 3]&gt;\n 3 &lt;split [891/340]&gt; Bootstrap03 &lt;tibble [2 √ó 4]&gt; &lt;tibble [3 √ó 3]&gt;\n 4 &lt;split [891/313]&gt; Bootstrap04 &lt;tibble [2 √ó 4]&gt; &lt;tibble [3 √ó 3]&gt;\n 5 &lt;split [891/326]&gt; Bootstrap05 &lt;tibble [2 √ó 4]&gt; &lt;tibble [3 √ó 3]&gt;\n 6 &lt;split [891/342]&gt; Bootstrap06 &lt;tibble [2 √ó 4]&gt; &lt;tibble [3 √ó 3]&gt;\n 7 &lt;split [891/316]&gt; Bootstrap07 &lt;tibble [2 √ó 4]&gt; &lt;tibble [3 √ó 3]&gt;\n 8 &lt;split [891/323]&gt; Bootstrap08 &lt;tibble [2 √ó 4]&gt; &lt;tibble [3 √ó 3]&gt;\n 9 &lt;split [891/338]&gt; Bootstrap09 &lt;tibble [2 √ó 4]&gt; &lt;tibble [3 √ó 3]&gt;\n10 &lt;split [891/331]&gt; Bootstrap10 &lt;tibble [2 √ó 4]&gt; &lt;tibble [3 √ó 3]&gt;\n11 &lt;split [891/319]&gt; Bootstrap11 &lt;tibble [2 √ó 4]&gt; &lt;tibble [3 √ó 3]&gt;\n12 &lt;split [891/322]&gt; Bootstrap12 &lt;tibble [2 √ó 4]&gt; &lt;tibble [3 √ó 3]&gt;\n13 &lt;split [891/340]&gt; Bootstrap13 &lt;tibble [2 √ó 4]&gt; &lt;tibble [3 √ó 3]&gt;\n14 &lt;split [891/321]&gt; Bootstrap14 &lt;tibble [2 √ó 4]&gt; &lt;tibble [3 √ó 3]&gt;\n15 &lt;split [891/320]&gt; Bootstrap15 &lt;tibble [2 √ó 4]&gt; &lt;tibble [3 √ó 3]&gt;\n\nThere were issues with some computations:\n\n  - Warning(s) x1: Column(s) have zero variance so scaling cannot be used: `final_gr...   - Warning(s) x1: Column(s) have zero variance so scaling cannot be used: `final_gr...   - Warning(s) x1: Column(s) have zero variance so scaling cannot be used: `final_gr...   - Warning(s) x1: Column(s) have zero variance so scaling cannot be used: `final_gr...   - Warning(s) x1: Column(s) have zero variance so scaling cannot be used: `final_gr...   - Warning(s) x1: Column(s) have zero variance so scaling cannot be used: `final_gr...   - Warning(s) x1: Column(s) have zero variance so scaling cannot be used: `final_gr...   - Warning(s) x1: Column(s) have zero variance so scaling cannot be used: `final_gr...   - Warning(s) x1: Column(s) have zero variance so scaling cannot be used: `final_gr...   - Warning(s) x1: Column(s) have zero variance so scaling cannot be used: `final_gr...   - Warning(s) x1: Column(s) have zero variance so scaling cannot be used: `final_gr...   - Warning(s) x1: Column(s) have zero variance so scaling cannot be used: `final_gr...   - Warning(s) x1: Column(s) have zero variance so scaling cannot be used: `final_gr...   - Warning(s) x1: Column(s) have zero variance so scaling cannot be used: `final_gr...   - Warning(s) x1: Column(s) have zero variance so scaling cannot be used: `final_gr...   - Warning(s) x4: Column(s) have zero variance so scaling cannot be used: `final_gr...   - Warning(s) x11: Column(s) have zero variance so scaling cannot be used: `final_gr...   - Warning(s) x15: Column(s) have zero variance so scaling cannot be used: `final_gr...\n\nRun `show_notes(.Last.tune.result)` for more information.\n\n\nWhat does the .metrics list contain?\n\n\nCode\ntitanic_rs$result[[1]]$.metrics[[1]]\n\n\n# A tibble: 2 √ó 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.682 Preprocessor1_Model1\n2 roc_auc  binary         0.655 Preprocessor1_Model1\n\n\nSo .metrics is a list of results that is generated for each of the folds. The above tibble is the resulting metrics after cross-validation of the first fold of titanic_folds using glm (logisitic classification).\nIf the intent is to create the errorgraph manually, then we‚Äôll need to extract the data within each of these tibbles.\nNow I wasn‚Äôt able to check the code for autoplot() and I‚Äôm pretty certain there‚Äôs a more elegant method out there. Hit me up if you feel there‚Äôs a better way to extract data from within tibbles in the comments here üëâüèº\nFirst step is to create an empty tibble consisting of 4 models, 2 metrics and 80 empty cells that need to be filled in from titanic_rs object.\n\n\nCode\ntidy_titanic_rs &lt;- tibble(wf = rep(unique(titanic_rs$wflow_id), 15*2),\n                          metrics = rep(c(rep(\"accuracy\", 4), \n                                          rep(\"roc_auc\",4)),\n                                        15),\n                          values = rep(NA, 4*15*2))\nhead(tidy_titanic_rs, 8)\n\n\n# A tibble: 8 √ó 3\n  wf                   metrics  values\n  &lt;chr&gt;                &lt;chr&gt;    &lt;lgl&gt; \n1 recipe_logistic_reg  accuracy NA    \n2 recipe_rand_forest   accuracy NA    \n3 recipe_svm_rbf       accuracy NA    \n4 recipe_decision_tree accuracy NA    \n5 recipe_logistic_reg  roc_auc  NA    \n6 recipe_rand_forest   roc_auc  NA    \n7 recipe_svm_rbf       roc_auc  NA    \n8 recipe_decision_tree roc_auc  NA    \n\n\nDimensions of this empty table?\n\n\nCode\ndim(tidy_titanic_rs)\n\n\n[1] 120   3\n\n\nTo extract the values from the tibbles in titanic_rs and save in tidy_titanic_rs, I proceeded to employ a nested for loop. Dont‚Äô judge, i say!"
  },
  {
    "objectID": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html#extracting-the-tibbles",
    "href": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html#extracting-the-tibbles",
    "title": "Day 14 of #50daysofKaggle",
    "section": "Extracting the tibbles",
    "text": "Extracting the tibbles\npluck has got to be the coolest function I‚Äôve ever come across!\n\n\nCode\nbigtable &lt;- purrr:::pluck(titanic_rs, 4)\nwflow_id_titanic &lt;- unique(titanic_rs$wflow_id)\nfor(i in 1:length(wflow_id_titanic)){\n    \n  wflow_id &lt;- wflow_id_titanic[i]\n  smalltable &lt;- bigtable[[i]]\n  \n  for(j in 1:length(smalltable$.metrics)){\n    smallertable &lt;- purrr::pluck(smalltable$.metrics, j)\n    tidy_titanic_rs$values[(tidy_titanic_rs$wf==wflow_id & \n                              tidy_titanic_rs$metrics==\"accuracy\")][j] &lt;- smallertable$.estimate[smallertable$.metric == \"accuracy\"]\n    tidy_titanic_rs$values[(tidy_titanic_rs$wf==wflow_id & \n                              tidy_titanic_rs$metrics==\"roc_auc\")][j] &lt;- smallertable$.estimate[smallertable$.metric == \"roc_auc\"]\n    \n  }\n}\n\ntidy_titanic_rs2 &lt;- tidy_titanic_rs %&gt;% \n  group_by(wf, metrics) %&gt;% \n  summarise(value_min = mean(values) - 0.5*sd(values),\n            value_max = mean(values) + 0.5*sd(values),\n            value_mean = mean(values)) %&gt;% ungroup() %&gt;% \n  right_join(tidy_titanic_rs, by = c(\"wf\", \"metrics\"))\n\n\nFor some reason, value_max and value_min for the error-bar are 0.5 * ùùà or approximately ¬±19% of the mean of values"
  },
  {
    "objectID": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html#manually-generated-plot",
    "href": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html#manually-generated-plot",
    "title": "Day 14 of #50daysofKaggle",
    "section": "Manually generated plot",
    "text": "Manually generated plot\nWith the data in a rectangular tidy format, the rest of the magic is handled by ggplot\ncracks knuckles\n\n\nCode\ntidy_titanic_rs2 %&gt;% \nggplot(aes(x = reorder(wf, desc(value_mean)), \n             y = values, \n             color = wf))+\n  geom_errorbar(aes(ymax = value_max, ymin = value_min), \n                width = 0.1)+\n  geom_point(aes(y= value_mean))+\n  scale_y_continuous(breaks = seq(0.65, 0.9, 0.05),\n                     limits = c(0.65, 0.9))+\n  theme(legend.position = \"none\")+\n  labs(x = NULL, y = NULL)+\n  facet_wrap(~metrics)\n\n\n\n\n\n\n\n\n\nAnd here once again is the auto-generated plot. Not bad, eh?\n\n\nCode\nautoplot(titanic_rs)\n\n\n\n\n\n\n\n\n\n\n\n\nOooh yeeah!"
  },
  {
    "objectID": "posts/2024-09-16-mondaymotivation/index.html",
    "href": "posts/2024-09-16-mondaymotivation/index.html",
    "title": "Monday Motivation",
    "section": "",
    "text": "Monday Motivation: Beginner‚Äôs Luck\nEver tried something for the first time and nailed it? Wonder why that happens?\nRecently, I revisited Paulo Coehlo‚Äôs ‚ÄúThe Alchemist‚Äù and I still find pearls of wisdom scattered in his story. Early in the book, Santiago‚Äôs fortune changes when he meets an old man who calls himself the King of Salem. The old man asks him to meet him the next day at noon with a tenth of his flock. The boy does as instructed but is surprised to note how easy it was for him to sell the remaining sheep. The old man tells him about the power of ‚ÄúBeginner‚Äôs Luck‚Äù or ‚ÄúPrinciple of Favourability‚Äù\n\n‚Äú‚Ä¶when you play for the first time, you are almost sure to win. Beginner‚Äôs luck‚Äù\n‚ÄúWhy is that?‚Äù\n‚ÄúBecause there is a force that wants you to realize your destiny; it whets your appetite with a taste of success‚Äù\n\nWhat a beautiful way of explaining such a strong notion to a child.\n\n\n\nWhy does this happen?\nBut why does the ‚ÄúPrinciple of Favourability‚Äù even exist? Here‚Äôs my hot take on the psychological forces behind the magic.\n\nFresh Perspective: More often than not, a new set of eyes on any problem will point out to solution that would have been overlooked by others. Whenever a team has been struggling with a project for prolonged periods of time, they wouldn‚Äôt mind taking a newcomer‚Äôs opinion\nNo Fear of Failure: Its such a refreshing feeling not to be burdened by failure. More often than not we‚Äôre shy of trying out something new because past experience or our social circle tells us that it doesn‚Äôt work. There is a much lesser threshold of expectations with a newcomer\nPositive energy: Always an important part of the solution that is never given enough credence. Sometimes the sheer enthusiasm and exuberance of a newcomer generates positive vibes leading to unexpected success\nExperimental mindset: Beginners are yet to learn the rules of engagement and would be inclined to try unconventional approaches\n‚ÄúThe Universe is guiding you‚Äù: Some may call it mumbo-jumbo, but Paulo Coelho terms it simply as a way of the Universe trying to motivate you in not giving up. If you were to lose patience and confidence in the first attempt, that would be the end of the road. But now that you‚Äôve tasted succeess, you must now persevere to achieve the end goal. It‚Äôs a spiritual argument rather than a rational one - but I have qualms in believing in it üòÅ\n\n\n\nHow do I make use of this?\n\nKeep the momentum: Super critical - one good deed will lead to another. Stopping or getting distracted by something else will make it appear as another flash in the pan. There‚Äôs a good chance that if you‚Äôre smart enough, you‚Äôd have learnt something new to get the next attempt right\nStay Curious: ‚ÄúThink like a child‚Äù - absorb and soak in new experiences to maintain that fresh perspective and creativity\nStay humble & learn from your wins: Universe and all that is fine but its always advisable to critically analyse what went right. Understanding your strengths will help in replicating the right outcomes. No need to get cocky about getting it right the first time. No-one said beginner‚Äôs luck lasts foreverüôÇ\nMaintain the positivity: Retaining the winning attitude is important. High probability that the next set of challenges will only be more daunting than the first, therefore keep that optimism. It will help attracting supportive relationships and favourable circumstances.\nTrust the process: My favourite and oft-repeated phrase. Keep putting in the effort and don‚Äôt worry about the outcomes. Believe that this early success is a sign that you‚Äôre on the right path. Trust your instincts and the process, even when things get tough later.\n\nLiterally every time I‚Äôve started a new project, I‚Äôve seen the ‚ÄúPrinciple of Favourability‚Äù play out in front of me. Do you have an instance to share as well? Let me know in the comments on the right‚û°Ô∏è"
  },
  {
    "objectID": "posts/2022-10-27-holiday-break-for-50daysofkaggle/index.en.html",
    "href": "posts/2022-10-27-holiday-break-for-50daysofkaggle/index.en.html",
    "title": "Holiday break for #50daysofkaggle",
    "section": "",
    "text": "Diwali is here\nIts festive season and I‚Äôve been really tied up with a bunch of things\n\n4 day long weekend where it was just about family and festivities. frankly one of the most rejuvenating and happy experiences to spend time with my daughter. She‚Äôs at the stage where she‚Äôs old enough to keep herself occupied alone. Its a pleasure to just watch her paint, draw, sing, role-play, read and whatever it is she loves doing.\nJob hunting has been taken up by a notch. Four major initiatives:\n\napplying more aggresively on applying to more companies in a day (spread across sites such as linkedin, iimjobs and naukri)\nOpening up the longlist to include SEA markets (Phillipines, Vietnam, Singapore, Indonesia)\nvolunteered to take up a course at FLAME starting in December. Its a 30 hour course that‚Äôs going to be quite gruelling to deliver. So will need to really start preparing as early as possible\nbuilding longlist of contacts for outreach. Downloaded LinkedIn contacts lists and sorted in order of priority\n\n\nCurrently pausing blog related updates to focus on picking up the job hunt search. Hopefully, post-Diwali there will be some action once people are back in offices!\nedit: enabled comments using hypothes.is as enabling Disqus is going to take some more effort"
  },
  {
    "objectID": "posts/2024-08-22-restart/index.html",
    "href": "posts/2024-08-22-restart/index.html",
    "title": "Restart",
    "section": "",
    "text": "Been a busy year away from the blog. Checking in to see if all systems are running ok.\n\n\n\nShuru kare leke prabhu ka naam\n\n\nminor edit (22-Aug): checking for github deployment, lowercasing folder name"
  },
  {
    "objectID": "posts/2024-09-30-mondaymotivation/index.html",
    "href": "posts/2024-09-30-mondaymotivation/index.html",
    "title": "Monday Motivation",
    "section": "",
    "text": "Monday Motivation: The Boring Stuff post\nWhy does monotonous work weigh us down?\nHere‚Äôs a parable I never get tired of telling my team - A thief tries to steal from a chinese monastery but is caught. The monks are furious and want to teach him a lesson with the harshest punishment. The head priest asks him to be tied up to a chair so that he can‚Äôt move his limbs. A pot of cold water is hung from the ceiling with a tiny crack so that only a single drop falls on the thief‚Äôs down at any moment. That‚Äôs it.. that‚Äôs the punishment. The chinese water torture doesn‚Äôt seem like much in the beginning but the weight of each drop feels like a mountain over a period of time.\n\nSound familiar? Boring stuff is boring because it kills excrutiatingly slowly. I know it ‚Äôcos I‚Äôve been there myself. Why does this happen?\n\nLoss of any meaning or goal\nDoesn‚Äôt feel challenging\nSuper hard to focus and the mind drifts away to other thoughts which seem more important\nLack of autonomy - (we were told to do things in this way, therefore its the only way to do it)\npoor work environment doesn‚Äôt value change\n\n#5 is the bitter truth. Many times its a rite of passage that we all have to go through.\nHow do you escape this rut?\n\nAcknowledge that its boring and enjoy the monotony. Sometimes its just good to build muscle memory through boring stuff and get done with it.\nBreak it down into smaller tasks and see if there are ways to automate or replace them.\nFind out why these tasks matter in the larger scheme of things. There‚Äôs probably someone somewhere in the organisation it matters more than you think. Address the big picture and see how you contribute to make it more meaningful.\nLearn something new - no kidding. this is an instant way to shake the rhythm. Use these mundane tasks as a way to test your knowledge of something new you may have learnt. A coding language or a skill that needs sharpening, maybe?\nCollaborate with others and see if these tasks can be shared. If you‚Äôre really good at sticking around, you may even delegate this! (LOL.. j/k. Try not to make someone else miserable!)\n\nLet us talk more the comments on the right‚û°Ô∏è"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I am an independent marketing consultant 16+ years experience across Marketing, Business & Analytics teams in consumer companies. I specialise in helping founders understand how to build revenue growth levers with marketing channels and remain invested in long term marketing efforts.\nThroughout my professional journey I‚Äôve helped businesses solve for user acquisition, engagement and revenue - from entertainment powerhouses like MX Player & Viu to smaller, nimble footed startups like Turno selling Electric Vehicles. I am passionate about being at the intersection of marketing, consumer insights and analytics while working alongside teams to bring growth ideas to life. Fluent in English, Hindi & Telugu. My favourite weekend activity is to switch off the laptop and spend time with my family exploring Mumbai üë®‚Äçüë©‚Äçüëß"
  },
  {
    "objectID": "about.html#why-the-null-hypothesis",
    "href": "about.html#why-the-null-hypothesis",
    "title": "About Me",
    "section": "Why the Null Hypothesis?",
    "text": "Why the Null Hypothesis?\nBritish Nobel-laureate Ronald Coase famously said ‚ÄúIf you torture the data long enough, it will confess to anything.‚Äù True words that ring loudly in any digital-first company. As someone who‚Äôs cut their teeth in quantitative research, I firmly it is consumer behaviour & insight that is sacred and we must seek it out with the help of data (big or small).\nIn statistics, the null hypothesis conjecture is the first step to begin any inferential analysis. Similarly, I begin every growth project with a null hypothesis to help me understand the consumer better. The hope is to document and share my learnings through this journal.\nFeel free to leave a message using the tab on the rightüëâüèº"
  },
  {
    "objectID": "about.html#executive-competencies",
    "href": "about.html#executive-competencies",
    "title": "About Me",
    "section": "Executive Competencies",
    "text": "Executive Competencies\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  Marketing P&L\n  Consumer Research\n  Growth Marketing\n\n\n  Creative Strategy\n  Analytics\n  Business Management\n\n\n\n  Team Mentorship"
  },
  {
    "objectID": "about.html#qualifications",
    "href": "about.html#qualifications",
    "title": "About Me",
    "section": "Qualifications",
    "text": "Qualifications\n\nWorkEducationCertificationsTeaching\n\n\nTurno (May‚Äô23 - Sep‚Äô24)\nHead of Marketing\n\nTransformed the acquisition strategy on Commercial Vehicle business by scaling paid digital channels,¬†lowering unit economics¬†and¬†improving contribution margins by 40%. Hands-on expertise in¬†team building¬†that contributes to 95% of vehicle sales\nBuilt the Marketing Analytics platform that captures the full consumer journey in¬†continuous collaboration with Analytics & Tech¬†teams. Resulted in¬†lowering CAC by 50%¬†to maximise cost efficient channels (affiliates, biddable & non-biddable networks)\nManaged and forecasted ~INR 3 cr annual marketing budget¬†to scale digital marketing channels across Search, Display, Video campaigns. Started leveraging¬†first-party data¬†(online and offline events) and proceeded to use deep-funnel events for¬†conversion rate optimisation\nEnabled¬†A/B experimentations¬†across acquisition and activation stages (80+ hypothesis every quarter¬†iterating on creatives, landing pages, campaign parameters, sales scripts) resulting in identifying¬†high-impact¬†winner creative communication + TG\n\nMX Player (Feb ‚Äô20 - Oct ‚Äô22)\nDirector, Insights & Growth Strategy\n\nDevised the¬†launch campaigns¬†and strategies for¬†40+ MX original web series¬†including localised campaigns (Hindi, Telugu, Tamil, Marathi, Punjabi among others)\nGrew MX movie catalogue (in Hindi and 9+ languages - India‚Äôs 2nd largest AVOD collection)¬†by 30% using engagement-optimised experiments\nConceptualised and executed ‚ÄúMX Vdesi Wednesdays‚Äù - first-in-OTT branding initiative that brought gripping localised dramas from Turkey, S. Korea, China, Ukraine, Russia, UK, USA.¬†Boosted the userbase by 40%¬†with footprint across 80 towns\nDeveloped on & off platform campaigns¬†for INR 400cr acquired content portfolio [contributed to 85%+ of watchtime minutes]\nInstituted long term retention & remarketing growth loops¬†involving cross-functional teams from Content, Product, Tech\n\nViu India (Sep ‚Äô16 - Feb ‚Äô20)\nAssociate Director, Growth Marketing\n\nManaged USD 15M annual marketing plan¬†and spearheaded platform growth achieving remarkable 10x growth - from 0.5M to 5M MAU in span of 14 months - through brand, paid, organic and offline marketing channels (SEM, SEO, SM, Influencers, performance marketing, brand digital etc)\nBuilt Google, Facebook and affiliate accounts for app-install, display and search campaigns that¬†improved Cost Per Install by 40% across 9 months¬†- outcome after running 15+ monthly digital conversion experiments across geos\nSuccessfully drove¬†Viu Originals‚Äô launch brand campaigns¬†across Hindi, Telugu, and Tamil markets. Responsible for cross-media campaigns, agencies relations , user-acquisition, creative teams - outcomes measured using Quarterly Brand Studies\n\nSony Pictures Network India (Jun ‚Äô08 - Sep ‚Äô16)\nManagment Trainee to Sr.¬†Manager\n\nWorked my way up the ladder across multiple roles in the company with wide experience across Strategy, Research, Marketing, Revenue Planning and Content Acquisition\n\n\n\n Post Graduate Diploma in Communications, 2008\n\nMICA (Mudra Institute of Communications, Ahmedabad)\n\n B.E (Computer Science & Engineering), 2006\n\nVasavi College of Engineering (affiliated to Osmania University), Hyderabad\n\n\n\n\nIBM Data Science Professional Certificate, Sep‚Äô22\nMachine learning, Feb ‚Äô21 (Stanford University)\n\nExecutive Program in Strategic Digital Marketing, Dec‚Äô19 (University of Cambridge Judge Business School)\nProfessional Certificate Programme in Business Negotiation, Feb ‚Äô22 (SP Jain Institute of Management & Research)\n\n\n\nVisiting Faculty at\n\nFlame University, Pune\nSNDT Women‚Äôs University, Mumbai"
  },
  {
    "objectID": "posts/2025-02-25-teaching-at-FLAME/index.html",
    "href": "posts/2025-02-25-teaching-at-FLAME/index.html",
    "title": "Campus Chronicles",
    "section": "",
    "text": "Campus Chronicles: Reflections from a Media Educator‚Äôs Return\nI recently completed my fifth year teaching at FLAME University, guiding post-graduate students through the evolving landscape of media business and marketing. Returning to that lush green campus and stimulating discussions is always a lovely respite from the routine corporate drudgery.\nThe 30-hour course I designed aims to bridge the gap between media theory and industry application. My humble attempt to equip the students for the real world by hoping they:\n\nappear smart for placements and\nknow enough industry gyaan to at least hold a 2 minute conversation with their skip manager!\n\nWhat continues to fascinate me is how quickly the ‚Äúpractical‚Äù becomes historical. Material I prepared for previous cohort has already started showing its age‚ÄîOTT platforms, which I once framed as disruptive challengers, now dominate the landscape, commanding 30% of the Media & Entertainment industry in 2025.\nWhat was revolutionary has become conventional - and the students (unknown to them) are in the middle of seismic industry shifts. How do you convince them otherwsise?\nTeaching these bright minds has taught me far more than I could possibly impart to them. When a student challenges an industry ‚Äútruth‚Äù I‚Äôve taken for granted for years, it forces me to reevaluate my own assumptions‚Äîoften leading to more nuanced understanding for us both.\nPerhaps most importantly, I‚Äôve learned that real education happens in the spaces between formal instruction‚Äîin brief hallway conversations, in the insightful question asked after class ends. These young souls are about to enter the big bad industry with large starry-eyes hopeful for their future. I only wish that they preserve that curiousity for their entire career.\nBeyond the intellectual challenge, academia offers something corporate life rarely does ‚Äî a brief escape from emails and Gmeet calls into a space where ideas (not deadlines) drive the conversation.\nKudos to Sajith Narayanan and Gangaraju S S for bringing this industry-centric teaching for their students. Cheering for their progress in the years ahead!As I prepare materials for other courses, I find myself energized by the challenge of capturing this rapidly changing industry.\nIf your institution is looking for someone to bring industry perspective to your marketing programs this coming academic year, I‚Äôd welcome the conversation. There‚Äôs nothing quite like the intellectual stimulation of campus life to remind us why we chose this path in the first place.\nWhich one of these resonated with you and which didn‚Äôt? Let us talk more the comments on the right‚û°Ô∏è"
  },
  {
    "objectID": "posts/2024-11-12-mythbusters/index.html",
    "href": "posts/2024-11-12-mythbusters/index.html",
    "title": "Myths about Growth Marketing",
    "section": "",
    "text": "Busting myths around Growth Marketing\nGrowth Marketing has been around for roughly a decade now and there‚Äôs a fair share of baggage to that word. A common phrase I‚Äôve heard is ‚ÄúIf everyone in the company is focussed on growth, why should 1 person/ team own that role?‚Äù Very true. I agree here. Fundamentally, it is marketing principles using different set of tools. Unfortunately, many people throw the term around without truly grasping its nuances. Here‚Äôs my attempt to demistify some of myths I‚Äôve come across.\n\n1) Myth: Growth Marketing Is Just Another Name for Paid Acquisition.\nGotta admit. This is the easisest slope to slip on. More so, because traditional practices led by marketing teams seldom have such a high data visibility. Once we start spending on paid ads and watch the entire funnel come alive, it becomes very easy to say that Growth has been achieved. Alas, the messenger became the message. In my mind, it‚Äôs a holistic approach that considers the entire customer journey, from initial awareness to long-term retention. While paid acquisition can be part of a growth strategy, it‚Äôs just one piece of the puzzle.\n\n\n2) Myth: Growth Marketing Relies on ‚ÄúGrowth Hacks‚Äù and Quick Fixes\nSeen those Instagram vids where marketing gurus explain 5 simple steps to grow your follower count. They sure did pull-off a growth hack. They hacked their way to get your attention! True growth marketing is not about chasing fleeting trends or viral gimmicks. It‚Äôs about building sustainable growth engines through a deep understanding of your target audience, data-driven experimentation, and continuous optimisation. My biggest challenge in the initial days was thinking in frameworks. That‚Äôs not an easy skill to master and this requires a long-term perspective, not a quick-fix mentality. To put it mildly, one needs to figure how to accelerate the ability to get your product into the hands of more users faster and communicate the value to the users over the usage period.\n\n\n3) Myth: Growth Marketing Ignores Branding and Focuses Solely on Performance Metrics.\nSigh! If you‚Äôre happy and you know it, park your brand budgets seperately. Happens in many places and it is genuinely unfair to blame anyone for this. While growth marketers are highly data-driven, they also recognise the importance of brand building. A strong brand enhances trust, loyalty, and ultimately, growth. A successful growth strategy aligns performance marketing efforts with a clear brand identity and messaging. They are literally two sides of the same coin and shouldn‚Äôt be differentiated.\n\n\n4) Myth: Growth Marketing Is Only Relevant for Early-Stage Startups.\nWhether its a small startup or an established enterprise, the pace with which the market is evolving always makes room for experimentation. The focus on frameworks built on data, iteration and customer-centricity can be practiced within small teams or across large organisations. Which strategy one employs depends on the company‚Äôs stage of maturity. For instance, early-stage startups should focus on ‚Äúbig swings‚Äù by leveraging activation & engagement loops, while more mature companies can benefit from smaller optimisations at scale.\nOne of the best reasons to drop the term growth marketing is because simple growth principles are being used effectively across product, design, tech and sales teams. Growth is literally not marketing any longer!\nDoes this make you feel that you‚Äôre a growth marketer yourself? Chances are that you may be and didn‚Äôt know it either. Do let me know if there are any other myths that you‚Äôve encountered.\nWhich one of these resonated with you and which didn‚Äôt? Let us talk more the comments on the right‚û°Ô∏è"
  },
  {
    "objectID": "posts/2025-03-05-MJ-and-marketing/index.html",
    "href": "posts/2025-03-05-MJ-and-marketing/index.html",
    "title": "MJ & Marketing",
    "section": "",
    "text": "What Michael Jackson taught me about marketing: The Art of Reach & Frequency\nBack in 2010, a young sprightly executive with big dreams found himself staring at a marketing challenge: how do we make Sony PIX‚Äôs premiere of ‚ÄúThis Is It‚Äù - Michael Jackson‚Äôs posthumous documentary - a ratings hit? PIX had already delivered a smashing performance with Slumdog Millionaire. Expectations were sky-high, and as the junior-most member of the team, I had a chance to experiment and learn on the fly.\n\nThrough some old-school data analysis I discovered something powerful about reach and frequency that still guides my campaigns today.\n\nOur secret weapon was PIX‚Äôs signature ‚Äúsingle scene promo‚Äù - full 90-second clips from the film (imagine that luxury in today‚Äôs 5-second attention economy!). My analysis showed that high-frequency advertising early in a campaign builds initial reach, but you need to continuously add new audiences to maximize impact.\n\nSo we orchestrated a symphony of content: 10-second teasers built early awareness, 30-second exclusives deepened interest, 20-second contests drove engagement, and those magnificent 90-second single scenes delivered the emotional punch.\n\nThe strategy? Build hype with teasers for 15 days, then unleash exclusive promos and single scenes. In the final 10 days, we reversed course: more quick teasers and fewer long scenes. We frontloaded emotion, then optimized for reach.\n\nThe result? ‚ÄúThis Is It‚Äù became one of the most-watched non-fiction English movies on Indian TV at that time.\n\nThis principle translates perfectly to today‚Äôs digital landscape. Think about your social campaigns - how often do you start with your best creative, only to see engagement drop? Instead, consider this:\n-&gt; Phase 1: Quick, thumb-stopping content to establish presence. On YouTube, short non-skippable ads (6s) work much like our initial 10s teasers, driving quick awareness without losing attention.\n-&gt; Phase 2: Deep, value-rich content to build emotional connection. leverage sequential messaging. Platforms like Meta and Google allow you to retarget audiences with sequential ads, guiding them through a journey much like our phased promo strategy.\n-&gt; Phase 3: Return to quick, action-oriented content with clear CTAs. Don‚Äôt forget Frequency Capping. Ensuring that ads are seen enough to build recall but not so much that they lead to fatigue‚Äîa digital marketer‚Äôs version of scaling back the 90s promos.\n\nI‚Äôve seen this exact framework deliver for streaming platforms, where we start with character teasers, progress to scene previews, then finish with ‚Äústreaming now‚Äù urgency messages.\n\nThe fundamental truth doesn‚Äôt change: reach without frequency falls flat, but frequency without expanding reach just creates annoying ads. Balance creativity with analytics, and experiment fearlessly to hit the right notes!\n\nWhat‚Äôs your experience balancing these elements in the digital space?\nLet us talk more the comments on the right‚û°Ô∏è"
  },
  {
    "objectID": "posts/2025-03-10-linkedin-content-engine/index.html",
    "href": "posts/2025-03-10-linkedin-content-engine/index.html",
    "title": "Building a content engine",
    "section": "",
    "text": "Fun fact - did you know linkedin as a social network predates Facebook by a year? Implying that social networking for professionals was a much needed solution to an existing problem - How do I appear smart at work without really having to peddle my resume all the time?\nWe all know the importance of maintaining a good linkedin profile. Helps in professional and personal circles to create awareness about what you‚Äôre upto at work. I‚Äôve had varying degrees of success with linkedin across the years. While job searches are a pain (more on that in a seperate blog!), I‚Äôve been trying to post regularly since the past year or so ‚Äì namely, once a week.\nEarly Feb, I started tinkering around with Claude, Perplexity and ChatGPT to finetune my posting skills. Its pretty easy these days to spot an AI generated post. You know how they go:\n\nTone is overly formal / generic : AI uses neutral or overtly formal tone. Human-written posts often have more nuances and a conversational style\nPerfect grammar and sentence structure: geez‚Ä¶ did we all graduate with such perfect English?\nGeneric gyaan: yah.. that‚Äôs pretty obvious to spot.\nLack of depth or any real analysis: so many posts appear real shallow without any perspective. just a bunch of public facts regurgitated to read nicely.\nEverything Is Written In Sentence Case For ChatGPT‚Äôs Mimicing Their Neatly Formatted Training Data\nBullet points everywhere for everything (gulp!)\n\nI wanted to try break out of this routine and create a framework that allows me to use AI tools to generate ideas but still at the same time devote some time to making it a naturally written post.\nThis is what I did, so keep your notepad ready!"
  },
  {
    "objectID": "posts/2025-03-10-linkedin-content-engine/index.html#step-1-define-yourself---tu-beer-hai",
    "href": "posts/2025-03-10-linkedin-content-engine/index.html#step-1-define-yourself---tu-beer-hai",
    "title": "Building a content engine",
    "section": "Step 1: Define yourself - ‚Äútu beer hai‚Äù",
    "text": "Step 1: Define yourself - ‚Äútu beer hai‚Äù\nno.. you‚Äôre not beer. You‚Äôre a living breathing entity that just needs to write a post.\nSo take that sarcasm a notch lower and try painting a picture of yourself within 100 words. My suggestion is as follows:\n\nI am a &lt;profession name&gt; specialising in the &lt;field of work&gt; since &lt;career start year&gt;. I have worked in companies such as &lt;company names&gt;. I specialise in &lt;skills and experties&gt;. I have a proven track record in &lt;results delivered&gt;. &lt;add more relevant data about the last role, positions held and mandates delivered. one sentence each on your career journey and how you progressed to bigger problems - breadth of work, team leadership, cxo discussions, budget management, vendor management and so on&gt;\n\nFirst of all, make it easy to read for yourself. If you‚Äôre like me - you‚Äôll really end up spending a lot of time on this‚Ä¶ because we all have a very large view of ourselves! I know its difficult but you‚Äôve got to be practical. Maybe you won‚Äôt get it right the first time but try writing one for starters. You may be able to iterate as u go along."
  },
  {
    "objectID": "posts/2025-03-10-linkedin-content-engine/index.html#step-2-defining-the-customers",
    "href": "posts/2025-03-10-linkedin-content-engine/index.html#step-2-defining-the-customers",
    "title": "Building a content engine",
    "section": "Step 2: Defining the customers",
    "text": "Step 2: Defining the customers\nThis one is relatively simpler. List down all the kinds of people that you want your post to be read. In marketing terms - ‚Äúu want to generate impressions within this target segment‚Äù\nDon‚Äôt forget to also mention the intended response from the TG you are expecting. The reason this needs to be specified is that in every campaign we use a certain measured outcome. Since this is language we‚Äôre talking about, one needn‚Äôt specificy a KPI but at least try articulating your expectation so that the AI tool can generate its answer accordingly.\nIn my case, this is what I listed down:\n\n\nRecruiters and headhunters in Mumbai and Bengaluru region operating in the startup and digital-first companies.\n\n\nexpected reaction after reading = need to consider me as a strong candidate to hire or recommend for a job\n\n\nD2C founders or head of marketing teams.\n\n\nexpected reaction after reading = identify me as a marketing expert who can solve their business problems\n\n\nPeers who are around 10-15 years of experience in the similar life stage\n\n\nexpected reaction after reading = engage and interact on the posts with their opinions and counter arguments\n\n\nJuniors from the industry with less than 5 years of experience.\n\n\nexpected reaction after reading = reach out for career guidance, resume review, interview preperation or referral\n\n\nAcademic/Training Institutions in my network\n\n\nexpected reaction after reading = engage with me on academic pursuits, co-author white papers, invite for guest lectures"
  },
  {
    "objectID": "posts/2025-03-10-linkedin-content-engine/index.html#step-4-build-themes",
    "href": "posts/2025-03-10-linkedin-content-engine/index.html#step-4-build-themes",
    "title": "Building a content engine",
    "section": "Step 4: Build themes",
    "text": "Step 4: Build themes\nDon‚Äôt just create arbitrary posts. Build some boundaries that you want your content to work within. In my case, here are the themes I chose:\n\nTrending topics regarding Marketing & digital tools\nApplication & impact for brands\nCurrent affairs and opinions - non market related\nHumour"
  },
  {
    "objectID": "posts/2025-03-10-linkedin-content-engine/index.html#step-3-stringing-it-all-together",
    "href": "posts/2025-03-10-linkedin-content-engine/index.html#step-3-stringing-it-all-together",
    "title": "Building a content engine",
    "section": "Step 3: Stringing it all together",
    "text": "Step 3: Stringing it all together\nWith all this done, lets see what my prompt now looks like:\n\nI am a marketing professional specialising in the field of media and consumer marketing since 2008. I have worked in companies such as Sony Pictures Network India, Viu India, MX Player and Turno. I specialise in translating consumer insights into actionable growth levers for businesses. I have a proven track record in scaling consumer-facing platforms and generating revenues through digital and traditional marketing channels. In my last role as marketing head for a series-A startup, I handled the entire marketing P&L which involves growth, performance, brand and content marketing. Previously in my 16 years of industry experience, I‚Äôve led roles to build platform and revenue growth at MX Player and Viu OTT apps. As a team leader, I work closely with CXO, Product, Tech and Sales heads to drive cross-functional initiatives that set and achieve business AOP goals.\nI want to create a linkedin content engine. Give me 5 ideas within each of the below target segment with the intended responses. Do not make it sound generic nor should it appear to have been written by an AI tool:\n\nrecruiters and headhunters in Mumbai and Bengaluru region operating in the startup and digital-first companies.\n\nexpected reaction after reading = need to consider me as a strong candidate to hire or recommend for a job\n\nD2C founders or head of marketing teams.\n\nexpected reaction after reading = identify me as a marketing expert who can solve their business problems\n\nPeers who are around 10-15 years of experience in the similar life stage\n\nexpected reaction after reading = engage and interact on the posts with their opinions and counter arguments\n\nJuniors from the industry with less than 5 years of experience.\n\nexpected reaction after reading = reach out for career guidance, resume review, interview preperation or referral\n\nAcademic/Training Institutions in my network\n\nexpected reaction after reading = engage with me on academic pursuits, co-author white papers, invite for guest lectures\n\n\nthe themes I want to explore are:\n\nTrending topics regarding Marketing & digital tools\nApplication & impact for brands\nCurrent affairs and opinions - non market related\nHumour"
  },
  {
    "objectID": "posts/2025-03-10-linkedin-content-engine/index.html#output",
    "href": "posts/2025-03-10-linkedin-content-engine/index.html#output",
    "title": "Building a content engine",
    "section": "Output",
    "text": "Output\nhere‚Äôs what my ideas looked like on ChatGPT.\n\nThe best thing is that you get different ideas for Claude and Perplexity. There really is not dearth of ideas for u to keep trying out.\nNeat if you ask me. I‚Äôve used this for a few weeks and I‚Äôve been super impressed how easily I get inspired to write. Dip into your own personal memories to think about which topic you relate with. And most importantly, give yourself a time limit - typically 5 minutes or less to spend on coming up with a post idea. Spend the remainder of the time in actually crafting the post.\nNow please just don‚Äôt dump each idea into the prompt window and ask chatGPT to generate the post for you. Some hard work and thinking here will really help you stand out from the crowd!\nHow does your content engine look like?\nLet us talk more the comments on the right‚û°Ô∏è"
  },
  {
    "objectID": "posts/2025-03-12-team-evolution/index.html",
    "href": "posts/2025-03-12-team-evolution/index.html",
    "title": "Marketing teams are evolving",
    "section": "",
    "text": "The Evolution of marketing teams\nThe evolution of marketing teams: This one is for all youngsters trying to build a career in marketing.Over the past few years, I‚Äôve started witnessing a very strong trend in the composition of marketing teams. Early in my career, while I worked with the larger companies, I would see individuals such as :\n\nMedia buying ninjas who negotiated deals with vendors\nCampaign strategists collaborating with business teams and agencies\nCreative directors orchestrating asset development\nCommunications experts managing social, PR, and on-ground activations\nMarket researchers uncovering consumer insights\n\nIronically, as my career progressed, the companies I worked with became smaller‚Äîand that‚Äôs when I witnessed a dramatic transformation in hiring preferences. Digital-first organizations began prioritizing entirely different skill sets:\n\nAnalytics experts who could translate data into stories\nData scientists building measurement and attribution models\nProduct marketers with tech-first thinking\nCRM specialists optimizing customer journeys and retention\nGrowth hackers - project managers driving experiments using creativity with technical expertise\n\nIn my last company, I was working directly with business development and sales teams on a daily level to optimise marketing campaigns. This shift isn‚Äôt just about how smaller companies operate. It‚Äôs a clear signal about where marketing is heading.\nTo those fresh from business school or with less than 3 years of experience: marketing today demands versatility beyond a single specialized skill. My advice? Choose your category/ industry/ brand, then study the entire consumer journey. Identify every department that touches that journey and are in-charge of optimising it. Now ask yourself: ‚ÄúWhat would it take for me to learn what they‚Äôre doing?‚Äù\nYou don‚Äôt need to replace entire teams, but high probability that some Gen AI tool might! This will rapidly create redundancies. Cross-functional capabilities aren‚Äôt optional luxuries‚Äîthey‚Äôre survival skills.\nWhat marketing skills are you developing that sit outside your comfort zone?\nLet us talk more the comments on the right‚û°Ô∏è"
  },
  {
    "objectID": "posts/2025-03-26-blusmart/index.html",
    "href": "posts/2025-03-26-blusmart/index.html",
    "title": "Blusmart: A fallen leader",
    "section": "",
    "text": "Blusmart: What went wrong\nBlusmart is going through a stormy patch in the rough & tumble world of ride-sharing. It occupies market share of ~10% in Delhi and Bengaluru (but 2% aross all India) distinguished by its EV-only fleet.If you‚Äôve traveled to and from Bengaluru and Delhi airports, you can attest to the premium experience Blusmart offers: drivers are consistently professional, the booking process is seamless with zero cancellations, pickups are punctual and vehicle hygiene stands out as superior.\nI heard Punit Goyal, co-founder of Blusmart speak on Nikhil Kamath‚Äôs WTF podcast, offer fascinating insights from his entrepreneurial journey in the solar energy sector and transitioning to electric mobility. The company operates an asset-light model, leasing around 6,000 cars‚Äî5,000 in the NCR/Delhi region and the rest in Bengaluru. Yet, as of January 2024, TechCrunch reported Blusmart was losing INR 600 per trip, despite each trip costing over INR 400, with significant maintenance and charging infrastructure costs adding pressure.\nChallenges at the parent company, Gensol Engineering, have quickly impacted operations. Blusmart was heavily dependent on Gensol as a leasing partner. The cash crunch at the parent company accelerated the fall of dominos in the last 8 weeks. In Feb, Mint recently noted Blusmart defaulted on INR 30 crore, though it settled the amount after delays. ValueResearch‚Äôs analysis of Gensol revealed deeper issues: stock price manipulation, a debt-to-equity ratio worsening from 0.3 in FY21 to 4.3 in FY24, and overhyped EV orders. Yesterday, Storyboard reports of high-level managerial exits with rumours of Uber acquiring it floating around.\nBlusmart‚Äôs arc from an innovator in mobility to grappling with existential crisis reflects the high-risk, high-reward nature of the EV and ride-sharing industries. What lessons can other mobility startups learn from BluSmart‚Äôs recent challenges? And can strategic pivots still save the day for BluSmart?\nKeen to hear different perspectives on this.\nLet us talk more the comments on the right‚û°Ô∏è"
  }
]