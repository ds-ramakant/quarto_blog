[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I am a media professional with a natural curiosity towards media and technology. I‚Äôve worked across broadcast and OTT companies since 2008 with a wide experience across Content, Business & Marketing. In my last role at MX Player, I worked on cross-functional initiatives to help boost platform growth and engagement. I am fluent in English, Hindi & Telugu. My favourite weekend activity is to switch off the laptop and listen to vinyls with my daughter üë®‚Äçüë©‚Äçüëß"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Null Hypothesis",
    "section": "",
    "text": "Hi there üëãüèº\nMy name is Ramakant and I am a digital marketing enthusiast with a natural curiosity towards data and technology. I started this blog as a way to sharpen my data science skills and occasionally pour out those creative juices.\nFeel free to explore the topics below - possibly a fraction what‚Äôs on my mind these days. (rest are in the drafts.. or worse - unwrittenüòÑ). The page ‚ÄúAbout Me‚Äùhas more details about my professional qualifications and my motivation to create this site."
  },
  {
    "objectID": "index.html#get-in-touch",
    "href": "index.html#get-in-touch",
    "title": "The Null Hypothesis",
    "section": "Get In Touch ‚úçÔ∏è",
    "text": "Get In Touch ‚úçÔ∏è\nI‚Äôm always on the lookout for a good conversation. Feel free to mail me at d.s.ramakant@gmail.com or check out my Linkedin profile. If you‚Äôd like to leave comments, please click the vertical bar on the right üëâüèº"
  },
  {
    "objectID": "index.html#blog-posts",
    "href": "index.html#blog-posts",
    "title": "The Null Hypothesis",
    "section": "Blog Posts",
    "text": "Blog Posts"
  },
  {
    "objectID": "posts/2022-01-14-hello-world2/index.en.html",
    "href": "posts/2022-01-14-hello-world2/index.en.html",
    "title": "Hello World2",
    "section": "",
    "text": "A lot of this is courtesy of Apres Hill‚Äôs blog\nhttps://www.apreshill.com/blog/2020-12-new-year-new-blogdown/#step-1-create-repo"
  },
  {
    "objectID": "posts/2022-04-05-what-a-wonderful-wordle/index.en.html",
    "href": "posts/2022-04-05-what-a-wonderful-wordle/index.en.html",
    "title": "What a wonderful ‚Äòwordle‚Äô!",
    "section": "",
    "text": "Who ever thought that a bunch of black and green boxes would bring out the logophile in us all? With friends and family groups sharing their progress, I find this to be an entertaining mind-puzzle to kickstart the day.\nAnd I was not alone in my quest for 5 letter words. Wordle has tickled the fascination of many in the data science community. I found Arthur Holtz‚Äôs lucid breakdown of the Wordle dataset quite interesting. Of course, there is 3B1B‚Äôs incredibly detailed videos on applying Information Theory to this 6-by-5 grid. (original video as well as the follow-up errata)\nOthers have simulated the wordle game (like here) or even solved it for you (like this blog). I‚Äôve read at least one blog post that has an academic take on the matter.\nFortunately for the reader, none of the above will be attempted by me. My inspiration comes from Gerry Chng‚Äôs Frequency Analysis Approach where I‚Äôve tried to understand the most commonly occuring letters in the official word list by position by considering a ranking mechanism"
  },
  {
    "objectID": "posts/2022-04-05-what-a-wonderful-wordle/index.en.html#what-is-a-wordle",
    "href": "posts/2022-04-05-what-a-wonderful-wordle/index.en.html#what-is-a-wordle",
    "title": "What a wonderful ‚Äòwordle‚Äô!",
    "section": "What is a wordle?",
    "text": "What is a wordle?\nThe game rules are fairly simple:\n\nYou need to guess a 5-letter word. One new word is given every day\nYou are given 6 guesses\nAfter every guess, each square is coded by a color\n\nGREY: chosen letter is not in the word\nYELLOW: chosen letter is in the word by wrong position\nGREEN: chosen letter is in the word and in the correct position\n\nRepetition of letters is allowed\n\nThat‚Äôs it!\nIn my opinion, one of the reasons for the game going viral is the way the results are shared. You‚Äôve possibly seen something like this floating around:\n\n\n\nSample world share\n\n\n‚Ä¶And if your family too has been bitten hard by the Wordle bug, then you would be familiar with group messages like this!\n\n\n\nWorld share in whatsapp"
  },
  {
    "objectID": "posts/2022-04-05-what-a-wonderful-wordle/index.en.html#frequency-analysis",
    "href": "posts/2022-04-05-what-a-wonderful-wordle/index.en.html#frequency-analysis",
    "title": "What a wonderful ‚Äòwordle‚Äô!",
    "section": "Frequency analysis",
    "text": "Frequency analysis\nArthur Hotlz‚Äôs blog is a good place to start for extracting and loading the Official Wordle list. After parsing and cleaning the data, here‚Äôs all the words broken down into a single rectangular dataframe word_list .\nUpdate 29th Jan ‚Äô23: NYT‚Äôs .js file is not retrieving any list for some reason. I‚Äôve referred to Arjun Vikram‚Äôs repo on dagshub\n\n\nCode\nknitr::opts_chunk$set(warning = FALSE, message = FALSE) \nsuppressMessages({ \nlibrary(httr)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(ggthemes)\nlibrary(scales)\nlibrary(tidyr)\nlibrary(tibble)\nlibrary(forcats)\nlibrary(knitr)\nlibrary(kableExtra)\ntheme_set(theme_light())\n})\n\nurl <- \"https://www.nytimes.com/games/wordle/main.18637ca1.js\" #not working\nurl2 <- \"https://dagshub.com/arjvik/wordle-wordlist/raw/e8d07d33a59a6b05f3b08bd827385604f89d89a0/answerlist.txt\"\nwordle_script_text <- GET(url2) %>% \n  content(as = \"text\", encoding = \"UTF-8\")\n# word_list = substr(\n#   wordle_script_text,\n#   # cigar is the first word\n#   str_locate(wordle_script_text, \"cigar\")[,\"start\"],\n#   # shave is the last word\n#   str_locate(wordle_script_text, \"shave\")[,\"end\"]) %>%\n#   str_remove_all(\"\\\"\") %>%\n#   str_split(\",\") %>%\n#   data.frame() %>%\n#   select(word = 1) %>%\n#   mutate(word = toupper(word))\n\n\nwordle_list <- str_split(wordle_script_text, \"\\n\")\n\nwordle_list <- data.frame(wordle_list) \n\nwordle_list <- rename(wordle_list, word = names(wordle_list)[1] ) %>% mutate(word = toupper(word)) #renaming column to 'word'\n\ndim(wordle_list)\n\n\n[1] 2310    1\n\n\nCode\nhead(wordle_list)\n\n\n   word\n1 CIGAR\n2 REBUT\n3 SISSY\n4 HUMPH\n5 AWAKE\n6 BLUSH\n\n\nModification to the above is another dataframe with each of the characters separated into columns which we‚Äôll call position_word_list\nThe line select(-x) removes the empty column that is created due to the seperate() function\n\n\nCode\nposition_word_list <- wordle_list %>% \n  separate(word, \n           sep = \"\", \n           into = c(\"x\",\"p1\",\"p2\",\"p3\",\"p4\",\"p5\")) %>% \n  select(-x)\nhead(position_word_list,10)\n\n\n   p1 p2 p3 p4 p5\n1   C  I  G  A  R\n2   R  E  B  U  T\n3   S  I  S  S  Y\n4   H  U  M  P  H\n5   A  W  A  K  E\n6   B  L  U  S  H\n7   F  O  C  A  L\n8   E  V  A  D  E\n9   N  A  V  A  L\n10  S  E  R  V  E\n\n\nNow onto some frequency analysis. Here‚Äôs a breakdown of all the letters in the wordle list sorted by number of occurrences stored in letter_list and creating a simple bar graph.\n\n\nCode\nletter_list <- wordle_list %>%\n  as.character() %>%\n  str_split(\"\") %>% \n  as.data.frame() %>% \n  select(w_letter = 1) %>% \n  filter(row_number()!=1) %>%\n  filter(w_letter %in% LETTERS) %>% \n  mutate(type = case_when(w_letter %in% c(\"A\",\"E\",\"I\",\"O\",\"U\") ~ \"vowel\",\n                          T ~ \"consonant\")) %>% \n  group_by(w_letter, type) %>% \n  summarise(freq = n()) %>% \n  arrange(desc(freq))\n\nletter_list %>% ungroup() %>% \n  ggplot(aes(x = reorder(w_letter, -freq), y = freq))+\n  geom_col(aes(fill = type))+\n  scale_y_continuous(labels = comma)+\n  geom_text(aes(label = freq), \n            size = 3)+\n  labs(x = \"Letter\", y = \"Frequency\",\n       title = \"Frequency of letters in Official Wordle list\")\n\n\n\n\n\nThis is interesting. Now I‚Äôm curious to know the top words by each position. To do this, I created a single table called freq_table that provides me with the frequency of occurrences by position for each letter. To iterate this process across all the 5 places, I used a for loop. Output is generated via the kableExtra package which provides a neat scrollable window\n\n\nCode\n#declaring null table\nfreq_table <- tibble(alpha = LETTERS)\n\nfor(i in 1:5){\n    test <- position_word_list %>% \n    select(all_of(i)) %>%\n# group_by_at() used for column index ID\n    group_by_at(1) %>% \n    summarise(f = n()) %>% \n    arrange(desc(f)) %>% \n#first column returns p1, p2.. etc and is standardised\n    rename(a = 1) \n\n#adding the freq values to a new dataframe\n    freq_table <- freq_table %>%\n    left_join(test, by = c(\"alpha\" = \"a\")) \n\n#renaming column name to reflect the position number\n    colnames(freq_table)[1+i] = paste0(\"p\",i)\n    rm(test)\n}\n#replacing NA with zero\nfreq_table[is.na(freq_table)] <- 0 \n#output using kable's scrollable window \nkable(freq_table, \n      format = \"html\", \n      caption = \"Frequency Table\") %>%\n    kable_styling() %>%\n    scroll_box(width = \"70%\", height = \"300px\") %>% \n  kable_classic()\n\n\n\n\nFrequency Table\n \n  \n    alpha \n    p1 \n    p2 \n    p3 \n    p4 \n    p5 \n  \n \n\n  \n    A \n    140 \n    304 \n    306 \n    162 \n    63 \n  \n  \n    B \n    173 \n    16 \n    56 \n    24 \n    11 \n  \n  \n    C \n    198 \n    40 \n    56 \n    150 \n    31 \n  \n  \n    D \n    111 \n    20 \n    75 \n    69 \n    118 \n  \n  \n    E \n    72 \n    241 \n    177 \n    318 \n    422 \n  \n  \n    F \n    135 \n    8 \n    25 \n    35 \n    26 \n  \n  \n    G \n    115 \n    11 \n    67 \n    76 \n    41 \n  \n  \n    H \n    69 \n    144 \n    9 \n    28 \n    137 \n  \n  \n    I \n    34 \n    201 \n    266 \n    158 \n    11 \n  \n  \n    J \n    20 \n    2 \n    3 \n    2 \n    0 \n  \n  \n    K \n    20 \n    10 \n    12 \n    55 \n    113 \n  \n  \n    L \n    87 \n    200 \n    112 \n    162 \n    155 \n  \n  \n    M \n    107 \n    38 \n    61 \n    68 \n    42 \n  \n  \n    N \n    37 \n    87 \n    137 \n    182 \n    130 \n  \n  \n    O \n    41 \n    279 \n    243 \n    132 \n    58 \n  \n  \n    P \n    141 \n    61 \n    57 \n    50 \n    56 \n  \n  \n    Q \n    23 \n    5 \n    1 \n    0 \n    0 \n  \n  \n    R \n    105 \n    267 \n    163 \n    150 \n    212 \n  \n  \n    S \n    365 \n    16 \n    80 \n    171 \n    36 \n  \n  \n    T \n    149 \n    77 \n    111 \n    139 \n    253 \n  \n  \n    U \n    33 \n    185 \n    165 \n    82 \n    1 \n  \n  \n    V \n    43 \n    15 \n    49 \n    45 \n    0 \n  \n  \n    W \n    82 \n    44 \n    26 \n    25 \n    17 \n  \n  \n    X \n    0 \n    14 \n    12 \n    3 \n    8 \n  \n  \n    Y \n    6 \n    22 \n    29 \n    3 \n    364 \n  \n  \n    Z \n    3 \n    2 \n    11 \n    20 \n    4 \n  \n\n\n\n\n\nThis table looks good. However, for my visualisation, I want to plot the top 10 letters in each position. For this, I‚Äôm going to use pivot_longer() to make it easier to generate the viz.\n\n\nCode\nfreq_table_long10 <- freq_table %>% \n  pivot_longer(cols = !alpha, names_to = \"position\", values_to = \"freq\") %>% \n  select(position, alpha, freq) %>% \n  arrange(position, -freq) %>% \n  group_by(position) %>% \n  slice_head(n = 10) %>% ungroup\n\nkable(freq_table_long10, \n      format = \"html\", \n      caption = \"Top 10 letters within each position\") %>%\n    kable_styling() %>%\n    scroll_box(height = \"200px\") %>% \n  kable_classic()\n\n\n\n\nTop 10 letters within each position\n \n  \n    position \n    alpha \n    freq \n  \n \n\n  \n    p1 \n    S \n    365 \n  \n  \n    p1 \n    C \n    198 \n  \n  \n    p1 \n    B \n    173 \n  \n  \n    p1 \n    T \n    149 \n  \n  \n    p1 \n    P \n    141 \n  \n  \n    p1 \n    A \n    140 \n  \n  \n    p1 \n    F \n    135 \n  \n  \n    p1 \n    G \n    115 \n  \n  \n    p1 \n    D \n    111 \n  \n  \n    p1 \n    M \n    107 \n  \n  \n    p2 \n    A \n    304 \n  \n  \n    p2 \n    O \n    279 \n  \n  \n    p2 \n    R \n    267 \n  \n  \n    p2 \n    E \n    241 \n  \n  \n    p2 \n    I \n    201 \n  \n  \n    p2 \n    L \n    200 \n  \n  \n    p2 \n    U \n    185 \n  \n  \n    p2 \n    H \n    144 \n  \n  \n    p2 \n    N \n    87 \n  \n  \n    p2 \n    T \n    77 \n  \n  \n    p3 \n    A \n    306 \n  \n  \n    p3 \n    I \n    266 \n  \n  \n    p3 \n    O \n    243 \n  \n  \n    p3 \n    E \n    177 \n  \n  \n    p3 \n    U \n    165 \n  \n  \n    p3 \n    R \n    163 \n  \n  \n    p3 \n    N \n    137 \n  \n  \n    p3 \n    L \n    112 \n  \n  \n    p3 \n    T \n    111 \n  \n  \n    p3 \n    S \n    80 \n  \n  \n    p4 \n    E \n    318 \n  \n  \n    p4 \n    N \n    182 \n  \n  \n    p4 \n    S \n    171 \n  \n  \n    p4 \n    A \n    162 \n  \n  \n    p4 \n    L \n    162 \n  \n  \n    p4 \n    I \n    158 \n  \n  \n    p4 \n    C \n    150 \n  \n  \n    p4 \n    R \n    150 \n  \n  \n    p4 \n    T \n    139 \n  \n  \n    p4 \n    O \n    132 \n  \n  \n    p5 \n    E \n    422 \n  \n  \n    p5 \n    Y \n    364 \n  \n  \n    p5 \n    T \n    253 \n  \n  \n    p5 \n    R \n    212 \n  \n  \n    p5 \n    L \n    155 \n  \n  \n    p5 \n    H \n    137 \n  \n  \n    p5 \n    N \n    130 \n  \n  \n    p5 \n    D \n    118 \n  \n  \n    p5 \n    K \n    113 \n  \n  \n    p5 \n    A \n    63 \n  \n\n\n\n\n\nSo we have the # of occurences in each position laid out in a tidy format in one long rectangular dataframe. Now sprinkling some magic courtesy ggplot\n\nSide note on reordering within facets\nI tried my best to understand why I was unable to sort within each facet in spite of using free_y. Apparently that‚Äôs a known issue and a workaround has been discussed by David Robinson, Julia Silger and Tyler Rinker. To achieve this, two more functions need to be created reorder_within and scale_y_reordered\n\n\nCode\nreorder_within <- function(x, by, within, fun = mean, sep = \"___\", ...) {\n  new_x <- paste(x, within, sep = sep)\n  stats::reorder(new_x, by, FUN = fun)\n}\n\nscale_y_reordered <- function(..., sep = \"___\") {\n  reg <- paste0(sep, \".+$\")\n  ggplot2::scale_y_discrete(labels = function(x) gsub(reg, \"\", x), ...)\n}\n\nfreq_table_long10 %>% \n  mutate(type = case_when(alpha %in% c(\"A\",\"E\",\"I\",\"O\",\"U\") ~ \"vowel\",\n                          T ~ \"consonant\")) %>% \n  ggplot(aes(y = reorder_within(alpha, freq, position), x = freq))+\n  geom_col(aes(fill = type))+\n  scale_y_reordered()+\n  facet_wrap(~position, \n             scales = \"free_y\", \n             ncol = 5)+\n  labs(x = \"Frequency\", y = \"Letter\",\n       title = \"Frequency of top 10 letters by position in Official Wordle list \",\n       caption = \"D.S.Ramakant Raju\\nwww.linkedin.com/in/dsramakant/\")\n\n\n\n\n\nAha! Things are starting to get more clearer. Highly common letters in the 1st position are S, C, B, T and P - notice there‚Äôs only 1 vowel (A) that occurs in the top 10. Vowels appear more frequently in the 2nd and 3rd positions. Last position has a higher occurrence of E, Y, T, R & L"
  },
  {
    "objectID": "posts/2022-04-05-what-a-wonderful-wordle/index.en.html#which-words-can-be-the-best-worlde-openers",
    "href": "posts/2022-04-05-what-a-wonderful-wordle/index.en.html#which-words-can-be-the-best-worlde-openers",
    "title": "What a wonderful ‚Äòwordle‚Äô!",
    "section": "Which words can be the best Worlde openers?",
    "text": "Which words can be the best Worlde openers?\nArmed with the above knowledge, we now can filter out the commonly occurring words. Also I use a naive method to rank these words basis the occurrence of the letters. For instance, in the picture above, the word S A I N T seems to be a valid word comprising of the top occurring letters.\nAdmittedly, I use a pretty crude method to determine the best openers. Known drawbacks of this methodology are:\n\nDoesn‚Äôt consider the future path of the word (number of steps to get to the right word)\nOnly considers the rank of the letters and not the actual probability of occurrence\n\nWith that out of the way, I was able to determine that there are 39 words that can be formed with the top 5 occurring letters in each position. I‚Äôve created a score that is determined by the rank of each letter within its position. For instance, S A I N T gets a score of 9 by summing up 1 (S in first position) + 1 (A in second position) + 2 (I in third) + 2 (N in fourth) + 3 (T in fifth). The lower the score, the higher the frequency of occurrences. Scroll below to read the rest of the words.\n\n\nCode\n#function to pick the top 5 letters\ntop5_selection <- function(x)\n{x %>% arrange(desc(x[2])) %>% head(5) %>% select(1)}\n#defining null table\nfinal_grid <- tibble(ranking = 1:5)\n\nfor(i in 2:length(freq_table)){\n  t <- top5_selection(select(freq_table,1,all_of(i)))\n  final_grid <- cbind(final_grid,t)\n  colnames(final_grid)[i] = paste0(\"p\",i-1)\n}\ntopwords <- position_word_list %>% \nfilter(p1 %in% final_grid$p1,\n       p2 %in% final_grid$p2,\n       p3 %in% final_grid$p3,\n       p4 %in% final_grid$p4,\n       p5 %in% final_grid$p5) \n\n#finding consolidated score of each word\ntopwords %<>%\n  rowwise() %>% \n  mutate(p1_rank = which(p1 == final_grid$p1),\n         p2_rank = which(p2 == final_grid$p2),\n         p3_rank = which(p3 == final_grid$p3),\n         p4_rank = which(p4 == final_grid$p4),\n         p5_rank = which(p5 == final_grid$p5))\n\ntopwords2 <- topwords %>% \n  transmute(word = paste0(p1,p2,p3,p4,p5),\n         score = sum(p1_rank, p2_rank,p3_rank, p4_rank, p5_rank)) %>% \n  arrange(score)\n\nkable(topwords2, \n      format = \"html\",\n      caption = \"Top 39 words\") %>%\n    kable_styling() %>%\n    scroll_box(width = \"50%\", height = \"400px\") %>% \n  kable_classic()\n\n\n\n\nTop 39 words\n \n  \n    word \n    score \n  \n \n\n  \n    SAINT \n    9 \n  \n  \n    CRANE \n    9 \n  \n  \n    COAST \n    11 \n  \n  \n    BRINE \n    11 \n  \n  \n    CEASE \n    11 \n  \n  \n    CRONE \n    11 \n  \n  \n    CAUSE \n    12 \n  \n  \n    CRIER \n    12 \n  \n  \n    BRINY \n    12 \n  \n  \n    BOAST \n    12 \n  \n  \n    TAINT \n    12 \n  \n  \n    CRONY \n    12 \n  \n  \n    TEASE \n    13 \n  \n  \n    POISE \n    13 \n  \n  \n    TOAST \n    13 \n  \n  \n    PAINT \n    13 \n  \n  \n    BOOST \n    14 \n  \n  \n    POINT \n    14 \n  \n  \n    COUNT \n    14 \n  \n  \n    PRONE \n    14 \n  \n  \n    BEAST \n    14 \n  \n  \n    PRINT \n    15 \n  \n  \n    PAUSE \n    15 \n  \n  \n    TAUNT \n    15 \n  \n  \n    PROSE \n    15 \n  \n  \n    CREST \n    15 \n  \n  \n    CRUST \n    16 \n  \n  \n    BRIAR \n    16 \n  \n  \n    BOULE \n    16 \n  \n  \n    POESY \n    16 \n  \n  \n    CRUEL \n    16 \n  \n  \n    PRUNE \n    16 \n  \n  \n    BRUNT \n    16 \n  \n  \n    TRUER \n    17 \n  \n  \n    TREAT \n    18 \n  \n  \n    TRIAL \n    18 \n  \n  \n    TRUST \n    18 \n  \n  \n    TRULY \n    19 \n  \n  \n    TROLL \n    20 \n  \n\n\n\n\n\nThere we have it. My take on the best opening words.\nI‚Äôve used words such as SAINT, CRANE, COAST etc and they‚Äôve been reasonably useful to me.\nWhich are your favourite opening words? Please do leave a comment to let me know!"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn‚Äôt specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/2022-06-01-tidytuesday-nyt-bestsellers-list/index.en.html",
    "href": "posts/2022-06-01-tidytuesday-nyt-bestsellers-list/index.en.html",
    "title": "TidyTuesday NYT Bestsellers list",
    "section": "",
    "text": "‚ÄúLearn by practice!‚Äù is a maxim that every coder/analyst agrees upon. One of the admirable initiatives by the R/ RStudio community is Tidy Tuesday - every week a new dataset is released for enthusiasts to dig into. A few days back, an interesting dataset caught my eye - NYT‚Äôs Bestsellers List from 1930 to 2021. This one was particularly unique as it mirrored a lot of projects that I‚Äôve been doing on the OTT side as well. So I cracked my knuckles and jumped right in!"
  },
  {
    "objectID": "posts/2022-06-01-tidytuesday-nyt-bestsellers-list/index.en.html#objective",
    "href": "posts/2022-06-01-tidytuesday-nyt-bestsellers-list/index.en.html#objective",
    "title": "TidyTuesday NYT Bestsellers list",
    "section": "Objective",
    "text": "Objective\n\nUnderstanding longevity & seasonality of how books track on the NYT bestseller‚Äôs list\nDeeper understanding of using customizing themes and fonts on the ggplot package\n\n\nLoading the data\nStarting off by loading the data and the libraries\n\n\nCode\nknitr::opts_chunk$set(warning = FALSE, message = FALSE) \nsuppressMessages({\n  library(tidyverse)\n  library(scales)\n  library(knitr)\n  library(tidytuesdayR)\n  library(forcats)\n  library(lubridate)\n  library(RColorBrewer)\n})\n\ntt_raw <- tt_load(\"2022-05-10\")\n\n\n--- Compiling #TidyTuesday Information for 2022-05-10 ----\n\n\n--- There are 2 files available ---\n\n\n--- Starting Download ---\n\n\n\n    Downloading file 1 of 2: `nyt_titles.tsv`\n    Downloading file 2 of 2: `nyt_full.tsv`\n\n\n--- Download complete ---\n\n\nCode\ndata <- tt_raw$nyt_titles\n\n\nWhat‚Äôs in the Tidy Tuesday dataset?\n\n\nCode\nglimpse(data)\n\n\nRows: 7,431\nColumns: 8\n$ id          <dbl> 0, 1, 10, 100, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1‚Ä¶\n$ title       <chr> \"\\\"H\\\" IS FOR HOMICIDE\", \"\\\"I\\\" IS FOR INNOCENT\", \"''G'' I‚Ä¶\n$ author      <chr> \"Sue Grafton\", \"Sue Grafton\", \"Sue Grafton\", \"W. Bruce Cam‚Ä¶\n$ year        <dbl> 1991, 1992, 1990, 2012, 2006, 2016, 1985, 1994, 2002, 1999‚Ä¶\n$ total_weeks <dbl> 15, 11, 6, 1, 1, 3, 16, 5, 4, 1, 3, 2, 11, 6, 9, 8, 1, 1, ‚Ä¶\n$ first_week  <date> 1991-05-05, 1992-04-26, 1990-05-06, 2012-05-27, 2006-02-1‚Ä¶\n$ debut_rank  <dbl> 1, 14, 4, 3, 11, 1, 9, 7, 7, 12, 13, 5, 12, 2, 11, 13, 2, ‚Ä¶\n$ best_rank   <dbl> 2, 2, 8, 14, 14, 7, 2, 10, 12, 17, 13, 13, 8, 5, 5, 11, 4,‚Ä¶\n\n\n\n\nExploratory Data Analysis (sort of)\nQuick EDA tells us that there the number of books in the #1 spot each year during the 50s have been increasing while the number of weeks they‚Äôve spent on the NYT list has been decreasing. 2020-21 is excluded as I‚Äôm breaking up the period into decades for easy analysis\n\n\nCode\ndata %>% \n  mutate(decade = factor(10*year %/% 10)) %>% \n  filter(best_rank==1, year<2020) %>% \n  group_by(decade) %>% \n  summarise(avg_weeks = mean(total_weeks),\n            no_of_rank1 = n_distinct(title))\n\n\n# A tibble: 9 √ó 3\n  decade avg_weeks no_of_rank1\n  <fct>      <dbl>       <int>\n1 1930        17.1          74\n2 1940        30.1          59\n3 1950        52.4          35\n4 1960        45.7          31\n5 1970        38.6          46\n6 1980        29.5          78\n7 1990        25.7          99\n8 2000        12.5         220\n9 2010        10.3         306\n\n\nThis is a fantastic starting point. Intuitively, this makes a lot of sense. There‚Äôs far more competition for the #1 spot in the last 20 years which is driving down the longevity. Compare the 50‚Äôs to the 2010‚Äôs and the trend is hard to miss. This table is only for the books that made it to the #1 position. But how about the rest of the other books? A visual representation draws the same conclusion more elegantly.\n\n\nVisualising Longevity\nHat-tip to a few outstanding viz I came across while researching the NYT theme. Bob Rudis‚Äô Supreme Annotations and Rahul Sangole‚Äôs Visualizing Correlations\n\n\nCode\n#loading fonts that resemble the NYT viz\n#inspired by https://rud.is/b/2016/03/16/supreme-annotations/\n\nlibrary(showtext)\nshowtext_auto()\nfont_add(family = \"Open Sans\", \n         regular = \"OpenSans-CondLight.ttf\", \n         italic = \"OpenSans-CondLightItalic.ttf\", \n         bold = \"OpenSans-CondBold.ttf\")\n\n#changing facet labels as shown here \n#https://ggplot2.tidyverse.org/reference/as_labeller.html\nfacet_labels <- as_labeller(c(`1930`= \"1930 to 1939\",\n                              `1940`= \"1940 to 1949\",\n                              `1950`= \"1950 to 1959\",\n                              `1960`= \"1960 to 1969\",\n                              `1970`= \"1970 to 1979\",\n                              `1980`= \"1980 to 1989\",\n                              `1990`= \"1990 to 1999\",\n                              `2000`= \"2000 to 2009\",\n                              `2010`= \"2010 to 2019\"))\n\n\n#annotations for individual facet as discussed here https://stackoverflow.com/a/11889798/7938068\nannot_x <- data.frame(debut_rank = 5, \n                      total_weeks = 111,\n                      lab = \"Each dot\\n is a book\",\n                      decade = 1940)\n\ngraph1 <- data %>% \n  filter(best_rank==1,year<2020) %>% \n  mutate(decade = factor(10*year %/% 10)) %>% \n  ggplot(aes(x = debut_rank, y  = total_weeks))+\n  geom_point(aes(color = decade, group = debut_rank))+\n  facet_grid(~decade ,labeller = facet_labels)\n\ngraph1 <- graph1+\n  theme_minimal(base_family = \"Open Sans\")+\n  scale_color_brewer(palette = \"Paired\")+\n  labs(title = \"Longevity of NYT bestsellers has been decreasing\", \n       subtitle = \"Analysis of books that reached highest of #1 on the NYT chart tells us that starting from the 1950s, the bestsellers have reduced their longevity - or time spent on the chart.\\nFor instance, the top ranked books released in the 50s spent around 52 weeks on the chart while in contrast by the 2010s, they only spent 10 weeks.\",\n       caption = \"TidyTuesday Week 19, 2022\\n Prepared by D.S.Ramakant Raju (www.ds-ramakant.com)\",\n       x = \"Rank of title on debut week\",\n       y = \"Number of weeks on the bestsellers list\")+\n  theme(panel.border = element_rect(color = \"#2b2b2b\", \n                                    fill = NA), #borders for each facet panel\n        legend.position = \"none\", #removing legend\n        strip.text = element_text(face = \"italic\"),\n        plot.title = element_text(size = 14, face = \"bold\"),\n        panel.grid.major.x = element_line(linetype = \"dotted\", \n                                          color = \"black\"),\n        panel.grid.minor.x = element_blank(),\n        panel.grid.minor.y = element_blank(),\n        plot.caption = element_text(size = 12),\n        plot.subtitle = element_text(size = 12)) +\n  scale_y_continuous(breaks = seq(from = 25, to = 175, by = 25))+\n  #annotations by default is applied to all facets\n  #for individual facet annotations, check https://stackoverflow.com/a/11889798/7938068\n  geom_text(data = annot_x, \n            aes(x = debut_rank, y = total_weeks, \n                family = \"Open Sans\", alpha = 0.8,\n                hjust = -0.2, vjust = -0.2),\n            label = annot_x$lab\n            )\n\n\n\nprint(graph1)\n\n\n\n\n\n\n\nVisualising Seasonality\nNow lets look at seasonality - is there any trend as far as the launch month is concerned? For the sake of analysis, I‚Äôve truncated the analysis period to 2010 onwards to keep it more relevant and exlcude irrelvant historical data.\n\n\nCode\ngraph2 <- data %>% \n  filter(best_rank<11, year> 2010) %>% \n  mutate(month = month(first_week, label = T),\n         stage = case_when(year<=2015 ~ \"2011-2015\",\n                           year> 2015 ~ \"2016-2020\",\n                           T ~ \"x\")) %>% \n  group_by(stage,year,month) %>% \n  summarise(n = n_distinct(title)) %>% \n  mutate(all_titles = ave(n, year, FUN = sum),\n         pct = n/all_titles) %>%  \n  ggplot(aes(x = month, y = pct, group = 1))+\n  geom_point(size = 2, \n             alpha = 0.5, position = \"jitter\")+\n  geom_smooth(se = T) \n\n\ngraph2 <- graph2+\n  theme_minimal(base_family = \"Open Sans\")+\n  scale_color_brewer(palette = \"Paired\")+\n  labs(title = \"Monthly seasonality of books that featured in the top 10 of NYT Bestsellers list (2010-2021)\", \n       subtitle = \"Books launched in Summer (Apr-May) or Fall (Sep-Oct) were more likely to make it feature in the top 10\",\n       caption = \"TidyTuesday Week 19, 2022\\n Prepared by D.S.Ramakant Raju, www.ds-ramakant.com\",\n       x = \"Months (2010-2019)\",\n       y = \"%age of books launched within that year\")+\n  scale_y_continuous(labels = label_percent(accuracy = 1),\n                     breaks = seq(from = 0, to = 0.2, by= 0.05), \n                     limits = c(0,0.15))+\n  theme(axis.line.x = element_line(color = \"grey\"),\n        panel.grid.minor.y = element_blank())\n\ngraph2\n\n\n\n\n\nThis is a fairly straightforward and replicable analysis. If you‚Äôre a #TidyTuesday fan please feel free to share your work in the comments below"
  },
  {
    "objectID": "posts/2022-10-07-day-1-of-50daysofkaggle/index.en.html",
    "href": "posts/2022-10-07-day-1-of-50daysofkaggle/index.en.html",
    "title": "day 1 of #50daysofkaggle",
    "section": "",
    "text": "Introducing my own personal sprint training ‚Äú50 Days of Kaggle‚Äù\nThe task is simple:\nI‚Äôd want to use this blog to journal my progress. Hopefully by 26th Nov‚Äô22, I‚Äôd have improved from where I‚Äôm starting out.\nSo what do we have for Day 1?"
  },
  {
    "objectID": "posts/2022-10-07-day-1-of-50daysofkaggle/index.en.html#reading-the-data",
    "href": "posts/2022-10-07-day-1-of-50daysofkaggle/index.en.html#reading-the-data",
    "title": "day 1 of #50daysofkaggle",
    "section": "Reading the data",
    "text": "Reading the data\nFirst things first, import libraries\n\n\nCode\nimport requests\nimport numpy as np\nimport pandas as pd\nimport kaggle\n\nkaggle.api.authenticate()\n\n\nNote to self: below command did not work\n\n\nCode\n#kaggle.api.dataset_download_files(\"titanic\", path = \".\", unzip = True)\n\n\nHowever, this one does as per this link https://www.kaggle.com/general/138914\n\n\nCode\nkaggle.api.competition_download_files(\"titanic\", path = \".\")\n\n\nThis pulls the .zip file in the local folder. because this is a zip file, we need package called zipfile(note to self: don‚Äôt forget the console command reticulate::py_install(\"zipfile\"))\nhttps://stackoverflow.com/a/56786517/7938068\nReading and checking the first rows of train\n\n\nCode\nimport zipfile\n\nzf = zipfile.ZipFile(\"titanic.zip\")\ntrain = pd.read_csv(zf.open(\"train.csv\"))\ntest = pd.read_csv(zf.open(\"test.csv\"))\ntrain.head()\n\n\n\n\n\n\n  \n    \n      \n      PassengerId\n      Survived\n      Pclass\n      Name\n      Sex\n      Age\n      SibSp\n      Parch\n      Ticket\n      Fare\n      Cabin\n      Embarked\n    \n  \n  \n    \n      0\n      1\n      0\n      3\n      Braund, Mr. Owen Harris\n      male\n      22.0\n      1\n      0\n      A/5 21171\n      7.2500\n      NaN\n      S\n    \n    \n      1\n      2\n      1\n      1\n      Cumings, Mrs. John Bradley (Florence Briggs Th...\n      female\n      38.0\n      1\n      0\n      PC 17599\n      71.2833\n      C85\n      C\n    \n    \n      2\n      3\n      1\n      3\n      Heikkinen, Miss. Laina\n      female\n      26.0\n      0\n      0\n      STON/O2. 3101282\n      7.9250\n      NaN\n      S\n    \n    \n      3\n      4\n      1\n      1\n      Futrelle, Mrs. Jacques Heath (Lily May Peel)\n      female\n      35.0\n      1\n      0\n      113803\n      53.1000\n      C123\n      S\n    \n    \n      4\n      5\n      0\n      3\n      Allen, Mr. William Henry\n      male\n      35.0\n      0\n      0\n      373450\n      8.0500\n      NaN\n      S\n    \n  \n\n\n\n\nChecking the first rows of test\n\n\nCode\ntest.head()\n\n\n\n\n\n\n  \n    \n      \n      PassengerId\n      Pclass\n      Name\n      Sex\n      Age\n      SibSp\n      Parch\n      Ticket\n      Fare\n      Cabin\n      Embarked\n    \n  \n  \n    \n      0\n      892\n      3\n      Kelly, Mr. James\n      male\n      34.5\n      0\n      0\n      330911\n      7.8292\n      NaN\n      Q\n    \n    \n      1\n      893\n      3\n      Wilkes, Mrs. James (Ellen Needs)\n      female\n      47.0\n      1\n      0\n      363272\n      7.0000\n      NaN\n      S\n    \n    \n      2\n      894\n      2\n      Myles, Mr. Thomas Francis\n      male\n      62.0\n      0\n      0\n      240276\n      9.6875\n      NaN\n      Q\n    \n    \n      3\n      895\n      3\n      Wirz, Mr. Albert\n      male\n      27.0\n      0\n      0\n      315154\n      8.6625\n      NaN\n      S\n    \n    \n      4\n      896\n      3\n      Hirvonen, Mrs. Alexander (Helga E Lindqvist)\n      female\n      22.0\n      1\n      1\n      3101298\n      12.2875\n      NaN\n      S\n    \n  \n\n\n\n\nThis took me a whole day to figure out. End of Day1 ü§∑‚Äç‚ôÇÔ∏è"
  },
  {
    "objectID": "posts/2022-10-10-day-4-of-50daysofkaggle/index.en.html",
    "href": "posts/2022-10-10-day-4-of-50daysofkaggle/index.en.html",
    "title": "Day 4 of #50daysofkaggle",
    "section": "",
    "text": "Previously I managed to download the titanic zip file using the kaggle api and extract two datasets train and test .\n\n\n\n\nCode\nimport requests\nimport numpy as np\nimport pandas as pd\nimport kaggle \nimport zipfile \n\nkaggle.api.authenticate()\n\nkaggle.api.competition_download_files(\"titanic\", path = \".\")\n\nzf = zipfile.ZipFile(\"titanic.zip\")\ntrain = pd.read_csv(zf.open(\"train.csv\"))\ntest = pd.read_csv(zf.open(\"test.csv\"))\n\n\n\n\n\nLets see what we have here in the train data\n\n\nCode\ntrain.head()\n\n\n\n\n\n\n  \n    \n      \n      PassengerId\n      Survived\n      Pclass\n      Name\n      Sex\n      Age\n      SibSp\n      Parch\n      Ticket\n      Fare\n      Cabin\n      Embarked\n    \n  \n  \n    \n      0\n      1\n      0\n      3\n      Braund, Mr. Owen Harris\n      male\n      22.0\n      1\n      0\n      A/5 21171\n      7.2500\n      NaN\n      S\n    \n    \n      1\n      2\n      1\n      1\n      Cumings, Mrs. John Bradley (Florence Briggs Th...\n      female\n      38.0\n      1\n      0\n      PC 17599\n      71.2833\n      C85\n      C\n    \n    \n      2\n      3\n      1\n      3\n      Heikkinen, Miss. Laina\n      female\n      26.0\n      0\n      0\n      STON/O2. 3101282\n      7.9250\n      NaN\n      S\n    \n    \n      3\n      4\n      1\n      1\n      Futrelle, Mrs. Jacques Heath (Lily May Peel)\n      female\n      35.0\n      1\n      0\n      113803\n      53.1000\n      C123\n      S\n    \n    \n      4\n      5\n      0\n      3\n      Allen, Mr. William Henry\n      male\n      35.0\n      0\n      0\n      373450\n      8.0500\n      NaN\n      S\n    \n  \n\n\n\n\nChecking more details on train columns.\n\n\nCode\ntrain.info()\n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.7+ KB\n\n\nPassengerID is the unique identifier for each row while Survived is the column to be predicted. Finding only the numeric columns and dropping the above two (ref - this link)\n\n\nCode\nnum_col = train.select_dtypes(include=np.number).columns.tolist()\n\ndel num_col[0:2] #.remove() can remove only 1 item. so for more than 1, use for loop \nselect_col = num_col\n\n\nAmong the string columns, only Sex and Embarked are relevant for our analysis. Ref - selecting columns by intersection\n\n\nCode\nstr_col= [\"Sex\", \"Embarked\", \"Survived\"]\nstr_col\n\n\nselect_col.extend(str_col)\nselect_col\n\ntrain_eda= train[train.columns.intersection(select_col)]\n\ntrain_eda.info()\n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 891 entries, 0 to 890\nData columns (total 8 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Survived  891 non-null    int64  \n 1   Pclass    891 non-null    int64  \n 2   Sex       891 non-null    object \n 3   Age       714 non-null    float64\n 4   SibSp     891 non-null    int64  \n 5   Parch     891 non-null    int64  \n 6   Fare      891 non-null    float64\n 7   Embarked  889 non-null    object \ndtypes: float64(2), int64(4), object(2)\nmemory usage: 55.8+ KB\n\n\n\n\n\nSeems like the older folks were luckier than the younger ones\n\n\nCode\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.catplot(data = train_eda, x = \"Age\", y = \"Fare\", hue = \"Survived\")\n            \nplt.show()\n\n\n\n\n\nDistinction between Class 1 and Class 3 is clear - poorer folks in Class 3 were younger (mean being just under 30 years) than the richer folks in Class 1\n\n\nCode\nplt.clf()\nsns.boxplot(data = train_eda, y = \"Age\", x = \"Pclass\", hue = \"Survived\")\nplt.show()\n\n\n\n\n\nBelow graph shows us that among the survivors, there were a lot more women than men survived the disaster.\n\n\nCode\nplt.clf()\nplot = sns.FacetGrid(data = train_eda, col = \"Survived\", hue = \"Sex\", col_wrap = 2)\nplot.map(sns.scatterplot, \"Age\", \"Fare\")\nplot.axes[-1].legend()\nplt.show()\n\n\n<Figure size 672x480 with 0 Axes>\n\n\n\n\n\nWe continue to notice the clearer skew towards Class 1 (richer) compared to Class 3 (poorer)\n\n\nCode\nplt.clf()\nplot = sns.FacetGrid(data = train_eda, col = \"Survived\", hue = \"Pclass\", col_wrap = 2)\nplot.map(sns.scatterplot, \"Age\", \"Fare\")\nplot.axes[-1].legend()\nplt.show()\n\n\n<Figure size 672x480 with 0 Axes>\n\n\n\n\n\n\n\nCode\nplt.clf()\nplot = sns.FacetGrid(data = train_eda, col = \"Survived\", hue = \"SibSp\", col_wrap = 2)\nplot.map(sns.scatterplot, \"Age\", \"Fare\")\nplot.axes[-1].legend()\nplt.show()\n\n\n<Figure size 672x480 with 0 Axes>\n\n\n\n\n\n\n\n\n\n\nCode\ntrain_df = pd.get_dummies(train_eda, columns = [\"Sex\", \"Embarked\"])\n\ntrain_df.info()\n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 891 entries, 0 to 890\nData columns (total 11 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   Survived    891 non-null    int64  \n 1   Pclass      891 non-null    int64  \n 2   Age         714 non-null    float64\n 3   SibSp       891 non-null    int64  \n 4   Parch       891 non-null    int64  \n 5   Fare        891 non-null    float64\n 6   Sex_female  891 non-null    uint8  \n 7   Sex_male    891 non-null    uint8  \n 8   Embarked_C  891 non-null    uint8  \n 9   Embarked_Q  891 non-null    uint8  \n 10  Embarked_S  891 non-null    uint8  \ndtypes: float64(2), int64(4), uint8(5)\nmemory usage: 46.2 KB\n\n\nAnd day 2 comes to an endü§∑‚Äç‚ôÇÔ∏è"
  },
  {
    "objectID": "posts/2022-10-12-day-6-of-50daysofkaggle/index.en.html",
    "href": "posts/2022-10-12-day-6-of-50daysofkaggle/index.en.html",
    "title": "Day 6 of #50daysofkaggle",
    "section": "",
    "text": "Progress till date:\n\nDownload titanic dataset and assign to train & test\nRearranging the data\nEDA\n\nTo do today:\n\nwrite function to find share of survivors by each variable\nattempt to create model\n\n\n\nLoading the data using kaggle library and examining the top rows of relevant columns.\n\n\nCode\nimport requests\nimport numpy as np\nimport pandas as pd\nimport kaggle \nimport zipfile \n\nkaggle.api.authenticate()\n\nkaggle.api.competition_download_files(\"titanic\", path = \".\")\n\nzf = zipfile.ZipFile(\"titanic.zip\")\ntrain = pd.read_csv(zf.open(\"train.csv\"))\ntest = pd.read_csv(zf.open(\"test.csv\"))\n\n#Selecting only the numerical columns\nnum_col = train.select_dtypes(include=np.number).columns.tolist()\n\n#deslecting passenger ID and 'Survived' \ndel num_col[0:2] #.remove() can remove only 1 item. so for more than 1, use for loop \nselect_col = num_col\n\n#remaining columns\nstr_col= [\"Sex\", \"Embarked\", \"Survived\"]\n\n#Adding more elements into a list using `extend` and not `append`\nselect_col.extend(str_col)\n\ntrain_eda= train[train.columns.intersection(select_col)]\ntrain_eda.head()\n\n\n\n\n\n\n  \n    \n      \n      Survived\n      Pclass\n      Sex\n      Age\n      SibSp\n      Parch\n      Fare\n      Embarked\n    \n  \n  \n    \n      0\n      0\n      3\n      male\n      22.0\n      1\n      0\n      7.2500\n      S\n    \n    \n      1\n      1\n      1\n      female\n      38.0\n      1\n      0\n      71.2833\n      C\n    \n    \n      2\n      1\n      3\n      female\n      26.0\n      0\n      0\n      7.9250\n      S\n    \n    \n      3\n      1\n      1\n      female\n      35.0\n      1\n      0\n      53.1000\n      S\n    \n    \n      4\n      0\n      3\n      male\n      35.0\n      0\n      0\n      8.0500\n      S\n    \n  \n\n\n\n\nHow many columns with na values?\n\n\nCode\ntrain_eda.isna().sum().sort_values()\n\n\nSurvived      0\nPclass        0\nSex           0\nSibSp         0\nParch         0\nFare          0\nEmbarked      2\nAge         177\ndtype: int64\n\n\nAlmost 177 entries in the Age column have no value. Calculating the median age of remaining data.\n\n\nCode\ntrain_eda[\"Age\"].median() #28\n\n\n28.0\n\n\nReplacing these with the median age (28) instead of removing them.\n\n\nCode\ntrain_eda[\"Age\"].fillna(value = train_eda[\"Age\"].median(), inplace = True)\ntrain_eda.isna().sum().sort_values()\n\n\nC:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_5312\\1076914416.py:1: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\nSurvived    0\nPclass      0\nSex         0\nAge         0\nSibSp       0\nParch       0\nFare        0\nEmbarked    2\ndtype: int64\n\n\nToday I want to calculate the survival rate of each of these attributes (Pclass, Sex, Embarked).\n\n\nCode\ndf_copy2 = pd.DataFrame(columns = {\"category\", \"col\", \"survive_rate\"})\n\nfor t in [\"Pclass\", \"Sex\", \"Embarked\"]:\n  df_copy = train_eda.groupby([t])[\"Survived\"].mean().reset_index()\n  df_copy[\"category\"] = t\n  #trying to create a `tidy` version of the data \n  df_copy.rename(columns = {t: \"col\", \"Survived\": \"survive_rate\"}, errors = \"raise\", inplace = True)\n  df_copy = df_copy[[\"category\", \"col\", \"survive_rate\"]]\n  df_copy2= pd.concat([df_copy2, df_copy], ignore_index = True)\n\n\n#final table in a tidy format that can be used to create graphs. but that i'm keeping for later\ndf_copy2[[\"category\", \"col\", \"survive_rate\"]]\n\n\n\n\n\n\n  \n    \n      \n      category\n      col\n      survive_rate\n    \n  \n  \n    \n      0\n      Pclass\n      1\n      0.62963\n    \n    \n      1\n      Pclass\n      2\n      0.472826\n    \n    \n      2\n      Pclass\n      3\n      0.242363\n    \n    \n      3\n      Sex\n      female\n      0.742038\n    \n    \n      4\n      Sex\n      male\n      0.188908\n    \n    \n      5\n      Embarked\n      C\n      0.553571\n    \n    \n      6\n      Embarked\n      Q\n      0.38961\n    \n    \n      7\n      Embarked\n      S\n      0.336957\n    \n  \n\n\n\n\nWith this, its pretty clear that among the sex category, males had the least likelihood of surviving with 19%. The richer class 1 managed a 63% chance of survival while only 24% of the lower class 3 survived. Finally those that embarked from Cherbourg had a higher survival rate 55% compared to Southampton at 34%.\n\n\n\nSeperating the X & y. Here‚Äôs the first 5 rows of X\n\n\nCode\ntrain_eda.isna().sum().sort_values()\ntrain_eda = train_eda.dropna(axis = 0) #removing all rows with NA\n\nX = train_eda[[\"Age\", \"SibSp\", \"Parch\", \"Fare\"]]\n\nX = pd.concat([X,pd.get_dummies(data = train_eda[[\"Sex\", \"Embarked\", \"Pclass\"]], columns = [\"Sex\", \"Embarked\", \"Pclass\"])], axis = 1)\n\nX.head()\n\n\n\n\n\n\n  \n    \n      \n      Age\n      SibSp\n      Parch\n      Fare\n      Sex_female\n      Sex_male\n      Embarked_C\n      Embarked_Q\n      Embarked_S\n      Pclass_1\n      Pclass_2\n      Pclass_3\n    \n  \n  \n    \n      0\n      22.0\n      1\n      0\n      7.2500\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      1\n    \n    \n      1\n      38.0\n      1\n      0\n      71.2833\n      1\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n    \n    \n      2\n      26.0\n      0\n      0\n      7.9250\n      1\n      0\n      0\n      0\n      1\n      0\n      0\n      1\n    \n    \n      3\n      35.0\n      1\n      0\n      53.1000\n      1\n      0\n      0\n      0\n      1\n      1\n      0\n      0\n    \n    \n      4\n      35.0\n      0\n      0\n      8.0500\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      1\n    \n  \n\n\n\n\nFirst 5 rows of y\n\n\nCode\ny = train_eda[\"Survived\"].values\ny[0:5]\n\n\narray([0, 1, 1, 1, 0], dtype=int64)\n\n\nChecking dimensions of y & X\n\n\nCode\nlen(y) #889 after filling up the NA. previously 712\nX.shape #(889, 12)\n\n\n(889, 12)\n\n\n\n\nTransform X and printing the first 5 datapoints\n\n\nCode\nfrom sklearn import preprocessing\n\nX= preprocessing.StandardScaler().fit(X).transform(X)\nX[0:5]\n\n\narray([[-0.56367407,  0.43135024, -0.47432585, -0.50023975, -0.73534203,\n         0.73534203, -0.48271079, -0.30794088,  0.61679395, -0.56306042,\n        -0.51087465,  0.90032807],\n       [ 0.66921696,  0.43135024, -0.47432585,  0.78894661,  1.35991138,\n        -1.35991138,  2.07163382, -0.30794088, -1.62128697,  1.77600834,\n        -0.51087465, -1.11070624],\n       [-0.25545131, -0.47519908, -0.47432585, -0.48664993,  1.35991138,\n        -1.35991138, -0.48271079, -0.30794088,  0.61679395, -0.56306042,\n        -0.51087465,  0.90032807],\n       [ 0.43804989,  0.43135024, -0.47432585,  0.42286111,  1.35991138,\n        -1.35991138, -0.48271079, -0.30794088,  0.61679395,  1.77600834,\n        -0.51087465, -1.11070624],\n       [ 0.43804989, -0.47519908, -0.47432585, -0.4841333 , -0.73534203,\n         0.73534203, -0.48271079, -0.30794088,  0.61679395, -0.56306042,\n        -0.51087465,  0.90032807]])\n\n\n\n\n\nSplitting into test & train data and comparing the dimensions.\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\nprint ('Train set\\t :', X_train.shape,  y_train.shape,\n'\\nTest set\\t :', X_test.shape,  y_test.shape)\n\n\nTrain set    : (711, 12) (711,) \nTest set     : (178, 12) (178,)\n\n\n\n\n\nUsing KNN at k = 4\n\n\nCode\nfrom sklearn.neighbors import KNeighborsClassifier\nk = 4\nneighbours = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)\nneighbours\n\n\nKNeighborsClassifier(n_neighbors=4)\n\n\n\n\n\n\nCode\nyhat1 = neighbours.predict(X_test)\nyhat1[0:5]\n\n\narray([0, 1, 0, 0, 1], dtype=int64)\n\n\nCalculating the accuracy at k = 4\n\n\nCode\nfrom sklearn import metrics\n\nprint(\"Train set Accuracy \\t:\", metrics.accuracy_score(y_train, neighbours.predict(X_train)), \"\\nTest set Accuracy \\t:\", metrics.accuracy_score(y_test, yhat1))\n\n\nTrain set Accuracy  : 0.8509142053445851 \nTest set Accuracy   : 0.7584269662921348\n\n\n(without replacing na values, the previous test accuracy was 78%)\n\n\n\n\n\nCode\nfrom sklearn import metrics\n\nKs = 10\nmean_acc = np.zeros((Ks-1))\nstd_acc = np.zeros((Ks-1))\n\nfor n in range(1,Ks):\n    #Train Model and Predict  \n    neigh = KNeighborsClassifier(n_neighbors = n).fit(X_train,y_train)\n    yhat=neigh.predict(X_test)\n    mean_acc[n-1] = metrics.accuracy_score(y_test, yhat)\n    std_acc[n-1]=np.std(yhat==y_test)/np.sqrt(yhat.shape[0])\n\nmean_acc\n\n\narray([0.78651685, 0.76404494, 0.7752809 , 0.75842697, 0.78089888,\n       0.78651685, 0.80337079, 0.7752809 , 0.78089888])\n\n\nGlad that IBM coursera assignments came in handy! Now visualising the accuracy across each K\n\n\nCode\nimport matplotlib.pyplot as plt\n\nplt.plot(range(1,Ks),mean_acc,'g')\nplt.fill_between(range(1,Ks),mean_acc - 1 * std_acc,mean_acc + 1 * std_acc, alpha=0.10)\nplt.fill_between(range(1,Ks),mean_acc - 3 * std_acc,mean_acc + 3 * std_acc, alpha=0.10,color=\"green\")\nplt.legend(('Accuracy ', '+/- 1xstd','+/- 3xstd'))\nplt.ylabel('Accuracy ')\nplt.xlabel('Number of Neighbors (K)')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nLooks like accuracy of KNN is best at 7 neighbours. previously without replacing NA the accuracy was highest at k = 5\n\n\n\n\n\nCode\nk = 7\n\nneighbours_7 = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)\nyhat = neighbours_7.predict(X_test)\n\nprint(\"Train set Accuracy \\t:\", metrics.accuracy_score(y_train, neighbours_7.predict(X_train)),\"\\nTest set Accuracy \\t:\", metrics.accuracy_score(y_test, yhat),\"\\nRMSE \\t\\t\\t:\",metrics.mean_squared_error(y_test, yhat),\"\\nNormalised RMSE\\t\\t:\",metrics.mean_squared_error(y_test, yhat)/np.std(y_test))\n\n\nTrain set Accuracy  : 0.8509142053445851 \nTest set Accuracy   : 0.8033707865168539 \nRMSE            : 0.19662921348314608 \nNormalised RMSE     : 0.3997716243033934\n\n\nWe find that Test accuracy is around 80% for KNN1 with RMSE of 0.197 and Normalised RMSE of 40%2. formula for NRMSE here"
  },
  {
    "objectID": "posts/2022-10-14-day-8-of-kaggle/index.en.html",
    "href": "posts/2022-10-14-day-8-of-kaggle/index.en.html",
    "title": "Day 8 of #50daysofkaggle",
    "section": "",
    "text": "Progress till date:\n\nDownload titanic dataset and assign to train & test\nRearranging the data\nEDA (including plots and finding survival rate using .groupby())\nModelling\nData preparation - one-hot encoding the Sex, Pclass & Embarked columns - appending these to the numerical columns - normalising the data - splitting between train into X_train, y_train, X_test, y_test\nApplying KNN algo\n\nfinding the right K based on accuracy. (best at K = 7)\nCalculating the accuracy based on test\n\n\nTo do today: - Perform Decision Tree classification\n\n\nReading and printing the top 5 rows\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport zipfile\n\n\n#importing the zipfile already saved in the other folder. \nzf = zipfile.ZipFile(\"../2022-10-12-day-6-of-50daysofkaggle/titanic.zip\")\ntrain = pd.read_csv(zf.open(\"train.csv\"))\ntest = pd.read_csv(zf.open(\"test.csv\"))\n\n#Selecting only the numerical columns\nnum_col = train.select_dtypes(include=np.number).columns.tolist()\n\n#deslecting passenger ID and 'Survived' \ndel num_col[0:2] #.remove() can remove only 1 item. so for more than 1, use for loop \nselect_col = num_col\n\n#remaining columns\nstr_col= [\"Sex\", \"Embarked\", \"Survived\"]\n\n\n#Adding more elements into a list using `extend` and not `append`\nselect_col.extend(str_col)\n\ntrain_eda= train[train.columns.intersection(select_col)]\ntrain_eda.head()\n\n\n\n\n\n\n  \n    \n      \n      Survived\n      Pclass\n      Sex\n      Age\n      SibSp\n      Parch\n      Fare\n      Embarked\n    \n  \n  \n    \n      0\n      0\n      3\n      male\n      22.0\n      1\n      0\n      7.2500\n      S\n    \n    \n      1\n      1\n      1\n      female\n      38.0\n      1\n      0\n      71.2833\n      C\n    \n    \n      2\n      1\n      3\n      female\n      26.0\n      0\n      0\n      7.9250\n      S\n    \n    \n      3\n      1\n      1\n      female\n      35.0\n      1\n      0\n      53.1000\n      S\n    \n    \n      4\n      0\n      3\n      male\n      35.0\n      0\n      0\n      8.0500\n      S\n    \n  \n\n\n\n\n\n\n\nChecking all na values in the existing dataset\n\n\nCode\ntrain_eda.isna().sum().sort_values()\n\n\nSurvived      0\nPclass        0\nSex           0\nSibSp         0\nParch         0\nFare          0\nEmbarked      2\nAge         177\ndtype: int64\n\n\nFinding the mean\n\n\nCode\ntrain_eda[\"Age\"].median()\n\n\n28.0\n\n\nReplacing na cells with the mean\n\n\nCode\ntrain_eda[\"Age\"].fillna(value = train_eda[\"Age\"].median(), inplace = True)\ntrain_eda.isna().sum().sort_values()\n\n\nC:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_9908\\1076914416.py:1: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\nSurvived    0\nPclass      0\nSex         0\nAge         0\nSibSp       0\nParch       0\nFare        0\nEmbarked    2\ndtype: int64\n\n\nSidenote: Was getting a wierd warning (SettingWithCopyWarning) while using .fillna() to replace na with the median values. Turns out there‚Äôs a between calling a view or a copy. One way of avoiding this error is to use train_eda.loc[:,\"Age\"] instead of train_eda[\"Age\"]. This is because .loc returns the view (original) while using subsets. Elegant explanation here. Below code will not throw up a warning.\n\n\nCode\nxx = train_eda.copy()\nxx.loc[:,\"Age\"].fillna(value = xx.Age.median(), inplace = True)\nxx.isna().sum()\n\n\nSurvived    0\nPclass      0\nSex         0\nAge         0\nSibSp       0\nParch       0\nFare        0\nEmbarked    2\ndtype: int64\n\n\n\n\n\nSeperating X & y. Here‚Äôs the first 5 rows of X\n\n\nCode\ntrain_eda = train_eda.dropna(axis = 0) #removing all rows with NA\n\nX = train_eda[[\"Age\", \"SibSp\", \"Parch\", \"Fare\"]]\nX = pd.concat([X,pd.get_dummies(data = train_eda[[\"Sex\", \"Embarked\", \"Pclass\"]], columns = [\"Sex\", \"Embarked\", \"Pclass\"])], axis = 1)\n\nX.head()\n\n\n\n\n\n\n  \n    \n      \n      Age\n      SibSp\n      Parch\n      Fare\n      Sex_female\n      Sex_male\n      Embarked_C\n      Embarked_Q\n      Embarked_S\n      Pclass_1\n      Pclass_2\n      Pclass_3\n    \n  \n  \n    \n      0\n      22.0\n      1\n      0\n      7.2500\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      1\n    \n    \n      1\n      38.0\n      1\n      0\n      71.2833\n      1\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n    \n    \n      2\n      26.0\n      0\n      0\n      7.9250\n      1\n      0\n      0\n      0\n      1\n      0\n      0\n      1\n    \n    \n      3\n      35.0\n      1\n      0\n      53.1000\n      1\n      0\n      0\n      0\n      1\n      1\n      0\n      0\n    \n    \n      4\n      35.0\n      0\n      0\n      8.0500\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      1\n    \n  \n\n\n\n\nHere‚Äôs the first 5 rows of y\n\n\nCode\ny = train_eda[\"Survived\"].values\ny[0:5]\n\n\narray([0, 1, 1, 1, 0], dtype=int64)\n\n\ncomparing the shapes of X and y\n\n\nCode\nlen(y) #889 after filling up the NA. previously 712\nX.shape #(889, 12)\n\n\n(889, 12)\n\n\n\n\nStandardising and printing the first 5 datapoints.\n\n\nCode\nfrom sklearn import preprocessing\n\nX= preprocessing.StandardScaler().fit(X).transform(X)\nX[0:5]\n\n\narray([[-0.56367407,  0.43135024, -0.47432585, -0.50023975, -0.73534203,\n         0.73534203, -0.48271079, -0.30794088,  0.61679395, -0.56306042,\n        -0.51087465,  0.90032807],\n       [ 0.66921696,  0.43135024, -0.47432585,  0.78894661,  1.35991138,\n        -1.35991138,  2.07163382, -0.30794088, -1.62128697,  1.77600834,\n        -0.51087465, -1.11070624],\n       [-0.25545131, -0.47519908, -0.47432585, -0.48664993,  1.35991138,\n        -1.35991138, -0.48271079, -0.30794088,  0.61679395, -0.56306042,\n        -0.51087465,  0.90032807],\n       [ 0.43804989,  0.43135024, -0.47432585,  0.42286111,  1.35991138,\n        -1.35991138, -0.48271079, -0.30794088,  0.61679395,  1.77600834,\n        -0.51087465, -1.11070624],\n       [ 0.43804989, -0.47519908, -0.47432585, -0.4841333 , -0.73534203,\n         0.73534203, -0.48271079, -0.30794088,  0.61679395, -0.56306042,\n        -0.51087465,  0.90032807]])\n\n\n\n\n\nSplitting into test & train data and comparing the dimensions.\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\nprint ('Train set\\t :', X_train.shape,  y_train.shape,\n'\\nTest set\\t :', X_test.shape,  y_test.shape)\n\n\nTrain set    : (711, 12) (711,) \nTest set     : (178, 12) (178,)\n\n\n\n\n\nLets check the classification results using Decision trees. First 10 are as follows:\n\n\nCode\nfrom sklearn.tree import DecisionTreeClassifier\n\nDtree = DecisionTreeClassifier(criterion = \"entropy\", max_depth = 3)\nDtree.fit(X_train,y_train)\ny_test_hat = Dtree.predict(X_test)\nprint(\"First 10 actual\\t\\t:\", y_test[0:10],\"\\nFirst 10 predicted\\t:\", y_test_hat[0:10])\n\n\nFirst 10 actual     : [1 1 0 1 1 1 0 0 0 0] \nFirst 10 predicted  : [1 1 0 1 1 0 0 0 0 0]\n\n\n\n\n\nCalculating accuracy using Decision Tree classification for y_test\n\n\nCode\nfrom sklearn import metrics\n\nprint(\"Decision Tree Accuracy\\t:\", metrics.accuracy_score(y_test, y_test_hat),\"\\nRMSE\\t\\t\\t:\", metrics.mean_squared_error(y_test,y_test_hat),\"\\nNormalised RMSE\\t\\t:\", metrics.mean_squared_error(y_test,y_test_hat)/np.std(y_test))\n\n\nDecision Tree Accuracy  : 0.8314606741573034 \nRMSE            : 0.16853932584269662 \nNormalised RMSE     : 0.34266139226005143\n\n\nNot bad. We find that Test accuracy is around 83% for Decision Trees and RMSE of 0.168\n\n\n\nHere‚Äôs a neat little trick to see how the DT actually thinks.\n\n\nCode\nfrom sklearn import tree\nimport matplotlib.pyplot as plt\n\nplt.clf()\ntree.plot_tree(Dtree)\nplt.show()"
  },
  {
    "objectID": "posts/2022-10-16-day-10-of-50daysofkaggle/index.en.html",
    "href": "posts/2022-10-16-day-10-of-50daysofkaggle/index.en.html",
    "title": "Day 10 of #50daysofkaggle",
    "section": "",
    "text": "Part of an ongoing series to familiarise working on kaggle\nProgress till date:\n\nDownload titanic dataset and assign to train & test\nRearranging the data\nEDA (including plots and finding survival rate using .groupby())\nModelling\nData preparation - one-hot encoding the Sex, Pclass & Embarked columns - appending these to the numerical columns - normalising the data - splitting between train into X_train, y_train, X_test, y_test\nApplying KNN algo\n\nfinding the right K based on accuracy. (best at K = 7)\nCalculating the accuracy based on test\n\nApplying Decision Trees algo\n\nwith criterion = entropy and max_depth = 3\nsligthly better accuracy in prediction than KNN\n\n\nTo do today: - classification using Support Vector Machines algo\n\n\nReading and printing the top 5 rows\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport zipfile\n\n#importing the zipfile already saved in the other folder. \nzf = zipfile.ZipFile(\"../2022-10-12-day-6-of-50daysofkaggle/titanic.zip\")\ntrain = pd.read_csv(zf.open(\"train.csv\"))\ntest = pd.read_csv(zf.open(\"test.csv\"))\n\n#Selecting only the numerical columns\nnum_col = train.select_dtypes(include=np.number).columns.tolist()\n\n#deslecting passenger ID and 'Survived' \ndel num_col[0:2] #.remove() can remove only 1 item. so for more than 1, use for loop \nselect_col = num_col\n\n#remaining columns\nstr_col= [\"Sex\", \"Embarked\", \"Survived\"]\n#Adding more elements into a list using `extend` and not `append`\nselect_col.extend(str_col)\n\ntrain_eda= train[train.columns.intersection(select_col)]\ntrain_eda\n\n\n\n\n\n\n  \n    \n      \n      Survived\n      Pclass\n      Sex\n      Age\n      SibSp\n      Parch\n      Fare\n      Embarked\n    \n  \n  \n    \n      0\n      0\n      3\n      male\n      22.0\n      1\n      0\n      7.2500\n      S\n    \n    \n      1\n      1\n      1\n      female\n      38.0\n      1\n      0\n      71.2833\n      C\n    \n    \n      2\n      1\n      3\n      female\n      26.0\n      0\n      0\n      7.9250\n      S\n    \n    \n      3\n      1\n      1\n      female\n      35.0\n      1\n      0\n      53.1000\n      S\n    \n    \n      4\n      0\n      3\n      male\n      35.0\n      0\n      0\n      8.0500\n      S\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      886\n      0\n      2\n      male\n      27.0\n      0\n      0\n      13.0000\n      S\n    \n    \n      887\n      1\n      1\n      female\n      19.0\n      0\n      0\n      30.0000\n      S\n    \n    \n      888\n      0\n      3\n      female\n      NaN\n      1\n      2\n      23.4500\n      S\n    \n    \n      889\n      1\n      1\n      male\n      26.0\n      0\n      0\n      30.0000\n      C\n    \n    \n      890\n      0\n      3\n      male\n      32.0\n      0\n      0\n      7.7500\n      Q\n    \n  \n\n891 rows √ó 8 columns\n\n\n\n\n\n\nChecking all na values in the existing dataset.\n\n\nCode\ntrain_eda.isna().sum().sort_values()\n\n\nSurvived      0\nPclass        0\nSex           0\nSibSp         0\nParch         0\nFare          0\nEmbarked      2\nAge         177\ndtype: int64\n\n\nReplacing empty cells with median age (28)\n\n\nCode\nmedian_age = train_eda.Age.median() #28\ntrain_eda.loc[train_eda.Age.isna(), \"Age\"] = median_age #.loc returns the view and doesn't throw warning msg\ntrain_eda.isna().sum().sort_values()\n\n\nSurvived    0\nPclass      0\nSex         0\nAge         0\nSibSp       0\nParch       0\nFare        0\nEmbarked    2\ndtype: int64\n\n\n\n\n\nSeperating X & y\n\n\nCode\ntrain_eda = train_eda.dropna(axis = 0) #removing all rows with NA\n\nX = train_eda[[\"Age\", \"SibSp\", \"Parch\", \"Fare\"]]\nX = pd.concat([X,pd.get_dummies(data = train_eda[[\"Sex\", \"Embarked\", \"Pclass\"]], columns = [\"Sex\", \"Embarked\", \"Pclass\"])], axis = 1)\n\ny = train_eda[\"Survived\"].values\n\n\n\n\nTransform X and printing the first 5 datapoints\n\n\nCode\nfrom sklearn import preprocessing\n\nX= preprocessing.StandardScaler().fit(X).transform(X)\nX[0:5]\n\n\narray([[-0.56367407,  0.43135024, -0.47432585, -0.50023975, -0.73534203,\n         0.73534203, -0.48271079, -0.30794088,  0.61679395, -0.56306042,\n        -0.51087465,  0.90032807],\n       [ 0.66921696,  0.43135024, -0.47432585,  0.78894661,  1.35991138,\n        -1.35991138,  2.07163382, -0.30794088, -1.62128697,  1.77600834,\n        -0.51087465, -1.11070624],\n       [-0.25545131, -0.47519908, -0.47432585, -0.48664993,  1.35991138,\n        -1.35991138, -0.48271079, -0.30794088,  0.61679395, -0.56306042,\n        -0.51087465,  0.90032807],\n       [ 0.43804989,  0.43135024, -0.47432585,  0.42286111,  1.35991138,\n        -1.35991138, -0.48271079, -0.30794088,  0.61679395,  1.77600834,\n        -0.51087465, -1.11070624],\n       [ 0.43804989, -0.47519908, -0.47432585, -0.4841333 , -0.73534203,\n         0.73534203, -0.48271079, -0.30794088,  0.61679395, -0.56306042,\n        -0.51087465,  0.90032807]])\n\n\n\n\n\nSplitting into test & train data and comparing the dimensions.\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\nprint ('Train set\\t :', X_train.shape,  y_train.shape,\n'\\nTest set\\t :', X_test.shape,  y_test.shape)\n\n\nTrain set    : (711, 12) (711,) \nTest set     : (178, 12) (178,)\n\n\n\n\n\n\nLets check the classification results using SVM. First 10 are as follows:\n\n\nCode\nfrom sklearn import svm\nclf = svm.SVC(kernel='rbf')\nclf.fit(X_train, y_train) \n\nyhat_svm = clf.predict(X_test)\n\nprint(\"First 10 actual\\t\\t:\", y_test[0:10],\"\\nFirst 10 predicted\\t:\", yhat_svm[0:10])\n\n\nFirst 10 actual     : [1 1 0 1 1 1 0 0 0 0] \nFirst 10 predicted  : [0 1 0 1 1 0 0 0 0 0]\n\n\n\n\n\n\nCode\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport itertools\n\n# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_test, yhat_svm)\nnp.set_printoptions(precision=2)\n\nprint (classification_report(y_test, yhat_svm))\n\n\n              precision    recall  f1-score   support\n\n           0       0.78      0.95      0.85       105\n           1       0.90      0.60      0.72        73\n\n    accuracy                           0.81       178\n   macro avg       0.84      0.78      0.79       178\nweighted avg       0.83      0.81      0.80       178\n\n\n\n\n\n\n\n\nCode\nfrom sklearn import metrics\n\nprint(\"SVM Accuracy\\t:\", metrics.accuracy_score(y_test, yhat_svm),\"\\nRMSE\\t\\t\\t:\", metrics.mean_squared_error(y_test,yhat_svm),\"\\nNormalised RMSE\\t:\", metrics.mean_squared_error(y_test,yhat_svm)/np.std(y_test))\n\n\nSVM Accuracy    : 0.8089887640449438 \nRMSE            : 0.19101123595505617 \nNormalised RMSE : 0.38834957789472496\n\n\nAchieved 81% accuracy using SVM with RMSE of 0.1911. This is is not as good as Decision Trees which resulted in RMSE of 0.168\nTherefore after 10 days of struggle, I have come to the conclusion that Decision Trees is a good classification algorithm for the Titanic dataset."
  },
  {
    "objectID": "posts/2022-10-27-holiday-break-for-50daysofkaggle/index.en.html",
    "href": "posts/2022-10-27-holiday-break-for-50daysofkaggle/index.en.html",
    "title": "Holiday break for #50daysofkaggle",
    "section": "",
    "text": "Diwali is here\nIts festive season and I‚Äôve been really tied up with a bunch of things\n\n4 day long weekend where it was just about family and festivities. frankly one of the most rejuvenating and happy experiences to spend time with my daughter. She‚Äôs at the stage where she‚Äôs old enough to keep herself occupied alone. Its a pleasure to just watch her paint, draw, sing, role-play, read and whatever it is she loves doing.\nJob hunting has been taken up by a notch. Four major initiatives:\n\napplying more aggresively on applying to more companies in a day (spread across sites such as linkedin, iimjobs and naukri)\nOpening up the longlist to include SEA markets (Phillipines, Vietnam, Singapore, Indonesia)\nvolunteered to take up a course at FLAME starting in December. Its a 30 hour course that‚Äôs going to be quite gruelling to deliver. So will need to really start preparing as early as possible\nbuilding longlist of contacts for outreach. Downloaded LinkedIn contacts lists and sorted in order of priority\n\n\nCurrently pausing blog related updates to focus on picking up the job hunt search. Hopefully, post-Diwali there will be some action once people are back in offices!\nedit: enabled comments using hypothes.is as enabling Disqus is going to take some more effort"
  },
  {
    "objectID": "about.html#why-the-null-hypothesis",
    "href": "about.html#why-the-null-hypothesis",
    "title": "About Me",
    "section": "Why the Null Hypothesis?",
    "text": "Why the Null Hypothesis?\nBritish Nobel-laureate Ronald Coase famously said ‚ÄúIf you torture the data long enough, it will confess to anything.‚Äù True words that ring loudly in any digital-first company. As someone who‚Äôs cut their teeth in quantitative research, I firmly it is consumer behaviour & insight that is sacred and we must seek it out with the help of data (big or small).\nIn statistics, the null hypothesis conjecture is the first step to begin any inferential analysis. Similarly, I begin every growth project with a null hypothesis to help me understand consumer behaviour better. In some ways this blog is an attempt at trying a growth hack on myself. I‚Äôve been passionately teaching myself new tools to make more meaningful impact towards business. I hope to document and share my learnings through this journal.\nFeel free to leave a message using the tab on the rightüëâüèº"
  },
  {
    "objectID": "about.html#skills",
    "href": "about.html#skills",
    "title": "About Me",
    "section": "Skills",
    "text": "Skills\n\n\n\n\n\n\n\n\n\n\nR Programming\n\nPython Programming\n\nStatistics & Machine Learning\n\n\n\nConsumer Research\n\nGrowth Marketing\n\nBusiness Management"
  },
  {
    "objectID": "about.html#qualifications",
    "href": "about.html#qualifications",
    "title": "About Me",
    "section": "Qualifications",
    "text": "Qualifications\n\nWorkEducationCertificationsTeaching\n\n\nMX Player (Feb ‚Äô20 - Oct ‚Äô22)\nDirector, Consumer Insights & Strategy\n\nHeading the Consumer Insights division and leading a team of data scientists/analysts to centralize all data requirements across departments (such as SVOD, Marketing, Content & Product)\nDoubled the platform share of acquired shows and drove launch strategy of 35+ acquired shows cross-functionally with Product, Programming and Marketing teams\nDevised 5x growth of International dubbed category MX VDESI; responsible for GTM strategies that led to growth in user retention & consumption\n\nViu India (Sep ‚Äô16 - Feb ‚Äô20)\nAssociate Director, Growth Marketing\n\nSpearheaded the platform growth with 6x increase in user base & 7x increase in app installs. Managing annual marketing budgets and building the OTT audience acquisition models\nDeveloped Marketing Measurement Model to optimize TV GRP-to-new installs. This modelling mix was increased marketing efficiencies by 15%\n20% reduction in marketing costs by geo-mapping user consumption density at zip-code level thereby building more focused media plan for Telugu markets\n\nSony Pictures Network India (Jun ‚Äô08 - Sep ‚Äô15)\nManagment Trainee to Sr.¬†Manager\n\nMultiple roles in the company with wide experience across Strategy, Research, Marketing, Revenue Planning and Content Acquisition\n\n\n\n Post Graduate Diploma in Communications, 2008\n\nMICA (Mudra Institute of Communications, Ahmedabad)\n\n B.E (Computer Science & Engineering), 2006\n\nVasavi College of Engineering (affiliated to Osmania University), Hyderabad\n\n\n\n\nIBM Data Science Professional Certificate, Sep‚Äô22\nMachine learning, Feb ‚Äô21 (Stanford University)\n\nExecutive Program in Strategic Digital Marketing, Dec‚Äô19 (University of Cambridge Judge Business School)\nProfessional Certificate Programme in Business Negotiation, Feb ‚Äô22 (SP Jain Institute of Management & Research)\n\n\n\nVisiting Faculty at\n\nFlame University, Pune\nSNDT Women‚Äôs University, Mumbai"
  },
  {
    "objectID": "posts/2023-02-17-migrating-to-quarto/index.html",
    "href": "posts/2023-02-17-migrating-to-quarto/index.html",
    "title": "Migrating to Quarto",
    "section": "",
    "text": "The blog now takes a new look. Finally bought myself a ticket on Quarto hype-train and must say it seems quite interesting. Hopefully there‚Äôll be a separate blog with the nuts and bolts details about how I got this far. For now let me attempt to address why I did this.\nPython code using blogdown/Hugo was not performing consistently. I‚Äôve not been able to duplicate the errors, but they would happen once in a while. Visualisations not rendering in HTML, random crashes and occasional kernel errors.\nTo be fair, the issue could not have been due to just the blogdown framework. Part of my research led me to read up about Quarto that was introduced in early 2022. At that time, I figured with my non-existent tech skills, I‚Äôd be better off focussing on what I started with.\nBy 13th Feb, I had pretty much exhausted all my interview processes. So while I was just sitting tight waiting for that one company to revert, I figured this Quarto migration might be a good project to warm-up towards blogging. That turned out smoother than I imagined.\n\n\nVery early thoughts about what I‚Äôm seeing:\n\nLooks far more neater\nno glitches as far as code is concerned\nDeploying and customisation is super smooth. Documentation has it all covered.\nLoving the new file structure which makes it so much more easier to keep track of changes\nUser comments now appear on the right hand side instead of page bottom\nstill have to get used to the inline CSS/ HTML commands. This was frustrating in the beginning.\n\n\n\n\nCouple of tracks that I‚Äôd like to double down on:\n\nSolving exercises in Introduction to Statistical Learning in R v2 (PDF)\nSolving exercises in ‚ÄúHands-on Data Science for Marketing‚Äù, Packt (Hwang) (Kindle edition)\nMarketing Analytics projects using R, Python\n\nMarket Mix Modelling using Kaggle\nbuilding plotly and shiny dashboards\n\nrevise and blog about Quant research concepts from ‚ÄúMarketing Research‚Äù, Pearson 7e (Malhotra/Dash)\nTarget for daily check-ins all thru Feb\nIf above is completed, then proceed to ‚ÄúPython ML: ML & Deep Learning with Python, sci-kit and TensorFlow 2‚Äù, Packt (Raschka/Mirjalili) (PDF)\nResearch & academic articles in the field of digital marketing/ marketing analytics??ü§î\nExploring discussions on using NLP in the field of qualitative research\n\nAs always, please share feedback and inputs using comments tab on the right üëâüèº"
  },
  {
    "objectID": "posts/2023-02-20-day-11-of-50daysofkaggle/index.html",
    "href": "posts/2023-02-20-day-11-of-50daysofkaggle/index.html",
    "title": "Day 11 of #50daysofkaggle",
    "section": "",
    "text": "Till now I practiced creating classification predictions on the Titanic dataset using KNN, DT and SVM algorithms. As per Kaggle, my submission got a score of 77%. Now I‚Äôm going to try these approaches in R.\nsteps to do : data reading > cleaning > replacing NA > splitting > model using Decision Trees> comparing results"
  },
  {
    "objectID": "posts/2023-02-20-day-11-of-50daysofkaggle/index.html#splitting-the-data",
    "href": "posts/2023-02-20-day-11-of-50daysofkaggle/index.html#splitting-the-data",
    "title": "Day 11 of #50daysofkaggle",
    "section": "Splitting the data",
    "text": "Splitting the data\nSplitting the data into train & test\n\n\nCode\ndf_split <- initial_split(df_n, prop = 0.8)\ntrain <- training(df_split)\ntest <- testing(df_split)\n\ndf_split\n\n\n<Training/Testing/Total>\n<712/179/891>\n\n\ncreating the recipe\n\n\nCode\ndt_recipe <- recipe(Survived ~ ., data = df_n) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_normalize(all_numeric_predictors()) %>% \n  #replacing NA values in Age with median Age\n  step_mutate_at(Age, fn = ~ replace_na(Age, median(Age, na.rm = T))) %>% \n  #updating the role of the PassengerId to exclude from analysis\n  update_role(PassengerId, new_role = \"id_variable\")\n\ndt_recipe\n\n\nAnother way to view the recipe using tidy() function\n\n\nCode\ntidy(dt_recipe)\n\n\n# A tibble: 3 √ó 6\n  number operation type      trained skip  id             \n   <int> <chr>     <chr>     <lgl>   <lgl> <chr>          \n1      1 step      dummy     FALSE   FALSE dummy_jD1sy    \n2      2 step      normalize FALSE   FALSE normalize_CvfCk\n3      3 step      mutate_at FALSE   FALSE mutate_at_xAmJj"
  },
  {
    "objectID": "posts/2023-02-20-day-11-of-50daysofkaggle/index.html#model-creation",
    "href": "posts/2023-02-20-day-11-of-50daysofkaggle/index.html#model-creation",
    "title": "Day 11 of #50daysofkaggle",
    "section": "Model Creation",
    "text": "Model Creation\nDeclaring a model dt_model as a Decision Tree with depth as 3 and engine rpart\n\n\nCode\ndt_model <- decision_tree(mode = \"classification\", tree_depth = 3) %>% \n  set_engine(\"rpart\")\ndt_model %>% translate()\n\n\nDecision Tree Model Specification (classification)\n\nMain Arguments:\n  tree_depth = 3\n\nComputational engine: rpart \n\nModel fit template:\nrpart::rpart(formula = missing_arg(), data = missing_arg(), weights = missing_arg(), \n    maxdepth = 3)"
  },
  {
    "objectID": "posts/2023-02-20-day-11-of-50daysofkaggle/index.html#workflow-creation",
    "href": "posts/2023-02-20-day-11-of-50daysofkaggle/index.html#workflow-creation",
    "title": "Day 11 of #50daysofkaggle",
    "section": "Workflow creation",
    "text": "Workflow creation\nWorkflow = recipe + model\n\n\nCode\ndt_wf <- workflow() %>%\n  add_model(dt_model) %>% \n  add_recipe(dt_recipe)"
  },
  {
    "objectID": "posts/2023-02-20-day-11-of-50daysofkaggle/index.html#predicting-on-test-data",
    "href": "posts/2023-02-20-day-11-of-50daysofkaggle/index.html#predicting-on-test-data",
    "title": "Day 11 of #50daysofkaggle",
    "section": "Predicting on test data",
    "text": "Predicting on test data\nFitting the dt_wf workflow with model created on train data to predict the test data\n\n\nCode\nset.seed(2023)\ndt_predict <- predict(fit(dt_wf, data = train), test)\nhead(dt_predict)\n\n\n# A tibble: 6 √ó 1\n  .pred_class\n  <fct>      \n1 0          \n2 0          \n3 0          \n4 1          \n5 0          \n6 0          \n\n\nCreating a new tibble called preidcted_table by binding the predicted values .pred_class to the test data\n\n\nCode\npredicted_table <- bind_cols(test, dt_predict) %>% \n  rename(dt_yhat = .pred_class) %>% \n  select(Survived, dt_yhat) \nhead(predicted_table)\n\n\n# A tibble: 6 √ó 2\n  Survived dt_yhat\n  <fct>    <fct>  \n1 0        0      \n2 0        0      \n3 0        0      \n4 0        1      \n5 0        0      \n6 0        0"
  },
  {
    "objectID": "posts/2023-03-02-ISLRv2-Decision-Trees/index.html",
    "href": "posts/2023-03-02-ISLRv2-Decision-Trees/index.html",
    "title": "ISLR Lab - Decision Trees",
    "section": "",
    "text": "All the code here is derived from the legendary book ISRL 2nd edition‚Äôs chapter 8 ‚ÄúDecision Trees‚Äù. Its sometimes a wonder how elegant the base R language can be. The ISRL lab rarely mentions tidyverse syntax but yet manages to make the code so easy to read. The more you learn!ü§ì"
  },
  {
    "objectID": "posts/2023-03-02-ISLRv2-Decision-Trees/index.html#splitting-and-fitting-the-model",
    "href": "posts/2023-03-02-ISLRv2-Decision-Trees/index.html#splitting-and-fitting-the-model",
    "title": "ISLR Lab - Decision Trees",
    "section": "Splitting and fitting the model",
    "text": "Splitting and fitting the model\n\n\nCode\nset.seed(2)\ntrain <- sample(1:nrow(Carseats), 200)\nCarseats.test <- Carseats[-train,]\nHigh.test <- High[-train]\ntree.carseats <- tree(High ~ .-Sales, data = Carseats, \n                      subset = train)\n\n\nchecking the top few rows of predicted columns\n\n\nCode\ntree.predict <- predict(tree.carseats, Carseats.test, \n                        type = \"class\") #type is needed to declare classification model\nhead(tree.predict)\n\n\n[1] Yes No  No  Yes No  No \nLevels: No Yes\n\n\nComparing predicted with actual values\n\n\nCode\ntable(tree.predict, High.test)\n\n\n            High.test\ntree.predict  No Yes\n         No  104  33\n         Yes  13  50\n\n\nWhat‚Äôs the accuracy?\n\n\nCode\n(104+50)/200\n\n\n[1] 0.77\n\n\n77% Accuracy"
  },
  {
    "objectID": "posts/2023-03-02-ISLRv2-Decision-Trees/index.html#pruning-the-tree-for-improved-classification",
    "href": "posts/2023-03-02-ISLRv2-Decision-Trees/index.html#pruning-the-tree-for-improved-classification",
    "title": "ISLR Lab - Decision Trees",
    "section": "Pruning the tree for improved classification",
    "text": "Pruning the tree for improved classification\nTo improve the accuracy, lets attempt to prune the tree. For this cv.tree() function is used to determine the optimal level of tree complexity. Here the FUN argument is taken as prune.misclass to indicate that the cross-validation and tree pruning should be guided by the classification error instead of the default deviance.\n\n\nCode\nset.seed(7)\ncv.carseats <- cv.tree(tree.carseats, FUN = prune.misclass)\nnames(cv.carseats)\n\n\n[1] \"size\"   \"dev\"    \"k\"      \"method\"\n\n\nNote to self:\n\nk is the regularisation parameter \\(\\alpha\\) (alpha)\nsize is # of terminal nodes for each tree\ndev is the number of cross-validation errors\n\n\n\nCode\ncv.carseats\n\n\n$size\n[1] 21 19 14  9  8  5  3  2  1\n\n$dev\n[1] 75 75 75 74 82 83 83 85 82\n\n$k\n[1] -Inf  0.0  1.0  1.4  2.0  3.0  4.0  9.0 18.0\n\n$method\n[1] \"misclass\"\n\nattr(,\"class\")\n[1] \"prune\"         \"tree.sequence\"\n\n\nVisualising the tree. The classification error is least (74) at size  = 9\n\n\nCode\npar(mfrow = c(1,2))\nplot(cv.carseats$size, cv.carseats$dev, type = \"b\")\nplot(cv.carseats$k, cv.carseats$dev, type = \"b\")\n\n\n\n\n\nRelation between Deviance with tree size & regularisation parameter\n\n\n\n\nUsing the prune.misclass() function to prune the tree to the 9-node specification.\n\n\nCode\nprune.carseats= prune.misclass(tree.carseats, best = 9)\nplot(prune.carseats)\ntext(prune.carseats, pretty = 0)\n\n\n\n\n\nChecking the accuracy in the good-old fashioned way (its really that simple!)ü§ì\n\n\nCode\nprune.tree.pred <- predict(prune.carseats, Carseats.test, type = \"class\")\ntable(prune.tree.pred, High.test)\n\n\n               High.test\nprune.tree.pred No Yes\n            No  97  25\n            Yes 20  58\n\n\nSo what‚Äôs the accuracy?\n\n\nCode\n(97+58)/200\n\n\n[1] 0.775\n\n\n77.5% which is slightly better than the non-pruned tree. Not bad."
  },
  {
    "objectID": "posts/2023-03-02-ISLRv2-Decision-Trees/index.html#key-takeaways",
    "href": "posts/2023-03-02-ISLRv2-Decision-Trees/index.html#key-takeaways",
    "title": "ISLR Lab - Decision Trees",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nWithout tuning the model, the default DT algo creates a tree with 27 nodes\ndeviance measured as a result of changing the number of nodes indicates the best DT of 9 nodes.\nThe code needed to write this is surprisingly simple. However, the tidymodels interface allows for managing the resulting output and models in a more structured way."
  },
  {
    "objectID": "posts/2023-03-02-ISLRv2-Decision-Trees/index.html#making-the-predictions",
    "href": "posts/2023-03-02-ISLRv2-Decision-Trees/index.html#making-the-predictions",
    "title": "ISLR Lab - Decision Trees",
    "section": "Making the predictions",
    "text": "Making the predictions\n\n\nCode\nyhat <- predict(tree.boston, newdata = Boston[-train.boston,])\ntest.boston <- Boston[-train.boston,\"medv\"]\n\nplot(yhat, test.boston)\nabline(0,1, col = \"red\")\n\n\n\n\n\nThis plot visualises the Predicted v/s Actuals for Boston test data\n\n\n\n\nMean Square Error is defined as \\[MSE = \\frac{1}{n} \\sum_{i=1}^{n}(y_i - \\hat{y_i})^2\\]\n\n\nCode\nmean((yhat - test.boston)^2)\n\n\n[1] 35.28688\n\n\nRMSE which uses the same units as the output variable is:\n\n\nCode\n(mean((yhat - test.boston)^2))^0.5\n\n\n[1] 5.940276"
  },
  {
    "objectID": "posts/2023-03-02-ISLRv2-Decision-Trees/index.html#key-takeaways-1",
    "href": "posts/2023-03-02-ISLRv2-Decision-Trees/index.html#key-takeaways-1",
    "title": "ISLR Lab - Decision Trees",
    "section": "Key Takeaways:",
    "text": "Key Takeaways:\nAs the SD is the same units as the outcome variable, we can say that this model leads to predictions which on an average are within ¬±$5940 of the true median home value. Can we do better? Let‚Äôs keep digging"
  },
  {
    "objectID": "posts/2023-03-02-ISLRv2-Decision-Trees/index.html#bagging",
    "href": "posts/2023-03-02-ISLRv2-Decision-Trees/index.html#bagging",
    "title": "ISLR Lab - Decision Trees",
    "section": "Bagging",
    "text": "Bagging\n\n\nimportance parameter here will compute and return the importance measures of each predictor variable. Importance measures provide a way to assess the relative importance of each predictor variable in the random forest model, based on the decrease in accuracy that occurs when that variable is excluded from the model. This increases the runtime significantly on large datasets\n\n\nCode\nlibrary(randomForest)\nset.seed(1)\nbag.boston <- randomForest(medv ~ . , data = Boston, \n                           subset = train.boston, \n                           mtry = 12, # m = p\n                           importance = T)\nbag.boston\n\n\n\nCall:\n randomForest(formula = medv ~ ., data = Boston, mtry = 12, importance = T,      subset = train.boston) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 12\n\n          Mean of squared residuals: 11.40162\n                    % Var explained: 85.17\n\n\n\n\nCode\nyhat.bag <- predict(bag.boston, newdata = Boston[-train.boston, ])\nplot(yhat.bag, test.boston)\nabline(0,1,col = \"red\")\n\n\n\n\n\nPredicted v/s Actuals for Boston test data using Bagging\n\n\n\n\nWhat‚Äôs the accuracy here? Checking the MSE\n\n\nCode\nmean((yhat.bag - test.boston)^2)\n\n\n[1] 23.41916\n\n\nAnd square root of MSE or RMSE is:\n\n\nCode\n(mean((yhat.bag - test.boston)^2))^0.5\n\n\n[1] 4.839335\n\n\nThat‚Äôs $4839 which is better than $ 5940 derived from the 7-node decision tree discussed in Key Takeaways. Moving to Random Forest now."
  },
  {
    "objectID": "posts/2023-03-02-ISLRv2-Decision-Trees/index.html#random-forest",
    "href": "posts/2023-03-02-ISLRv2-Decision-Trees/index.html#random-forest",
    "title": "ISLR Lab - Decision Trees",
    "section": "Random Forest",
    "text": "Random Forest\nIts the same code, but we alter the number of predicted variables to \\(m= 6\\) which is the mtry parameter\n\n\nDefault settings for randomForest()\nfor regression analysis, \\(m = p/3\\)\nfor classification analysis, \\(m = \\sqrt p\\)\n\n\nCode\nset.seed(1)\nrf.boston <- randomForest(medv ~ . , data = Boston, \n                           subset = train.boston, \n                           mtry = 6, # m = p/2\n                           importance = T)\nrf.boston\n\n\n\nCall:\n randomForest(formula = medv ~ ., data = Boston, mtry = 6, importance = T,      subset = train.boston) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 6\n\n          Mean of squared residuals: 10.09466\n                    % Var explained: 86.87\n\n\n\n\nCode\nyhat.rf <- predict(rf.boston, newdata = Boston[-train.boston, ])\nplot(yhat.rf, test.boston)\nabline(0,1,col = \"red\")\n\n\n\n\n\nPredicted v/s Actuals for Boston test data using RandomForest\n\n\n\n\nWhat‚Äôs the MSE here?\n\n\nCode\nmean((yhat.rf - test.boston)^2)\n\n\n[1] 20.06644\n\n\n.. and therefore RMSE is:\n\n\nCode\nmean((yhat.rf - test.boston)^2)^0.5\n\n\n[1] 4.479558\n\n\nThat‚Äôs ¬±$4480 from the mean predicted values - which is better than $4839 by using the Bagging method.\nBefore moving ahead, we can also check the importance() function to determine key predictors\n\n\nCode\nimportance(rf.boston)\n\n\n          %IncMSE IncNodePurity\ncrim    19.435587    1070.42307\nzn       3.091630      82.19257\nindus    6.140529     590.09536\nchas     1.370310      36.70356\nnox     13.263466     859.97091\nrm      35.094741    8270.33906\nage     15.144821     634.31220\ndis      9.163776     684.87953\nrad      4.793720      83.18719\ntax      4.410714     292.20949\nptratio  8.612780     902.20190\nlstat   28.725343    5813.04833\n\n\nWhat are these columns?\n\n%IncMSE: Avg decrease in accuracy of predictions on out-of-bag samples when given variable is calculated\nIncNodePurity:Total decrease in node purity that results from split on that variable averaged over all trees.\n\nin regression trees, the node impurity measured by the training Residual Sum of Squares(RSS)\nin classification trees, it is the deviance\n\n\n\n\nCode\nvarImpPlot(rf.boston)\n\n\n\n\n\nPredicted v/s Actuals for Boston test data using Bagging\n\n\n\n\nThis shows that the two most important variables are rm (average number of rooms per dwelling) and lstat (lower status of the population in %)"
  },
  {
    "objectID": "posts/2023-03-02-ISLRv2-Decision-Trees/index.html#partial-dependence-plots-interaction-plot",
    "href": "posts/2023-03-02-ISLRv2-Decision-Trees/index.html#partial-dependence-plots-interaction-plot",
    "title": "ISLR Lab - Decision Trees",
    "section": "partial dependence plots & Interaction plot",
    "text": "partial dependence plots & Interaction plot\nThese plots illustrate the asflkjdsa;lfdsj marginal effect\n\n\nCode\npar(mfrow = c(1,2))\nplot(boost.boston, i = \"rm\")\n\n\n\n\n\nRelation between predictors rm and lstat to outcome medv\n\n\n\n\nCode\nplot(boost.boston, i = \"lstat\")\n\n\n\n\n\nRelation between predictors rm and lstat to outcome medv\n\n\n\n\n\n\nCode\n.caption_text{\n  margin: auto;\n  text-align: center;\n}"
  },
  {
    "objectID": "posts/2023-03-02-ISLRv2-Decision-Trees/index.html#partial-dependence-plots",
    "href": "posts/2023-03-02-ISLRv2-Decision-Trees/index.html#partial-dependence-plots",
    "title": "ISLR Lab - Decision Trees",
    "section": "partial dependence plots",
    "text": "partial dependence plots\nBy plotting the partial dependence of rm and lstat on outcome variable, we see that\n\nrm has a direct relation viz.¬†more the number of rooms, higher the price increases\nlstat has an inverse relation viz.¬†higher the lower stata in the neighbourhood, lower the price\n\n\n\nCode\npar(mfrow = c(1,2))\nplot(boost.boston, i = \"rm\")\n\n\n\n\n\nCode\nplot(boost.boston, i = \"lstat\")"
  },
  {
    "objectID": "posts/2023-03-02-ISLRv2-Decision-Trees/index.html#predictions",
    "href": "posts/2023-03-02-ISLRv2-Decision-Trees/index.html#predictions",
    "title": "ISLR Lab - Decision Trees",
    "section": "Predictions",
    "text": "Predictions\n\n\nCode\nyhat.boost <- predict(boost.boston, newdata = Boston[-train.boston,], \n                      n.trees = 5000)\n\n#| fig-cap: \"Predicted v/s Actuals for Boston test data using Boosted RF\"\n#| fig-cap-location: top\nplot(yhat.boost, test.boston)\nabline(0,1,col = \"red\")\n\n\n\n\n\nFigure looks so much better. testing the accuracy now. starting with the MSE\n\n\nCode\nmean((yhat.boost - test.boston)^2)\n\n\n[1] 12.98195\n\n\nWow.. that‚Äôs significantly lower. How about the RMSE?\n\n\nCode\nmean((yhat.boost - test.boston)^2)^0.5\n\n\n[1] 3.603047\n\n\nAmazing. This means our predicted value on an average is ¬±$3603 from the actual which is a signifcant improvement from the RMSE calculated by Random Forest ¬±$4480"
  },
  {
    "objectID": "posts/2023-03-02-ISLRv2-Decision-Trees/index.html#adding-regularisation-parameter-ùõå",
    "href": "posts/2023-03-02-ISLRv2-Decision-Trees/index.html#adding-regularisation-parameter-ùõå",
    "title": "ISLR Lab - Decision Trees",
    "section": "Adding regularisation parameter ùõå",
    "text": "Adding regularisation parameter ùõå\nAlso referred as the shrinkage parameter, the default value is 0.001 but we will change this to 0.01\n\n\nCode\nset.seed(1)\nboost.boston2 <- gbm(medv ~ ., data = Boston[train,], \n                    distribution = \"gaussian\",\n                    n.trees = 5000,\n                    interaction.depth = 4, \n                    shrinkage = 0.01)\nyhat.boost2 <- predict(boost.boston2, newdata = Boston[-train.boston,], \n                      n.trees = 5000)\n\n\nThe resulting MSE therefore is calculated as:\n\n\nCode\nmean((yhat.boost2 - test.boston)^2)^0.5\n\n\n[1] 3.593472\n\n\nNow we‚Äôve got it even lower at ¬±$3593"
  },
  {
    "objectID": "posts/2023-03-04-day-14-of-50daysofkaggle/index.html",
    "href": "posts/2023-03-04-day-14-of-50daysofkaggle/index.html",
    "title": "Day 14 of #50daysofKaggle",
    "section": "",
    "text": "Today‚Äôs post is an attempt to use the tidymodels framework to screen multiple models. Inspiration for this post comes from"
  },
  {
    "objectID": "posts/2023-03-04-day-13-of-50daysofkaggle/index.html",
    "href": "posts/2023-03-04-day-13-of-50daysofkaggle/index.html",
    "title": "Day 13 of #50daysofKaggle",
    "section": "",
    "text": "Checking in at the checkpoint\nAs none of you are wondering, I‚Äôm taking this Saturday evening to update myself on what I‚Äôve been trying to do with all the work so far. And since none of you have commented, I‚Äôm going to pretend I‚Äôm talking to my invisible reader and assuage none of their concerns. üôà\n\n\nWhere am I right now?\nSo ‚Ä¶ what is #50daysofKaggle?\nAttempting a kaggle competition has been one of the longest running aspiration for me. Exactly 6 years ago in March ‚Äô17, I created my profile. But I knew I had to start learning to code. So I just kept learning‚Ä¶ and learning‚Ä¶ and learning‚Ä¶ without any real application. As with any new project, I think I may have spent way too much time in planning and worrying about stuff instead of actually ‚Ä¶ doing stuff!\nMany false starts later, it was in the winter of 2022 I had to take that hard decision.\n#50daysofkaggle was my own personal challenge to check-in daily here to ensure that I‚Äôm giving at least 50 consecutive days to my worst fears. My first kaggle post was on 7th Oct.¬†Its roughly 150 days since then. Here are my thoughts so far:\n\nThis blogging is super helpful in not only reviewing code but also for revising theory thru ISLR labs. Offline I‚Äôm maintaining my own notes but this blog is all about application and it has given me more perspective than I intended at the start. Totally the right thing I should be investing my time into.\nCode-switching between R & Python is super helpful to learning from best of both worlds. I may have effectively eliminated the ‚Äúfear of coding‚Äù that held me back forever now.\nthese posts are taking longer to write than I thought. Seriously‚Ä¶ its been around 13 posts in the last 26 weeks. So that‚Äôs 1 post every two weeks. gulp! Reasons are obvious\n\nlearning to code is one of the biggest things slowing me down\nbusy with job-hunt\nholiday season in Dec‚Äô22 (was away from keyboard for almost an entire month)\n\nTill now the numbering of the posts & the days are largely immaterial. It should broadly be read as ‚Äúpost #13 out of 50 that I committed myself‚Äù. I keep skipping the count of days because each post actually takes me 2-3 days to finish. Going forward, I‚Äôm going change it to imply ‚Äúblog post number‚Äù instead of ‚Äúday number‚Äù. And this one definitely counts because its really important to what I want to keep doing\n\n\n\nWhere do I want to go?\nNow that I‚Äôve got that wee bit experiential learning, 7 posts have been made about my kaggle journey, so now I‚Äôm aiming for 43 more. I‚Äôm listing down a bunch of things that I want to achieve through #50daysofkaggle. topics that I definitely want to cover in order of priority:\n\nkeep the focus on Marketing Analytics & Marketing datasets (Hands-on DS for Marketing, Hwang).\nShiny App in R\nPlotly/ Dash App in Python\nNeed to highlight usecases to share with professional network\nsci-kit learn in python\nDeep learning algorithms\nusing coding for applying quant research (Marketing Research 7e, Malhotra/Dash)\nTidyTuesday participation?\ntime-series approach for content consumption data\n\n\n\nHow long will it take?\nNot anytime soon. Even if I make a post every 2 days (without holidays) starting today I‚Äôll be done by 31st May. At 3 days for each post it will 13th July. Incidentally, by August my financial runway is going to end. I‚Äôll at least have a blog. hurrah!üòÖ\n\n\nHow do I get there?\nThe only real answer is to keep practicing. Persevereance is key so the deadline makes it more important.\n\nspeeden up the EDA, feature engineering by picking datasets that comes from my domain experience. in other words, stick to strengths and don‚Äôt jump into areas I‚Äôve got no idea about.\ngetting to XGBoost was one of my goals with revising DT & Resampling methods\nEfficiency in model building should be over-ridden with explaining the business impact. Need to pull myself away to the 20,000 feet view for each dataset.\nUse communities like LetsCodeTogether on WA, KaggleNoobs+R4DS on Slack and Rstats on Twitter\nTarget 1 or 2 important real-world applications that will be worth showcasing at a job interview.\n\nOther issues I‚Äôll have to address:\n\nConsulting assignments that are coming up\nJob hunts (as of today, there‚Äôs only one process in hand)\n\nAnyways, that‚Äôs enough for today.\nNow my goal is in sight. I must proceed! üèÉ‚Äç‚ôÇÔ∏è"
  },
  {
    "objectID": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html",
    "href": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html",
    "title": "Day 14 of #50daysofKaggle",
    "section": "",
    "text": "How hard is it to evaluate multiple models when working on a given data problem?\nIf you‚Äôre using the tidymodels package, the answer is surprisingly simple.\nToday‚Äôs post is an attempt to use the tidymodels framework to screen multiple models. Inspiration for this post comes from the Ch 15 of the Tidymodels with R textbook along with two more noteworthy blogs\nI‚Äôm skipping the EDA component as I‚Äôve covered it in the previous posts. Moving on to some boring (but very necessary) sections.\nAs always, please feel free to leave a comment using the side-barüëâüèº"
  },
  {
    "objectID": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html#cleaning-data",
    "href": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html#cleaning-data",
    "title": "Day 14 of #50daysofKaggle",
    "section": "Cleaning data",
    "text": "Cleaning data\nChecking for NA values in the full df tells us that Age column has 86 missing in test & 177 missing values in train while there‚Äôs 1 missing NA value in Fare. The 418 missing values in Survived in test are the ones we need to predict.\n\n\nCode\ntitanic_data %>% \n  group_by(source) %>% \n  summarise_all(~ sum(is.na(.)))\n\n\n# A tibble: 2 √ó 13\n  source Passe‚Ä¶¬π Survi‚Ä¶¬≤ Pclass  Name   Sex   Age SibSp Parch Ticket  Fare Cabin\n  <chr>    <int>   <int>  <int> <int> <int> <int> <int> <int>  <int> <int> <int>\n1 test         0     418      0     0     0    86     0     0      0     1     0\n2 train        0       0      0     0     0   177     0     0      0     0     0\n# ‚Ä¶ with 1 more variable: Embarked <int>, and abbreviated variable names\n#   ¬π‚ÄãPassengerId, ¬≤‚ÄãSurvived\n\n\n\nJanitor::clean_names()\nNow i‚Äôve got a thing about keeping column names clean so summoning janitor with a magic wand:\n\n\nCode\ntitanic_data <- titanic_data %>% \n  mutate(family_count = SibSp+Parch+1) %>% \n  janitor::clean_names()\nnames(titanic_data)\n\n\n [1] \"passenger_id\" \"survived\"     \"pclass\"       \"name\"         \"sex\"         \n [6] \"age\"          \"sib_sp\"       \"parch\"        \"ticket\"       \"fare\"        \n[11] \"cabin\"        \"embarked\"     \"source\"       \"family_count\"\n\n\nVoila. Everything now in lower case and snake case!\n\n\nImputing NA values in embarked\nNow under embarked there are two rows that don‚Äôt have NA but are blank. That‚Äôs a bit oddü§®\n\n\nCode\ntitanic_data %>% \n  count(embarked, sort = T)\n\n\n  embarked   n\n1        S 914\n2        C 270\n3        Q 123\n4            2\n\n\nSince it is only 2 such rows, I‚Äôll be replacing the NA values with the most repeated embarked value. So now zero empty values in embarked\n\n\nCode\nmode_embarked <- titanic_data %>% \n  count(embarked, sort = T) %>% \n  select(embarked) %>% \n  head(1) %>% \n#this beautiful func comes from the purrr package. equivalent of .[[1]]\n  pluck(1)\n\ntitanic_data <- titanic_data %>% \n  mutate(embarked = if_else(embarked == \"\", mode_embarked, embarked))\n\ntitanic_data %>% \n  count(embarked, sort = T)\n\n\n  embarked   n\n1        S 916\n2        C 270\n3        Q 123\n\n\n\n\nImputing NA values in age\nThe age column has a bunch of missing values. My approach today is to substitute them with the median values when grouped by sex and class. Here‚Äôs a function written to impute within the test and train data accordingly.\nnote: Feature engineering functions can be addressed in the tidymodels framework at recipe stage.\n\n\nCode\nmedian_age_calc <- function(df){\n  median_ages <- df %>% \n    group_by(sex, pclass) %>% \n    summarise(age = median(age,na.rm = T))\n  df %>%\n    mutate(age = case_when((sex ==\"male\" & pclass ==1 & is.na(age)) ~ median_ages$age[1],\n                           (sex ==\"male\" & pclass ==2 & is.na(age)) ~ median_ages$age[2],\n                           (sex ==\"male\" & pclass ==3 & is.na(age)) ~ median_ages$age[3],\n                           (sex ==\"female\" & pclass ==1 & is.na(age)) ~ median_ages$age[4],\n                           (sex ==\"female\" & pclass ==2 & is.na(age)) ~ median_ages$age[5],\n                           (sex ==\"female\" & pclass ==3 & is.na(age)) ~ median_ages$age[6],\n                           .default = age)\n    )\n}\n\ntitanic_data <- titanic_data %>% \n  median_age_calc() %>% ungroup()\n\n\nAre there any na values in the titanic data now?\n\n\nCode\ntitanic_data %>% \n  select(source, survived, sex, pclass, fare, age) %>% \n  group_by(source) %>% \n  summarise_all(~ sum(is.na(.)))\n\n\n# A tibble: 2 √ó 6\n  source survived   sex pclass  fare   age\n  <chr>     <int> <int>  <int> <int> <int>\n1 test        418     0      0     1     0\n2 train         0     0      0     0     0\n\n\nPhew. So age is covered but one NA in fare. What are the median age values now?\n\n\nCode\ntitanic_data %>% \n  group_by(pclass, sex) %>%\n  summarise(median_age = median(age))\n\n\n# A tibble: 6 √ó 3\n# Groups:   pclass [3]\n  pclass sex    median_age\n   <int> <chr>       <dbl>\n1      1 female       37.5\n2      1 male         37  \n3      2 female       28  \n4      2 male         28  \n5      3 female       25  \n6      3 male         22  \n\n\n\n\nImputing NA values for fare\nThere‚Äôs this one person (passenger_id = 1044, male and pclass = 3) who has a na value in fare from test data. Replacing it with the median value.\n\n\nCode\ntitanic_data %>% \n  select(source, name, passenger_id, sex, age, pclass, fare) %>% \n  filter(is.na(fare))\n\n\n  source               name passenger_id  sex  age pclass fare\n1   test Storey, Mr. Thomas         1044 male 60.5      3   NA\n\n\n\n\nCode\ntitanic_data %>% \n  group_by(sex, pclass) %>% \n  summarise(median_fare = median(fare, na.rm = T))\n\n\n# A tibble: 6 √ó 3\n# Groups:   sex [2]\n  sex    pclass median_fare\n  <chr>   <int>       <dbl>\n1 female      1       80.9 \n2 female      2       23   \n3 female      3       10.5 \n4 male        1       49.5 \n5 male        2       13   \n6 male        3        7.90\n\n\nReplacing in the main df\n\n\nCode\ntitanic_data <- titanic_data %>% \n  mutate(fare = if_else(is.na(fare), \n#from the above table. Urgh!! hard coding for that 1 guy!! how inelegant.\n                        7.8958, age))\n\n\nchecking if has been replaced.\n\n\nCode\ntitanic_data %>% \n  select(source, name, passenger_id, sex, age, pclass, fare) %>% \n  filter(is.na(fare))\n\n\n[1] source       name         passenger_id sex          age         \n[6] pclass       fare        \n<0 rows> (or 0-length row.names)"
  },
  {
    "objectID": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html#splitting-names",
    "href": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html#splitting-names",
    "title": "Day 14 of #50daysofKaggle",
    "section": "Splitting names",
    "text": "Splitting names\nWith absolutely no guilt, I confess that this took me almost an entire day to figure out. And am I glad to have done it. The best thing about the tidyverse approach is the onus on making readable data. For that I‚Äôm grateful to discover functions like seperate_wider_regex.\nEssentially, it is a delimiter that breaks up columns based on the string patterns. So neat!\n\n\nCode\nnames_with_splchar <- regex(\"[A-Za-z]+[\\\\'\\\\-\\\\s]+[A-Za-z]+\")\nnames_with_3words <- regex(\"[A-Za-z]+\\\\s[A-Za-z]+\\\\s[A-Za-z]+\")\nnames_with_1word <- regex(\"[A-Za-z]+\") \nnames_with_2words <- regex(\"[A-Za-z]+\\\\s+[A-Za-z]+\") # for 'the countess'\n\n\ntitanic_data <- titanic_data %>% \n  separate_wider_regex(\n    name, \n    patterns = c(\n#IMP: ordering of regex patterns changes the outcome\n      surname = str_c(c(names_with_splchar, \n                        names_with_3words,\n                        names_with_1word), \n                      collapse = \"|\"),    # picks the first word before comma\n      \", \",                               # the comma  \n#IMP: ordering of regex patterns changes the outcome\n      title = str_c(c(names_with_2words , # two words with special char in between like 'the countess'\n                      names_with_1word),  # one word such as Mr Miss Mrs etc\n                    collapse = \"|\"),      \n      \". \",                               # the dot\n      given_name = \".+\"),                 # picks anything else which occurs at least once\n    cols_remove = F                       # retains the original column    \n  ) \n\ntitanic_data %>% \n  select(name, title, surname, given_name) %>% \n  head(10)\n\n\n# A tibble: 10 √ó 4\n   name                                                title  surname   given_‚Ä¶¬π\n   <chr>                                               <chr>  <chr>     <chr>   \n 1 Braund, Mr. Owen Harris                             Mr     Braund    Owen Ha‚Ä¶\n 2 Cumings, Mrs. John Bradley (Florence Briggs Thayer) Mrs    Cumings   John Br‚Ä¶\n 3 Heikkinen, Miss. Laina                              Miss   Heikkinen Laina   \n 4 Futrelle, Mrs. Jacques Heath (Lily May Peel)        Mrs    Futrelle  Jacques‚Ä¶\n 5 Allen, Mr. William Henry                            Mr     Allen     William‚Ä¶\n 6 Moran, Mr. James                                    Mr     Moran     James   \n 7 McCarthy, Mr. Timothy J                             Mr     McCarthy  Timothy‚Ä¶\n 8 Palsson, Master. Gosta Leonard                      Master Palsson   Gosta L‚Ä¶\n 9 Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)   Mrs    Johnson   Oscar W‚Ä¶\n10 Nasser, Mrs. Nicholas (Adele Achem)                 Mrs    Nasser    Nichola‚Ä¶\n# ‚Ä¶ with abbreviated variable name ¬π‚Äãgiven_name\n\n\nWhat is the break-up of titles now?\n\n\nCode\ntitanic_data %>% \n  count(title, sort= T)\n\n\n# A tibble: 18 √ó 2\n   title            n\n   <chr>        <int>\n 1 Mr             757\n 2 Miss           260\n 3 Mrs            197\n 4 Master          61\n 5 Dr               8\n 6 Rev              8\n 7 Col              4\n 8 Major            2\n 9 Mlle             2\n10 Ms               2\n11 Capt             1\n12 Don              1\n13 Dona             1\n14 Jonkheer         1\n15 Lady             1\n16 Mme              1\n17 Sir              1\n18 the Countess     1"
  },
  {
    "objectID": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html#creating-a-custom-grouping",
    "href": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html#creating-a-custom-grouping",
    "title": "Day 14 of #50daysofKaggle",
    "section": "Creating a custom grouping",
    "text": "Creating a custom grouping\nThe ticket is going to be broken up into ticket_tail (the last character) and ticket_head (all but the last character). Then we merge surname and ticket_head to create a group_id\n\n\nCode\ntitanic_data <- titanic_data %>% \n  mutate(ticket_head = substr(ticket, 1, nchar(ticket)-1),\n         ticket_tail = substr(ticket, nchar(ticket), nchar(ticket)),\n         group_id = paste0(surname, \"_\", ticket_head)) \n\n\nCreating columns that indicate the number of people and other flags\n\n\nCode\ntitanic_data <- titanic_data %>% \n  add_count(group_id) %>% rename(pax_in_group = n)\n\ntitanic_data <- titanic_data %>% \n  mutate(flag = case_when ((family_count==1 & pax_in_group==1) ~ \"1_solo\",\n                           family_count == pax_in_group ~ \"2_family_full\",\n                           !(family_count == pax_in_group) ~ \"3_clubbed\",\n                           .default = \"x\"))\n\ntitanic_data <- titanic_data %>% \n  add_count(ticket_head) %>% \n  rename(pax_in_ticket_head = n)\n\n# how many instances of the same ticket having multiple groups? \ntitanic_data <- titanic_data %>% \n  group_by(ticket) %>% \n  mutate(groups_in_ticket = n_distinct(group_id)) %>% ungroup()\n\n# which tickets that have more than 1 groups in them? \n#     these passengers will have ticket_grouping precedence as they may include \n#     nannies, relatives & friends that don't share the same surname\nticket_with_multiple_groups <- titanic_data %>% \n  filter(!groups_in_ticket==1) %>% \n  count(ticket, sort = T)\n\ntitanic_data <- titanic_data %>% \n  mutate(final_grouping = if_else(ticket %in% ticket_with_multiple_groups$ticket, \n                             ticket, group_id),\n         final_label = if_else(ticket %in% ticket_with_multiple_groups$ticket,\n                         \"4_ticket_grouping\", flag)) \n\n\nSince the code now has become a bit too long and I‚Äôm creating a checkpost here with a new df called titanic_data2\n\n\nCode\ntitanic_data2 <- titanic_data %>% \n  select(source, passenger_id, survived,  sex, age, fare, pclass, embarked, \n         family_count, pax_in_group, pax_in_ticket_head, groups_in_ticket,\n         final_grouping) %>% \n  mutate(final_grouping = as_factor(final_grouping),\n         survived = as_factor(survived),\n         pclass = as_factor(pclass),\n         sex = as_factor(sex),\n         embarked = as_factor(embarked))\nglimpse(titanic_data2)\n\n\nRows: 1,309\nColumns: 13\n$ source             <chr> \"train\", \"train\", \"train\", \"train\", \"train\", \"train‚Ä¶\n$ passenger_id       <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, ‚Ä¶\n$ survived           <fct> 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, ‚Ä¶\n$ sex                <fct> male, female, female, female, male, male, male, mal‚Ä¶\n$ age                <dbl> 22, 38, 26, 35, 35, 22, 54, 2, 27, 14, 4, 58, 20, 3‚Ä¶\n$ fare               <dbl> 22, 38, 26, 35, 35, 22, 54, 2, 27, 14, 4, 58, 20, 3‚Ä¶\n$ pclass             <fct> 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, ‚Ä¶\n$ embarked           <fct> S, C, S, S, S, Q, S, S, S, C, S, S, S, S, S, S, Q, ‚Ä¶\n$ family_count       <dbl> 2, 2, 1, 2, 1, 1, 1, 5, 3, 2, 3, 1, 1, 7, 1, 1, 6, ‚Ä¶\n$ pax_in_group       <int> 1, 2, 1, 2, 1, 1, 1, 5, 3, 2, 3, 1, 1, 7, 1, 1, 6, ‚Ä¶\n$ pax_in_ticket_head <int> 5, 14, 2, 8, 1, 1, 6, 5, 4, 4, 3, 14, 1, 22, 8, 1, ‚Ä¶\n$ groups_in_ticket   <int> 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ‚Ä¶\n$ final_grouping     <fct> Braund_A/5 2117, Cumings_PC 1759, Heikkinen_STON/O2‚Ä¶\n\n\nNow this is the part I don‚Äôt get. Why does Kaggle call it train and test when it can be easily told to be given_data and to_predict data?\n\n\nCode\nto_predict <- titanic_data2 %>% \n  filter(source == \"test\") %>% select(-survived, -source)\ngiven_data <- titanic_data2 %>% \n  filter(source == \"train\") %>% select(-source)\n\n\ngiven_data has all the necessary columns we need to train the algo on\n\n\nCode\nglimpse(given_data)\n\n\nRows: 891\nColumns: 12\n$ passenger_id       <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, ‚Ä¶\n$ survived           <fct> 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, ‚Ä¶\n$ sex                <fct> male, female, female, female, male, male, male, mal‚Ä¶\n$ age                <dbl> 22, 38, 26, 35, 35, 22, 54, 2, 27, 14, 4, 58, 20, 3‚Ä¶\n$ fare               <dbl> 22, 38, 26, 35, 35, 22, 54, 2, 27, 14, 4, 58, 20, 3‚Ä¶\n$ pclass             <fct> 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, ‚Ä¶\n$ embarked           <fct> S, C, S, S, S, Q, S, S, S, C, S, S, S, S, S, S, Q, ‚Ä¶\n$ family_count       <dbl> 2, 2, 1, 2, 1, 1, 1, 5, 3, 2, 3, 1, 1, 7, 1, 1, 6, ‚Ä¶\n$ pax_in_group       <int> 1, 2, 1, 2, 1, 1, 1, 5, 3, 2, 3, 1, 1, 7, 1, 1, 6, ‚Ä¶\n$ pax_in_ticket_head <int> 5, 14, 2, 8, 1, 1, 6, 5, 4, 4, 3, 14, 1, 22, 8, 1, ‚Ä¶\n$ groups_in_ticket   <int> 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ‚Ä¶\n$ final_grouping     <fct> Braund_A/5 2117, Cumings_PC 1759, Heikkinen_STON/O2‚Ä¶\n\n\nto_predict dataframe has everything else except survived column that needs to be predicted.\n\n\nCode\nglimpse(to_predict)\n\n\nRows: 418\nColumns: 11\n$ passenger_id       <int> 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 9‚Ä¶\n$ sex                <fct> male, female, male, male, female, male, female, mal‚Ä¶\n$ age                <dbl> 34.5, 47.0, 62.0, 27.0, 22.0, 14.0, 30.0, 26.0, 18.‚Ä¶\n$ fare               <dbl> 34.5, 47.0, 62.0, 27.0, 22.0, 14.0, 30.0, 26.0, 18.‚Ä¶\n$ pclass             <fct> 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 1, 1, 2, 1, 2, 2, ‚Ä¶\n$ embarked           <fct> Q, S, Q, S, S, S, Q, S, C, S, S, S, S, S, S, C, Q, ‚Ä¶\n$ family_count       <dbl> 1, 2, 1, 1, 3, 1, 1, 3, 1, 3, 1, 1, 2, 2, 2, 2, 1, ‚Ä¶\n$ pax_in_group       <int> 1, 1, 1, 1, 2, 1, 1, 3, 1, 3, 1, 1, 2, 2, 2, 2, 1, ‚Ä¶\n$ pax_in_ticket_head <int> 3, 1, 1, 6, 11, 3, 3, 6, 16, 4, 10, 3, 2, 2, 2, 4, ‚Ä¶\n$ groups_in_ticket   <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, ‚Ä¶\n$ final_grouping     <fct> Kelly_33091, Wilkes_36327, Myles_24027, Wirz_31515,‚Ä¶\n\n\nPhew. finally done with the pre-processing. Thanks for sticking around. Here have a gif."
  },
  {
    "objectID": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html#the-recipe",
    "href": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html#the-recipe",
    "title": "Day 14 of #50daysofKaggle",
    "section": "The Recipe",
    "text": "The Recipe"
  },
  {
    "objectID": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html#step-1-creating-resampling-folds",
    "href": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html#step-1-creating-resampling-folds",
    "title": "Day 14 of #50daysofKaggle",
    "section": "Step 1: creating resampling folds",
    "text": "Step 1: creating resampling folds\n\n\nCode\nset.seed(2023)\ntitanic_folds <- bootstraps(data = given_data, \n                            times = 15)\ntitanic_folds\n\n\n# Bootstrap sampling \n# A tibble: 15 √ó 2\n   splits            id         \n   <list>            <chr>      \n 1 <split [891/318]> Bootstrap01\n 2 <split [891/313]> Bootstrap02\n 3 <split [891/340]> Bootstrap03\n 4 <split [891/313]> Bootstrap04\n 5 <split [891/326]> Bootstrap05\n 6 <split [891/342]> Bootstrap06\n 7 <split [891/316]> Bootstrap07\n 8 <split [891/323]> Bootstrap08\n 9 <split [891/338]> Bootstrap09\n10 <split [891/331]> Bootstrap10\n11 <split [891/319]> Bootstrap11\n12 <split [891/322]> Bootstrap12\n13 <split [891/340]> Bootstrap13\n14 <split [891/321]> Bootstrap14\n15 <split [891/320]> Bootstrap15"
  },
  {
    "objectID": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html#step-2-recipe",
    "href": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html#step-2-recipe",
    "title": "Day 14 of #50daysofKaggle",
    "section": "Step 2: Recipe",
    "text": "Step 2: Recipe\nCreating the base_recipe object that has only 3 steps\n\n\nCode\nbase_recipe <- recipe(survived ~ ., data = given_data) %>% \n  update_role(passenger_id, new_role = \"id_variable\") %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_normalize(all_numeric_predictors()) \nbase_recipe\n\n\nOr if one would like to see it in a tidy format.\n\n\nCode\ntidy(base_recipe)\n\n\n# A tibble: 2 √ó 6\n  number operation type      trained skip  id             \n   <int> <chr>     <chr>     <lgl>   <lgl> <chr>          \n1      1 step      dummy     FALSE   FALSE dummy_FSxXz    \n2      2 step      normalize FALSE   FALSE normalize_bu1Ti"
  },
  {
    "objectID": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html#step-3-model-definitions",
    "href": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html#step-3-model-definitions",
    "title": "Day 14 of #50daysofKaggle",
    "section": "Step 3: Model definitions",
    "text": "Step 3: Model definitions\nAll the four models are defined as:\n\n\nCode\n#logistic regression\nglm_model <- logistic_reg() %>% \n  set_engine(\"glm\") %>% \n  set_mode(\"classification\")\n\n#random forest\nrf_model <- rand_forest(trees = 1000) %>% \n  set_engine(\"ranger\") %>% \n  set_mode(\"classification\")\n\n#support vector machines\nsvm_model <- svm_rbf() %>% # rbf - radial based\n  set_engine(\"kernlab\") %>% \n  set_mode(\"classification\")\n\n#decision tree\ndt_model <- decision_tree(mode = \"classification\", \n                          tree_depth = 3) %>% \n  set_engine(\"rpart\")"
  },
  {
    "objectID": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html#step-4-workflow-set",
    "href": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html#step-4-workflow-set",
    "title": "Day 14 of #50daysofKaggle",
    "section": "Step 4: Workflow set",
    "text": "Step 4: Workflow set\nJust as you would with a single model, the workflow combines the base recipe with the multiple models by using lists when declaring the object.\n\n\nCode\ntitanic_wf_set <- workflow_set(\n  list(base_recipe),\n  list(glm_model, rf_model, svm_model, dt_model),\n  cross = T\n)\ntitanic_wf_set\n\n\n# A workflow set/tibble: 4 √ó 4\n  wflow_id             info             option    result    \n  <chr>                <list>           <list>    <list>    \n1 recipe_logistic_reg  <tibble [1 √ó 4]> <opts[0]> <list [0]>\n2 recipe_rand_forest   <tibble [1 √ó 4]> <opts[0]> <list [0]>\n3 recipe_svm_rbf       <tibble [1 √ó 4]> <opts[0]> <list [0]>\n4 recipe_decision_tree <tibble [1 √ó 4]> <opts[0]> <list [0]>\n\n\nIn the table above, the result column shows a list[0] implying that it is currently empty. This is because till now, we‚Äôve only defined the workflows and models but are yet to pass the data though it.\nObligatory shout out to the phenom Julia Silge whose YT tutorials and blogs have been my learning books at each step. These steps are succinctly explained in her Tidy Tuesday post which I highly recommend."
  },
  {
    "objectID": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html#step-5-fitting-on-resampled-folds",
    "href": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html#step-5-fitting-on-resampled-folds",
    "title": "Day 14 of #50daysofKaggle",
    "section": "Step 5: Fitting on resampled folds",
    "text": "Step 5: Fitting on resampled folds\nSo this part of it needs to be dealt with care. Resampling may take a while‚Ä¶\n\n\nCode\nstart_time <- Sys.time()\nset.seed(2023)\ndoParallel::registerDoParallel()\ntitanic_rs <- workflow_map(\n  titanic_wf_set,\n  \"fit_resamples\",\n  resamples = titanic_folds\n)\nend_time <- Sys.time()\n\n\n‚Ä¶ which is ‚Ä¶\n\n\nCode\nend_time - start_time\n\n\nTime difference of 10.71708 mins\n\n\nHmm‚Ä¶ So moving on. What does the workflow_set object look like now?\n\n\nCode\ntitanic_rs\n\n\n# A workflow set/tibble: 4 √ó 4\n  wflow_id             info             option    result   \n  <chr>                <list>           <list>    <list>   \n1 recipe_logistic_reg  <tibble [1 √ó 4]> <opts[1]> <rsmp[+]>\n2 recipe_rand_forest   <tibble [1 √ó 4]> <opts[1]> <rsmp[+]>\n3 recipe_svm_rbf       <tibble [1 √ó 4]> <opts[1]> <rsmp[+]>\n4 recipe_decision_tree <tibble [1 √ó 4]> <opts[1]> <rsmp[+]>"
  },
  {
    "objectID": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html#step-6-finding-the-winning-model",
    "href": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html#step-6-finding-the-winning-model",
    "title": "Day 14 of #50daysofKaggle",
    "section": "Step 6: Finding the winning model",
    "text": "Step 6: Finding the winning model\nThis is how the tidymodels package all fits in. If you‚Äôve got all the steps covered this far, the decision making shouldn‚Äôt take much effort\n\n\nCode\ncollect_metrics(titanic_rs)\n\n\n# A tibble: 8 √ó 9\n  wflow_id             .config preproc model .metric .esti‚Ä¶¬π  mean     n std_err\n  <chr>                <chr>   <chr>   <chr> <chr>   <chr>   <dbl> <int>   <dbl>\n1 recipe_logistic_reg  Prepro‚Ä¶ recipe  logi‚Ä¶ accura‚Ä¶ binary  0.675    15 0.0147 \n2 recipe_logistic_reg  Prepro‚Ä¶ recipe  logi‚Ä¶ roc_auc binary  0.695    15 0.0184 \n3 recipe_rand_forest   Prepro‚Ä¶ recipe  rand‚Ä¶ accura‚Ä¶ binary  0.824    15 0.00484\n4 recipe_rand_forest   Prepro‚Ä¶ recipe  rand‚Ä¶ roc_auc binary  0.872    15 0.00488\n5 recipe_svm_rbf       Prepro‚Ä¶ recipe  svm_‚Ä¶ accura‚Ä¶ binary  0.780    15 0.00706\n6 recipe_svm_rbf       Prepro‚Ä¶ recipe  svm_‚Ä¶ roc_auc binary  0.806    15 0.00704\n7 recipe_decision_tree Prepro‚Ä¶ recipe  deci‚Ä¶ accura‚Ä¶ binary  0.809    15 0.00476\n8 recipe_decision_tree Prepro‚Ä¶ recipe  deci‚Ä¶ roc_auc binary  0.811    15 0.00684\n# ‚Ä¶ with abbreviated variable name ¬π‚Äã.estimator\n\n\nSince we‚Äôre looking at classification, lets see which one of the models resulted in the best roc_auc?\n\n\nCode\ncollect_metrics(titanic_rs) %>% \n  filter(.metric == \"roc_auc\") %>% \n  arrange(desc(mean))\n\n\n# A tibble: 4 √ó 9\n  wflow_id             .config preproc model .metric .esti‚Ä¶¬π  mean     n std_err\n  <chr>                <chr>   <chr>   <chr> <chr>   <chr>   <dbl> <int>   <dbl>\n1 recipe_rand_forest   Prepro‚Ä¶ recipe  rand‚Ä¶ roc_auc binary  0.872    15 0.00488\n2 recipe_decision_tree Prepro‚Ä¶ recipe  deci‚Ä¶ roc_auc binary  0.811    15 0.00684\n3 recipe_svm_rbf       Prepro‚Ä¶ recipe  svm_‚Ä¶ roc_auc binary  0.806    15 0.00704\n4 recipe_logistic_reg  Prepro‚Ä¶ recipe  logi‚Ä¶ roc_auc binary  0.695    15 0.0184 \n# ‚Ä¶ with abbreviated variable name ¬π‚Äã.estimator\n\n\nLooks like the rand_forest is the winner!\n\nAnother wonderful way it all ties in together is the visualisation with a single line of code.\n\n\nCode\nautoplot(titanic_rs)\n\n\n\n\n\nPieces of code that just fit into each other feels like a spoon-full of ice cream!"
  },
  {
    "objectID": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html#step-7-8-fitting-the-winner-and-predictions",
    "href": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html#step-7-8-fitting-the-winner-and-predictions",
    "title": "Day 14 of #50daysofKaggle",
    "section": "Step 7 & 8: Fitting the winner and predictions",
    "text": "Step 7 & 8: Fitting the winner and predictions\nFrom the object titanic_rs we need to pick the winning model and fit to to_predict\n\n\nCode\nfinal_fit <- extract_workflow(titanic_rs, \n                              \"recipe_rand_forest\") %>% \n  fit(given_data)\n\nfinal_fit\n\n\n‚ïê‚ïê Workflow [trained] ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nPreprocessor: Recipe\nModel: rand_forest()\n\n‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n2 Recipe Steps\n\n‚Ä¢ step_dummy()\n‚Ä¢ step_normalize()\n\n‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, num.trees = ~1000,      num.threads = 1, verbose = FALSE, seed = sample.int(10^5,          1), probability = TRUE) \n\nType:                             Probability estimation \nNumber of trees:                  1000 \nSample size:                      891 \nNumber of independent variables:  908 \nMtry:                             30 \nTarget node size:                 10 \nVariable importance mode:         none \nSplitrule:                        gini \nOOB prediction error (Brier s.):  0.1386875 \n\n\nCreating a new dataframe for predictions\n\n\nCode\nfinal_predictions <- predict(object = final_fit, \n                             new_data = to_predict)\n\nfinal_predictions <- final_predictions %>% \n  rename(Survived = .pred_class) %>% \n  bind_cols(PassengerId = to_predict$passenger_id)\nhead(final_predictions)\n\n\n# A tibble: 6 √ó 2\n  Survived PassengerId\n  <fct>          <int>\n1 0                892\n2 0                893\n3 0                894\n4 0                895\n5 0                896\n6 0                897"
  },
  {
    "objectID": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html#step-9-submit-to-kaggle",
    "href": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html#step-9-submit-to-kaggle",
    "title": "Day 14 of #50daysofKaggle",
    "section": "Step 9: Submit to Kaggle",
    "text": "Step 9: Submit to Kaggle\nConverting the final_predictions to csv and uploading to kaggle\n\n\nCode\nwrite.csv(final_predictions, row.names = F, \n          file = \"submissions.csv\")\n\n\nUploaded it to Kaggle and behold‚Ä¶\n\nNot bad !!! that‚Äôs like‚Ä¶ top 12% percentile. Woohoo!\n\n\n\n.. And so did you, dear reader!!"
  },
  {
    "objectID": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html#peeking-under-the-hood",
    "href": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html#peeking-under-the-hood",
    "title": "Day 14 of #50daysofKaggle",
    "section": "Peeking under the hood",
    "text": "Peeking under the hood\nI tried to replicate the same errorbar plot generated by the function. For me, this was possibly one of the bigger conceptual lessons about tidymodels framework. Here‚Äôs how I did it:\nThere are 15 bootsample folds that were generated. The workflow_set maps the fit_resamples function on the workflow. This implies that each of the 4 models have generated 2 pairs of metrics (accuracy & roc_auc) for each of the 15 resamples.\nThe collect_metrics function generates the mean vale of each metric allowing us to chose the best one.\nThe manner in which all of this is done within a single object is sublime. Information is stored in tibbles within tibbles!\nFor instance, let us look at the class of titanic_rs\n\n\nCode\nclass(titanic_rs)\n\n\n[1] \"workflow_set\" \"tbl_df\"       \"tbl\"          \"data.frame\"  \n\n\nIt is a tibble with 4 columns out of which results is a list object consisting of more tibbles.\n\n\nCode\ntitanic_rs\n\n\n# A workflow set/tibble: 4 √ó 4\n  wflow_id             info             option    result   \n  <chr>                <list>           <list>    <list>   \n1 recipe_logistic_reg  <tibble [1 √ó 4]> <opts[1]> <rsmp[+]>\n2 recipe_rand_forest   <tibble [1 √ó 4]> <opts[1]> <rsmp[+]>\n3 recipe_svm_rbf       <tibble [1 √ó 4]> <opts[1]> <rsmp[+]>\n4 recipe_decision_tree <tibble [1 √ó 4]> <opts[1]> <rsmp[+]>\n\n\nWhat does the first tibble in the results list look like? This corresponds to the logistic regression model as shown in the table above.\n\n\nCode\ntitanic_rs$result[[1]]\n\n\n# Resampling results\n# Bootstrap sampling \n# A tibble: 15 √ó 4\n   splits            id          .metrics         .notes          \n   <list>            <chr>       <list>           <list>          \n 1 <split [891/318]> Bootstrap01 <tibble [2 √ó 4]> <tibble [3 √ó 3]>\n 2 <split [891/313]> Bootstrap02 <tibble [2 √ó 4]> <tibble [3 √ó 3]>\n 3 <split [891/340]> Bootstrap03 <tibble [2 √ó 4]> <tibble [3 √ó 3]>\n 4 <split [891/313]> Bootstrap04 <tibble [2 √ó 4]> <tibble [3 √ó 3]>\n 5 <split [891/326]> Bootstrap05 <tibble [2 √ó 4]> <tibble [3 √ó 3]>\n 6 <split [891/342]> Bootstrap06 <tibble [2 √ó 4]> <tibble [3 √ó 3]>\n 7 <split [891/316]> Bootstrap07 <tibble [2 √ó 4]> <tibble [3 √ó 3]>\n 8 <split [891/323]> Bootstrap08 <tibble [2 √ó 4]> <tibble [3 √ó 3]>\n 9 <split [891/338]> Bootstrap09 <tibble [2 √ó 4]> <tibble [3 √ó 3]>\n10 <split [891/331]> Bootstrap10 <tibble [2 √ó 4]> <tibble [3 √ó 3]>\n11 <split [891/319]> Bootstrap11 <tibble [2 √ó 4]> <tibble [3 √ó 3]>\n12 <split [891/322]> Bootstrap12 <tibble [2 √ó 4]> <tibble [3 √ó 3]>\n13 <split [891/340]> Bootstrap13 <tibble [2 √ó 4]> <tibble [3 √ó 3]>\n14 <split [891/321]> Bootstrap14 <tibble [2 √ó 4]> <tibble [3 √ó 3]>\n15 <split [891/320]> Bootstrap15 <tibble [2 √ó 4]> <tibble [3 √ó 3]>\n\nThere were issues with some computations:\n\n  - Warning(s) x1: Column(s) have zero variance so scaling cannot be used: `final_gr...   - Warning(s) x1: Column(s) have zero variance so scaling cannot be used: `final_gr...   - Warning(s) x1: Column(s) have zero variance so scaling cannot be used: `final_gr...   - Warning(s) x1: Column(s) have zero variance so scaling cannot be used: `final_gr...   - Warning(s) x1: Column(s) have zero variance so scaling cannot be used: `final_gr...   - Warning(s) x1: Column(s) have zero variance so scaling cannot be used: `final_gr...   - Warning(s) x1: Column(s) have zero variance so scaling cannot be used: `final_gr...   - Warning(s) x1: Column(s) have zero variance so scaling cannot be used: `final_gr...   - Warning(s) x1: Column(s) have zero variance so scaling cannot be used: `final_gr...   - Warning(s) x1: Column(s) have zero variance so scaling cannot be used: `final_gr...   - Warning(s) x1: Column(s) have zero variance so scaling cannot be used: `final_gr...   - Warning(s) x1: Column(s) have zero variance so scaling cannot be used: `final_gr...   - Warning(s) x1: Column(s) have zero variance so scaling cannot be used: `final_gr...   - Warning(s) x1: Column(s) have zero variance so scaling cannot be used: `final_gr...   - Warning(s) x1: Column(s) have zero variance so scaling cannot be used: `final_gr...   - Warning(s) x4: Column(s) have zero variance so scaling cannot be used: `final_gr...   - Warning(s) x11: Column(s) have zero variance so scaling cannot be used: `final_gr...   - Warning(s) x15: Column(s) have zero variance so scaling cannot be used: `final_gr...\n\nRun `show_notes(.Last.tune.result)` for more information.\n\n\nWhat does the .metrics list contain?\n\n\nCode\ntitanic_rs$result[[1]]$.metrics[[1]]\n\n\n# A tibble: 2 √ó 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy binary         0.682 Preprocessor1_Model1\n2 roc_auc  binary         0.655 Preprocessor1_Model1\n\n\nSo .metrics is a list of results that is generated for each of the folds. The above tibble is the resulting metrics after cross-validation of the first fold of titanic_folds using glm (logisitic classification).\nIf the intent is to create the errorgraph manually, then we‚Äôll need to extract the data within each of these tibbles.\nNow I wasn‚Äôt able to check the code for autoplot() and I‚Äôm pretty certain there‚Äôs a more elegant method out there. Hit me up if you feel there‚Äôs a better way to extract data from within tibbles in the comments here üëâüèº\nFirst step is to create an empty tibble consisting of 4 models, 2 metrics and 80 empty cells that need to be filled in from titanic_rs object.\n\n\nCode\ntidy_titanic_rs <- tibble(wf = rep(unique(titanic_rs$wflow_id), 15*2),\n                          metrics = rep(c(rep(\"accuracy\", 4), \n                                          rep(\"roc_auc\",4)),\n                                        15),\n                          values = rep(NA, 4*15*2))\nhead(tidy_titanic_rs, 8)\n\n\n# A tibble: 8 √ó 3\n  wf                   metrics  values\n  <chr>                <chr>    <lgl> \n1 recipe_logistic_reg  accuracy NA    \n2 recipe_rand_forest   accuracy NA    \n3 recipe_svm_rbf       accuracy NA    \n4 recipe_decision_tree accuracy NA    \n5 recipe_logistic_reg  roc_auc  NA    \n6 recipe_rand_forest   roc_auc  NA    \n7 recipe_svm_rbf       roc_auc  NA    \n8 recipe_decision_tree roc_auc  NA    \n\n\nDimensions of this empty table?\n\n\nCode\ndim(tidy_titanic_rs)\n\n\n[1] 120   3\n\n\nTo extract the values from the tibbles in titanic_rs and save in tidy_titanic_rs, I proceeded to employ a nested for loop. Dont‚Äô judge, i say!"
  },
  {
    "objectID": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html#extracting-the-tibbles",
    "href": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html#extracting-the-tibbles",
    "title": "Day 14 of #50daysofKaggle",
    "section": "Extracting the tibbles",
    "text": "Extracting the tibbles\npluck has got to be the coolest function I‚Äôve ever come across!\n\n\nCode\nbigtable <- purrr:::pluck(titanic_rs, 4)\nwflow_id_titanic <- unique(titanic_rs$wflow_id)\nfor(i in 1:length(wflow_id_titanic)){\n    \n  wflow_id <- wflow_id_titanic[i]\n  smalltable <- bigtable[[i]]\n  \n  for(j in 1:length(smalltable$.metrics)){\n    smallertable <- purrr::pluck(smalltable$.metrics, j)\n    tidy_titanic_rs$values[(tidy_titanic_rs$wf==wflow_id & \n                              tidy_titanic_rs$metrics==\"accuracy\")][j] <- smallertable$.estimate[smallertable$.metric == \"accuracy\"]\n    tidy_titanic_rs$values[(tidy_titanic_rs$wf==wflow_id & \n                              tidy_titanic_rs$metrics==\"roc_auc\")][j] <- smallertable$.estimate[smallertable$.metric == \"roc_auc\"]\n    \n  }\n}\n\ntidy_titanic_rs2 <- tidy_titanic_rs %>% \n  group_by(wf, metrics) %>% \n  summarise(value_min = mean(values) - 0.5*sd(values),\n            value_max = mean(values) + 0.5*sd(values),\n            value_mean = mean(values)) %>% ungroup() %>% \n  right_join(tidy_titanic_rs, by = c(\"wf\", \"metrics\"))\n\n\nFor some reason, value_max and value_min for the error-bar are 0.5 * ùùà or approximately ¬±19% of the mean of values"
  },
  {
    "objectID": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html#manually-generated-plot",
    "href": "posts/2023-03-06-day-14-of-50daysofkaggle/index.html#manually-generated-plot",
    "title": "Day 14 of #50daysofKaggle",
    "section": "Manually generated plot",
    "text": "Manually generated plot\nWith the data in a rectangular tidy format, the rest of the magic is handled by ggplot\ncracks knuckles\n\n\nCode\ntidy_titanic_rs2 %>% \nggplot(aes(x = reorder(wf, desc(value_mean)), \n             y = values, \n             color = wf))+\n  geom_errorbar(aes(ymax = value_max, ymin = value_min), \n                width = 0.1)+\n  geom_point(aes(y= value_mean))+\n  scale_y_continuous(breaks = seq(0.65, 0.9, 0.05),\n                     limits = c(0.65, 0.9))+\n  theme(legend.position = \"none\")+\n  labs(x = NULL, y = NULL)+\n  facet_wrap(~metrics)\n\n\n\n\n\nAnd here once again is the auto-generated plot. Not bad, eh?\n\n\nCode\nautoplot(titanic_rs)\n\n\n\n\n\n\n\n\nOooh yeeah!"
  },
  {
    "objectID": "posts/2024-06-28-Back-after-a-while/index.html",
    "href": "posts/2024-06-28-Back-after-a-while/index.html",
    "title": "Back after a while",
    "section": "",
    "text": "Been a busy year and have been away from the blog for a while now. Publishing a new post to see if all systems are working as I last remember them to be.\n\n\n\nknock knock"
  },
  {
    "objectID": "posts/2023-05-22-Clustering-PCA-Kmeans/index.html",
    "href": "posts/2023-05-22-Clustering-PCA-Kmeans/index.html",
    "title": "Day 15 of #50daysofKaggle",
    "section": "",
    "text": "Today‚Äôs excercise is an attempt to replicate the customer segmentation applied in Winarta‚Äôs blog\n\n\nCode\nlibrary(tidyverse)\nlibrary(FactoMineR)\nlibrary(factoextra)\nlibrary(animation)"
  }
]