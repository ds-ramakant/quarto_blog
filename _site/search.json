[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I am a media professional with a natural curiosity towards media and technology. I‚Äôve worked across broadcast and OTT companies since 2008 with a wide experience across Content, Business & Marketing. In my last role at MX Player, I work on cross-functional initiatives to help boost platform growth and engagement. I am fluent in English, Hindi & Telugu. My favourite weekend activity is to switch off the laptop and listen to vinyls with my 5-year old daughter üë®‚Äçüë©‚Äçüëß"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Null Hypothesis",
    "section": "",
    "text": "Hi there üëãüèº My name is Ramakant and I am a consumer marketing enthusiast with a natural curiosity towards data and technology. I started this blog as a way to sharpen my data science skills and occasionally pour those creative juices."
  },
  {
    "objectID": "index.html#get-in-touch",
    "href": "index.html#get-in-touch",
    "title": "The Null Hypothesis",
    "section": "Get In Touch ‚úçÔ∏è",
    "text": "Get In Touch ‚úçÔ∏è\nI‚Äôm always on the lookout for a good conversation. Feel free to mail me at d.s.ramakant@gmail.com or check out my Linkedin profile\n\nLink to my resume (broken for now) üìÉ"
  },
  {
    "objectID": "index.html#blog-posts",
    "href": "index.html#blog-posts",
    "title": "The Null Hypothesis",
    "section": "Blog Posts",
    "text": "Blog Posts"
  },
  {
    "objectID": "posts/2022-01-14-hello-world2/index.en.html",
    "href": "posts/2022-01-14-hello-world2/index.en.html",
    "title": "Hello World2",
    "section": "",
    "text": "A lot of this is courtesy of Apres Hill‚Äôs blog\nhttps://www.apreshill.com/blog/2020-12-new-year-new-blogdown/#step-1-create-repo"
  },
  {
    "objectID": "posts/2022-04-05-what-a-wonderful-wordle/index.en.html",
    "href": "posts/2022-04-05-what-a-wonderful-wordle/index.en.html",
    "title": "What a wonderful ‚Äòwordle‚Äô!",
    "section": "",
    "text": "Who ever thought that a bunch of black and green boxes would bring out the logophile in us all? With friends and family groups sharing their progress, I find this to be an entertaining mind-puzzle to kickstart the day.\nAnd I was not alone in my quest for 5 letter words. Wordle has tickled the fascination of many in the data science community. I found Arthur Holtz‚Äôs lucid breakdown of the Wordle dataset quite interesting. Of course, there is 3B1B‚Äôs incredibly detailed videos on applying Information Theory to this 6-by-5 grid. (original video as well as the follow-up errata)\nOthers have simulated the wordle game (like here) or even solved it for you (like this blog). I‚Äôve read at least one blog post that has an academic take on the matter.\nFortunately for the reader, none of the above will be attempted by me. My inspiration comes from Gerry Chng‚Äôs Frequency Analysis Approach where I‚Äôve tried to understand the most commonly occuring letters in the official word list by position by considering a ranking mechanism"
  },
  {
    "objectID": "posts/2022-04-05-what-a-wonderful-wordle/index.en.html#what-is-a-wordle",
    "href": "posts/2022-04-05-what-a-wonderful-wordle/index.en.html#what-is-a-wordle",
    "title": "What a wonderful ‚Äòwordle‚Äô!",
    "section": "What is a wordle?",
    "text": "What is a wordle?\nThe game rules are fairly simple:\n\nYou need to guess a 5-letter word. One new word is given every day\nYou are given 6 guesses\nAfter every guess, each square is coded by a color\n\nGREY: chosen letter is not in the word\nYELLOW: chosen letter is in the word by wrong position\nGREEN: chosen letter is in the word and in the correct position\n\nRepetition of letters is allowed\n\nThat‚Äôs it!\nIn my opinion, one of the reasons for the game going viral is the way the results are shared. You‚Äôve possibly seen something like this floating around:\n\n\n\nSample world share\n\n\n‚Ä¶And if your family too has been bitten hard by the Wordle bug, then you would be familiar with group messages like this!\n\n\n\nWorld share in whatsapp"
  },
  {
    "objectID": "posts/2022-04-05-what-a-wonderful-wordle/index.en.html#frequency-analysis",
    "href": "posts/2022-04-05-what-a-wonderful-wordle/index.en.html#frequency-analysis",
    "title": "What a wonderful ‚Äòwordle‚Äô!",
    "section": "Frequency analysis",
    "text": "Frequency analysis\nArthur Hotlz‚Äôs blog is a good place to start for extracting and loading the Official Wordle list. After parsing and cleaning the data, here‚Äôs all the words broken down into a single rectangular dataframe word_list .\nUpdate 29th Jan ‚Äô23: NYT‚Äôs .js file is not retrieving any list for some reason. I‚Äôve referred to Arjun Vikram‚Äôs repo on dagshub\n\n\nCode\nknitr::opts_chunk$set(warning = FALSE, message = FALSE) \nsuppressMessages({ \nlibrary(httr)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(ggthemes)\nlibrary(scales)\nlibrary(tidyr)\nlibrary(tibble)\nlibrary(forcats)\nlibrary(knitr)\nlibrary(kableExtra)\ntheme_set(theme_light())\n})\n\nurl <- \"https://www.nytimes.com/games/wordle/main.18637ca1.js\" #not working\nurl2 <- \"https://dagshub.com/arjvik/wordle-wordlist/raw/e8d07d33a59a6b05f3b08bd827385604f89d89a0/answerlist.txt\"\nwordle_script_text <- GET(url2) %>% \n  content(as = \"text\", encoding = \"UTF-8\")\n# word_list = substr(\n#   wordle_script_text,\n#   # cigar is the first word\n#   str_locate(wordle_script_text, \"cigar\")[,\"start\"],\n#   # shave is the last word\n#   str_locate(wordle_script_text, \"shave\")[,\"end\"]) %>%\n#   str_remove_all(\"\\\"\") %>%\n#   str_split(\",\") %>%\n#   data.frame() %>%\n#   select(word = 1) %>%\n#   mutate(word = toupper(word))\n\n\nwordle_list <- str_split(wordle_script_text, \"\\n\")\n\nwordle_list <- data.frame(wordle_list) \n\nwordle_list <- rename(wordle_list, word = names(wordle_list)[1] ) %>% mutate(word = toupper(word)) #renaming column to 'word'\n\ndim(wordle_list)\n\n\n[1] 2310    1\n\n\nCode\nhead(wordle_list)\n\n\n   word\n1 CIGAR\n2 REBUT\n3 SISSY\n4 HUMPH\n5 AWAKE\n6 BLUSH\n\n\nModification to the above is another dataframe with each of the characters separated into columns which we‚Äôll call position_word_list\nThe line select(-x) removes the empty column that is created due to the seperate() function\n\n\nCode\nposition_word_list <- wordle_list %>% \n  separate(word, \n           sep = \"\", \n           into = c(\"x\",\"p1\",\"p2\",\"p3\",\"p4\",\"p5\")) %>% \n  select(-x)\nhead(position_word_list,10)\n\n\n   p1 p2 p3 p4 p5\n1   C  I  G  A  R\n2   R  E  B  U  T\n3   S  I  S  S  Y\n4   H  U  M  P  H\n5   A  W  A  K  E\n6   B  L  U  S  H\n7   F  O  C  A  L\n8   E  V  A  D  E\n9   N  A  V  A  L\n10  S  E  R  V  E\n\n\nNow onto some frequency analysis. Here‚Äôs a breakdown of all the letters in the wordle list sorted by number of occurrences stored in letter_list and creating a simple bar graph.\n\n\nCode\nletter_list <- wordle_list %>%\n  as.character() %>%\n  str_split(\"\") %>% \n  as.data.frame() %>% \n  select(w_letter = 1) %>% \n  filter(row_number()!=1) %>%\n  filter(w_letter %in% LETTERS) %>% \n  mutate(type = case_when(w_letter %in% c(\"A\",\"E\",\"I\",\"O\",\"U\") ~ \"vowel\",\n                          T ~ \"consonant\")) %>% \n  group_by(w_letter, type) %>% \n  summarise(freq = n()) %>% \n  arrange(desc(freq))\n\nletter_list %>% ungroup() %>% \n  ggplot(aes(x = reorder(w_letter, -freq), y = freq))+\n  geom_col(aes(fill = type))+\n  scale_y_continuous(labels = comma)+\n  geom_text(aes(label = freq), \n            size = 3)+\n  labs(x = \"Letter\", y = \"Frequency\",\n       title = \"Frequency of letters in Official Wordle list\")\n\n\n\n\n\nThis is interesting. Now I‚Äôm curious to know the top words by each position. To do this, I created a single table called freq_table that provides me with the frequency of occurrences by position for each letter. To iterate this process across all the 5 places, I used a for loop. Output is generated via the kableExtra package which provides a neat scrollable window\n\n\nCode\n#declaring null table\nfreq_table <- tibble(alpha = LETTERS)\n\nfor(i in 1:5){\n    test <- position_word_list %>% \n    select(all_of(i)) %>%\n# group_by_at() used for column index ID\n    group_by_at(1) %>% \n    summarise(f = n()) %>% \n    arrange(desc(f)) %>% \n#first column returns p1, p2.. etc and is standardised\n    rename(a = 1) \n\n#adding the freq values to a new dataframe\n    freq_table <- freq_table %>%\n    left_join(test, by = c(\"alpha\" = \"a\")) \n\n#renaming column name to reflect the position number\n    colnames(freq_table)[1+i] = paste0(\"p\",i)\n    rm(test)\n}\n#replacing NA with zero\nfreq_table[is.na(freq_table)] <- 0 \n#output using kable's scrollable window \nkable(freq_table, \n      format = \"html\", \n      caption = \"Frequency Table\") %>%\n    kable_styling() %>%\n    scroll_box(width = \"70%\", height = \"300px\") %>% \n  kable_classic()\n\n\n\n\nFrequency Table\n \n  \n    alpha \n    p1 \n    p2 \n    p3 \n    p4 \n    p5 \n  \n \n\n  \n    A \n    140 \n    304 \n    306 \n    162 \n    63 \n  \n  \n    B \n    173 \n    16 \n    56 \n    24 \n    11 \n  \n  \n    C \n    198 \n    40 \n    56 \n    150 \n    31 \n  \n  \n    D \n    111 \n    20 \n    75 \n    69 \n    118 \n  \n  \n    E \n    72 \n    241 \n    177 \n    318 \n    422 \n  \n  \n    F \n    135 \n    8 \n    25 \n    35 \n    26 \n  \n  \n    G \n    115 \n    11 \n    67 \n    76 \n    41 \n  \n  \n    H \n    69 \n    144 \n    9 \n    28 \n    137 \n  \n  \n    I \n    34 \n    201 \n    266 \n    158 \n    11 \n  \n  \n    J \n    20 \n    2 \n    3 \n    2 \n    0 \n  \n  \n    K \n    20 \n    10 \n    12 \n    55 \n    113 \n  \n  \n    L \n    87 \n    200 \n    112 \n    162 \n    155 \n  \n  \n    M \n    107 \n    38 \n    61 \n    68 \n    42 \n  \n  \n    N \n    37 \n    87 \n    137 \n    182 \n    130 \n  \n  \n    O \n    41 \n    279 \n    243 \n    132 \n    58 \n  \n  \n    P \n    141 \n    61 \n    57 \n    50 \n    56 \n  \n  \n    Q \n    23 \n    5 \n    1 \n    0 \n    0 \n  \n  \n    R \n    105 \n    267 \n    163 \n    150 \n    212 \n  \n  \n    S \n    365 \n    16 \n    80 \n    171 \n    36 \n  \n  \n    T \n    149 \n    77 \n    111 \n    139 \n    253 \n  \n  \n    U \n    33 \n    185 \n    165 \n    82 \n    1 \n  \n  \n    V \n    43 \n    15 \n    49 \n    45 \n    0 \n  \n  \n    W \n    82 \n    44 \n    26 \n    25 \n    17 \n  \n  \n    X \n    0 \n    14 \n    12 \n    3 \n    8 \n  \n  \n    Y \n    6 \n    22 \n    29 \n    3 \n    364 \n  \n  \n    Z \n    3 \n    2 \n    11 \n    20 \n    4 \n  \n\n\n\n\n\nThis table looks good. However, for my visualisation, I want to plot the top 10 letters in each position. For this, I‚Äôm going to use pivot_longer() to make it easier to generate the viz.\n\n\nCode\nfreq_table_long10 <- freq_table %>% \n  pivot_longer(cols = !alpha, names_to = \"position\", values_to = \"freq\") %>% \n  select(position, alpha, freq) %>% \n  arrange(position, -freq) %>% \n  group_by(position) %>% \n  slice_head(n = 10) %>% ungroup\n\nkable(freq_table_long10, \n      format = \"html\", \n      caption = \"Top 10 letters within each position\") %>%\n    kable_styling() %>%\n    scroll_box(height = \"200px\") %>% \n  kable_classic()\n\n\n\n\nTop 10 letters within each position\n \n  \n    position \n    alpha \n    freq \n  \n \n\n  \n    p1 \n    S \n    365 \n  \n  \n    p1 \n    C \n    198 \n  \n  \n    p1 \n    B \n    173 \n  \n  \n    p1 \n    T \n    149 \n  \n  \n    p1 \n    P \n    141 \n  \n  \n    p1 \n    A \n    140 \n  \n  \n    p1 \n    F \n    135 \n  \n  \n    p1 \n    G \n    115 \n  \n  \n    p1 \n    D \n    111 \n  \n  \n    p1 \n    M \n    107 \n  \n  \n    p2 \n    A \n    304 \n  \n  \n    p2 \n    O \n    279 \n  \n  \n    p2 \n    R \n    267 \n  \n  \n    p2 \n    E \n    241 \n  \n  \n    p2 \n    I \n    201 \n  \n  \n    p2 \n    L \n    200 \n  \n  \n    p2 \n    U \n    185 \n  \n  \n    p2 \n    H \n    144 \n  \n  \n    p2 \n    N \n    87 \n  \n  \n    p2 \n    T \n    77 \n  \n  \n    p3 \n    A \n    306 \n  \n  \n    p3 \n    I \n    266 \n  \n  \n    p3 \n    O \n    243 \n  \n  \n    p3 \n    E \n    177 \n  \n  \n    p3 \n    U \n    165 \n  \n  \n    p3 \n    R \n    163 \n  \n  \n    p3 \n    N \n    137 \n  \n  \n    p3 \n    L \n    112 \n  \n  \n    p3 \n    T \n    111 \n  \n  \n    p3 \n    S \n    80 \n  \n  \n    p4 \n    E \n    318 \n  \n  \n    p4 \n    N \n    182 \n  \n  \n    p4 \n    S \n    171 \n  \n  \n    p4 \n    A \n    162 \n  \n  \n    p4 \n    L \n    162 \n  \n  \n    p4 \n    I \n    158 \n  \n  \n    p4 \n    C \n    150 \n  \n  \n    p4 \n    R \n    150 \n  \n  \n    p4 \n    T \n    139 \n  \n  \n    p4 \n    O \n    132 \n  \n  \n    p5 \n    E \n    422 \n  \n  \n    p5 \n    Y \n    364 \n  \n  \n    p5 \n    T \n    253 \n  \n  \n    p5 \n    R \n    212 \n  \n  \n    p5 \n    L \n    155 \n  \n  \n    p5 \n    H \n    137 \n  \n  \n    p5 \n    N \n    130 \n  \n  \n    p5 \n    D \n    118 \n  \n  \n    p5 \n    K \n    113 \n  \n  \n    p5 \n    A \n    63 \n  \n\n\n\n\n\nSo we have the # of occurences in each position laid out in a tidy format in one long rectangular dataframe. Now sprinkling some magic courtesy ggplot\n\nSide note on reordering within facets\nI tried my best to understand why I was unable to sort within each facet in spite of using free_y. Apparently that‚Äôs a known issue and a workaround has been discussed by David Robinson, Julia Silger and Tyler Rinker. To achieve this, two more functions need to be created reorder_within and scale_y_reordered\n\n\nCode\nreorder_within <- function(x, by, within, fun = mean, sep = \"___\", ...) {\n  new_x <- paste(x, within, sep = sep)\n  stats::reorder(new_x, by, FUN = fun)\n}\n\nscale_y_reordered <- function(..., sep = \"___\") {\n  reg <- paste0(sep, \".+$\")\n  ggplot2::scale_y_discrete(labels = function(x) gsub(reg, \"\", x), ...)\n}\n\nfreq_table_long10 %>% \n  mutate(type = case_when(alpha %in% c(\"A\",\"E\",\"I\",\"O\",\"U\") ~ \"vowel\",\n                          T ~ \"consonant\")) %>% \n  ggplot(aes(y = reorder_within(alpha, freq, position), x = freq))+\n  geom_col(aes(fill = type))+\n  scale_y_reordered()+\n  facet_wrap(~position, \n             scales = \"free_y\", \n             ncol = 5)+\n  labs(x = \"Frequency\", y = \"Letter\",\n       title = \"Frequency of top 10 letters by position in Official Wordle list \",\n       caption = \"D.S.Ramakant Raju\\nwww.linkedin.com/in/dsramakant/\")\n\n\n\n\n\nAha! Things are starting to get more clearer. Highly common letters in the 1st position are S, C, B, T and P - notice there‚Äôs only 1 vowel (A) that occurs in the top 10. Vowels appear more frequently in the 2nd and 3rd positions. Last position has a higher occurrence of E, Y, T, R & L"
  },
  {
    "objectID": "posts/2022-04-05-what-a-wonderful-wordle/index.en.html#which-words-can-be-the-best-worlde-openers",
    "href": "posts/2022-04-05-what-a-wonderful-wordle/index.en.html#which-words-can-be-the-best-worlde-openers",
    "title": "What a wonderful ‚Äòwordle‚Äô!",
    "section": "Which words can be the best Worlde openers?",
    "text": "Which words can be the best Worlde openers?\nArmed with the above knowledge, we now can filter out the commonly occurring words. Also I use a naive method to rank these words basis the occurrence of the letters. For instance, in the picture above, the word S A I N T seems to be a valid word comprising of the top occurring letters.\nAdmittedly, I use a pretty crude method to determine the best openers. Known drawbacks of this methodology are:\n\nDoesn‚Äôt consider the future path of the word (number of steps to get to the right word)\nOnly considers the rank of the letters and not the actual probability of occurrence\n\nWith that out of the way, I was able to determine that there are 39 words that can be formed with the top 5 occurring letters in each position. I‚Äôve created a score that is determined by the rank of each letter within its position. For instance, S A I N T gets a score of 9 by summing up 1 (S in first position) + 1 (A in second position) + 2 (I in third) + 2 (N in fourth) + 3 (T in fifth). The lower the score, the higher the frequency of occurrences. Scroll below to read the rest of the words.\n\n\nCode\n#function to pick the top 5 letters\ntop5_selection <- function(x)\n{x %>% arrange(desc(x[2])) %>% head(5) %>% select(1)}\n#defining null table\nfinal_grid <- tibble(ranking = 1:5)\n\nfor(i in 2:length(freq_table)){\n  t <- top5_selection(select(freq_table,1,all_of(i)))\n  final_grid <- cbind(final_grid,t)\n  colnames(final_grid)[i] = paste0(\"p\",i-1)\n}\ntopwords <- position_word_list %>% \nfilter(p1 %in% final_grid$p1,\n       p2 %in% final_grid$p2,\n       p3 %in% final_grid$p3,\n       p4 %in% final_grid$p4,\n       p5 %in% final_grid$p5) \n\n#finding consolidated score of each word\ntopwords %<>%\n  rowwise() %>% \n  mutate(p1_rank = which(p1 == final_grid$p1),\n         p2_rank = which(p2 == final_grid$p2),\n         p3_rank = which(p3 == final_grid$p3),\n         p4_rank = which(p4 == final_grid$p4),\n         p5_rank = which(p5 == final_grid$p5))\n\ntopwords2 <- topwords %>% \n  transmute(word = paste0(p1,p2,p3,p4,p5),\n         score = sum(p1_rank, p2_rank,p3_rank, p4_rank, p5_rank)) %>% \n  arrange(score)\n\nkable(topwords2, \n      format = \"html\",\n      caption = \"Top 39 words\") %>%\n    kable_styling() %>%\n    scroll_box(width = \"50%\", height = \"400px\") %>% \n  kable_classic()\n\n\n\n\nTop 39 words\n \n  \n    word \n    score \n  \n \n\n  \n    SAINT \n    9 \n  \n  \n    CRANE \n    9 \n  \n  \n    COAST \n    11 \n  \n  \n    BRINE \n    11 \n  \n  \n    CEASE \n    11 \n  \n  \n    CRONE \n    11 \n  \n  \n    CAUSE \n    12 \n  \n  \n    CRIER \n    12 \n  \n  \n    BRINY \n    12 \n  \n  \n    BOAST \n    12 \n  \n  \n    TAINT \n    12 \n  \n  \n    CRONY \n    12 \n  \n  \n    TEASE \n    13 \n  \n  \n    POISE \n    13 \n  \n  \n    TOAST \n    13 \n  \n  \n    PAINT \n    13 \n  \n  \n    BOOST \n    14 \n  \n  \n    POINT \n    14 \n  \n  \n    COUNT \n    14 \n  \n  \n    PRONE \n    14 \n  \n  \n    BEAST \n    14 \n  \n  \n    PRINT \n    15 \n  \n  \n    PAUSE \n    15 \n  \n  \n    TAUNT \n    15 \n  \n  \n    PROSE \n    15 \n  \n  \n    CREST \n    15 \n  \n  \n    CRUST \n    16 \n  \n  \n    BRIAR \n    16 \n  \n  \n    BOULE \n    16 \n  \n  \n    POESY \n    16 \n  \n  \n    CRUEL \n    16 \n  \n  \n    PRUNE \n    16 \n  \n  \n    BRUNT \n    16 \n  \n  \n    TRUER \n    17 \n  \n  \n    TREAT \n    18 \n  \n  \n    TRIAL \n    18 \n  \n  \n    TRUST \n    18 \n  \n  \n    TRULY \n    19 \n  \n  \n    TROLL \n    20 \n  \n\n\n\n\n\nThere we have it. My take on the best opening words.\nI‚Äôve used words such as SAINT, CRANE, COAST etc and they‚Äôve been reasonably useful to me.\nWhich are your favourite opening words? Please do leave a comment to let me know!"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn‚Äôt specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/2022-06-01-tidytuesday-nyt-bestsellers-list/index.en.html",
    "href": "posts/2022-06-01-tidytuesday-nyt-bestsellers-list/index.en.html",
    "title": "TidyTuesday NYT Bestsellers list",
    "section": "",
    "text": "‚ÄúLearn by practice!‚Äù is a maxim that every coder/analyst agrees upon. One of the admirable initiatives by the R/ RStudio community is Tidy Tuesday - every week a new dataset is released for enthusiasts to dig into. A few days back, an interesting dataset caught my eye - NYT‚Äôs Bestsellers List from 1930 to 2021. This one was particularly unique as it mirrored a lot of projects that I‚Äôve been doing on the OTT side as well. So I cracked my knuckles and jumped right in!"
  },
  {
    "objectID": "posts/2022-06-01-tidytuesday-nyt-bestsellers-list/index.en.html#objective",
    "href": "posts/2022-06-01-tidytuesday-nyt-bestsellers-list/index.en.html#objective",
    "title": "TidyTuesday NYT Bestsellers list",
    "section": "Objective",
    "text": "Objective\n\nUnderstanding longevity & seasonality of how books track on the NYT bestseller‚Äôs list\nDeeper understanding of using customizing themes and fonts on the ggplot package\n\n\nLoading the data\nStarting off by loading the data and the libraries\n\n\nCode\nknitr::opts_chunk$set(warning = FALSE, message = FALSE) \nsuppressMessages({\n  library(tidyverse)\n  library(scales)\n  library(knitr)\n  library(tidytuesdayR)\n  library(forcats)\n  library(lubridate)\n  library(RColorBrewer)\n})\n\ntt_raw <- tt_load(\"2022-05-10\")\n\n\n--- Compiling #TidyTuesday Information for 2022-05-10 ----\n\n\n--- There are 2 files available ---\n\n\n--- Starting Download ---\n\n\n\n    Downloading file 1 of 2: `nyt_titles.tsv`\n    Downloading file 2 of 2: `nyt_full.tsv`\n\n\n--- Download complete ---\n\n\nCode\ndata <- tt_raw$nyt_titles\n\n\nWhat‚Äôs in the Tidy Tuesday dataset?\n\n\nCode\nglimpse(data)\n\n\nRows: 7,431\nColumns: 8\n$ id          <dbl> 0, 1, 10, 100, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1‚Ä¶\n$ title       <chr> \"\\\"H\\\" IS FOR HOMICIDE\", \"\\\"I\\\" IS FOR INNOCENT\", \"''G'' I‚Ä¶\n$ author      <chr> \"Sue Grafton\", \"Sue Grafton\", \"Sue Grafton\", \"W. Bruce Cam‚Ä¶\n$ year        <dbl> 1991, 1992, 1990, 2012, 2006, 2016, 1985, 1994, 2002, 1999‚Ä¶\n$ total_weeks <dbl> 15, 11, 6, 1, 1, 3, 16, 5, 4, 1, 3, 2, 11, 6, 9, 8, 1, 1, ‚Ä¶\n$ first_week  <date> 1991-05-05, 1992-04-26, 1990-05-06, 2012-05-27, 2006-02-1‚Ä¶\n$ debut_rank  <dbl> 1, 14, 4, 3, 11, 1, 9, 7, 7, 12, 13, 5, 12, 2, 11, 13, 2, ‚Ä¶\n$ best_rank   <dbl> 2, 2, 8, 14, 14, 7, 2, 10, 12, 17, 13, 13, 8, 5, 5, 11, 4,‚Ä¶\n\n\n\n\nExploratory Data Analysis (sort of)\nQuick EDA tells us that there the number of books in the #1 spot each year during the 50s have been increasing while the number of weeks they‚Äôve spent on the NYT list has been decreasing. 2020-21 is excluded as I‚Äôm breaking up the period into decades for easy analysis\n\n\nCode\ndata %>% \n  mutate(decade = factor(10*year %/% 10)) %>% \n  filter(best_rank==1, year<2020) %>% \n  group_by(decade) %>% \n  summarise(avg_weeks = mean(total_weeks),\n            no_of_rank1 = n_distinct(title))\n\n\n# A tibble: 9 √ó 3\n  decade avg_weeks no_of_rank1\n  <fct>      <dbl>       <int>\n1 1930        17.1          74\n2 1940        30.1          59\n3 1950        52.4          35\n4 1960        45.7          31\n5 1970        38.6          46\n6 1980        29.5          78\n7 1990        25.7          99\n8 2000        12.5         220\n9 2010        10.3         306\n\n\nThis is a fantastic starting point. Intuitively, this makes a lot of sense. There‚Äôs far more competition for the #1 spot in the last 20 years which is driving down the longevity. Compare the 50‚Äôs to the 2010‚Äôs and the trend is hard to miss. This table is only for the books that made it to the #1 position. But how about the rest of the other books? A visual representation draws the same conclusion more elegantly.\n\n\nVisualising Longevity\nHat-tip to a few outstanding viz I came across while researching the NYT theme. Bob Rudis‚Äô Supreme Annotations and Rahul Sangole‚Äôs Visualizing Correlations\n\n\nCode\n#loading fonts that resemble the NYT viz\n#inspired by https://rud.is/b/2016/03/16/supreme-annotations/\n\nlibrary(showtext)\nshowtext_auto()\nfont_add(family = \"Open Sans\", \n         regular = \"OpenSans-CondLight.ttf\", \n         italic = \"OpenSans-CondLightItalic.ttf\", \n         bold = \"OpenSans-CondBold.ttf\")\n\n#changing facet labels as shown here \n#https://ggplot2.tidyverse.org/reference/as_labeller.html\nfacet_labels <- as_labeller(c(`1930`= \"1930 to 1939\",\n                              `1940`= \"1940 to 1949\",\n                              `1950`= \"1950 to 1959\",\n                              `1960`= \"1960 to 1969\",\n                              `1970`= \"1970 to 1979\",\n                              `1980`= \"1980 to 1989\",\n                              `1990`= \"1990 to 1999\",\n                              `2000`= \"2000 to 2009\",\n                              `2010`= \"2010 to 2019\"))\n\n\n#annotations for individual facet as discussed here https://stackoverflow.com/a/11889798/7938068\nannot_x <- data.frame(debut_rank = 5, \n                      total_weeks = 111,\n                      lab = \"Each dot\\n is a book\",\n                      decade = 1940)\n\ngraph1 <- data %>% \n  filter(best_rank==1,year<2020) %>% \n  mutate(decade = factor(10*year %/% 10)) %>% \n  ggplot(aes(x = debut_rank, y  = total_weeks))+\n  geom_point(aes(color = decade, group = debut_rank))+\n  facet_grid(~decade ,labeller = facet_labels)\n\ngraph1 <- graph1+\n  theme_minimal(base_family = \"Open Sans\")+\n  scale_color_brewer(palette = \"Paired\")+\n  labs(title = \"Longevity of NYT bestsellers has been decreasing\", \n       subtitle = \"Analysis of books that reached highest of #1 on the NYT chart tells us that starting from the 1950s, the bestsellers have reduced their longevity - or time spent on the chart.\\nFor instance, the top ranked books released in the 50s spent around 52 weeks on the chart while in contrast by the 2010s, they only spent 10 weeks.\",\n       caption = \"TidyTuesday Week 19, 2022\\n Prepared by D.S.Ramakant Raju (www.ds-ramakant.com)\",\n       x = \"Rank of title on debut week\",\n       y = \"Number of weeks on the bestsellers list\")+\n  theme(panel.border = element_rect(color = \"#2b2b2b\", \n                                    fill = NA), #borders for each facet panel\n        legend.position = \"none\", #removing legend\n        strip.text = element_text(face = \"italic\"),\n        plot.title = element_text(size = 14, face = \"bold\"),\n        panel.grid.major.x = element_line(linetype = \"dotted\", \n                                          color = \"black\"),\n        panel.grid.minor.x = element_blank(),\n        panel.grid.minor.y = element_blank(),\n        plot.caption = element_text(size = 12),\n        plot.subtitle = element_text(size = 12)) +\n  scale_y_continuous(breaks = seq(from = 25, to = 175, by = 25))+\n  #annotations by default is applied to all facets\n  #for individual facet annotations, check https://stackoverflow.com/a/11889798/7938068\n  geom_text(data = annot_x, \n            aes(x = debut_rank, y = total_weeks, \n                family = \"Open Sans\", alpha = 0.8,\n                hjust = -0.2, vjust = -0.2),\n            label = annot_x$lab\n            )\n\n\n\nprint(graph1)\n\n\n\n\n\n\n\nVisualising Seasonality\nNow lets look at seasonality - is there any trend as far as the launch month is concerned? For the sake of analysis, I‚Äôve truncated the analysis period to 2010 onwards to keep it more relevant and exlcude irrelvant historical data.\n\n\nCode\ngraph2 <- data %>% \n  filter(best_rank<11, year> 2010) %>% \n  mutate(month = month(first_week, label = T),\n         stage = case_when(year<=2015 ~ \"2011-2015\",\n                           year> 2015 ~ \"2016-2020\",\n                           T ~ \"x\")) %>% \n  group_by(stage,year,month) %>% \n  summarise(n = n_distinct(title)) %>% \n  mutate(all_titles = ave(n, year, FUN = sum),\n         pct = n/all_titles) %>%  \n  ggplot(aes(x = month, y = pct, group = 1))+\n  geom_point(size = 2, \n             alpha = 0.5, position = \"jitter\")+\n  geom_smooth(se = T) \n\n\ngraph2 <- graph2+\n  theme_minimal(base_family = \"Open Sans\")+\n  scale_color_brewer(palette = \"Paired\")+\n  labs(title = \"Monthly seasonality of books that featured in the top 10 of NYT Bestsellers list (2010-2021)\", \n       subtitle = \"Books launched in Summer (Apr-May) or Fall (Sep-Oct) were more likely to make it feature in the top 10\",\n       caption = \"TidyTuesday Week 19, 2022\\n Prepared by D.S.Ramakant Raju, www.ds-ramakant.com\",\n       x = \"Months (2010-2019)\",\n       y = \"%age of books launched within that year\")+\n  scale_y_continuous(labels = label_percent(accuracy = 1),\n                     breaks = seq(from = 0, to = 0.2, by= 0.05), \n                     limits = c(0,0.15))+\n  theme(axis.line.x = element_line(color = \"grey\"),\n        panel.grid.minor.y = element_blank())\n\ngraph2\n\n\n\n\n\nThis is a fairly straightforward and replicable analysis. If you‚Äôre a #TidyTuesday fan please feel free to share your work in the comments below"
  },
  {
    "objectID": "posts/2022-10-07-day-1-of-50daysofkaggle/index.en.html",
    "href": "posts/2022-10-07-day-1-of-50daysofkaggle/index.en.html",
    "title": "day 1 of #50daysofkaggle",
    "section": "",
    "text": "Introducing my own personal sprint training ‚Äú50 Days of Kaggle‚Äù\nThe task is simple:\nI‚Äôd want to use this blog to journal my progress. Hopefully by 26th Nov‚Äô22, I‚Äôd have improved from where I‚Äôm starting out.\nSo what do we have for Day 1?"
  },
  {
    "objectID": "posts/2022-10-07-day-1-of-50daysofkaggle/index.en.html#reading-the-data",
    "href": "posts/2022-10-07-day-1-of-50daysofkaggle/index.en.html#reading-the-data",
    "title": "day 1 of #50daysofkaggle",
    "section": "Reading the data",
    "text": "Reading the data\nFirst things first, import libraries\n\n\nCode\nimport requests\nimport numpy as np\nimport pandas as pd\nimport kaggle\n\nkaggle.api.authenticate()\n\n\nNote to self: below command did not work\n\n\nCode\n#kaggle.api.dataset_download_files(\"titanic\", path = \".\", unzip = True)\n\n\nHowever, this one does as per this link https://www.kaggle.com/general/138914\n\n\nCode\nkaggle.api.competition_download_files(\"titanic\", path = \".\")\n\n\nThis pulls the .zip file in the local folder. because this is a zip file, we need package called zipfile(note to self: don‚Äôt forget the console command reticulate::py_install(\"zipfile\"))\nhttps://stackoverflow.com/a/56786517/7938068\nReading and checking the first rows of train\n\n\nCode\nimport zipfile\n\nzf = zipfile.ZipFile(\"titanic.zip\")\ntrain = pd.read_csv(zf.open(\"train.csv\"))\ntest = pd.read_csv(zf.open(\"test.csv\"))\ntrain.head()\n\n\n\n\n\n\n  \n    \n      \n      PassengerId\n      Survived\n      Pclass\n      Name\n      Sex\n      Age\n      SibSp\n      Parch\n      Ticket\n      Fare\n      Cabin\n      Embarked\n    \n  \n  \n    \n      0\n      1\n      0\n      3\n      Braund, Mr. Owen Harris\n      male\n      22.0\n      1\n      0\n      A/5 21171\n      7.2500\n      NaN\n      S\n    \n    \n      1\n      2\n      1\n      1\n      Cumings, Mrs. John Bradley (Florence Briggs Th...\n      female\n      38.0\n      1\n      0\n      PC 17599\n      71.2833\n      C85\n      C\n    \n    \n      2\n      3\n      1\n      3\n      Heikkinen, Miss. Laina\n      female\n      26.0\n      0\n      0\n      STON/O2. 3101282\n      7.9250\n      NaN\n      S\n    \n    \n      3\n      4\n      1\n      1\n      Futrelle, Mrs. Jacques Heath (Lily May Peel)\n      female\n      35.0\n      1\n      0\n      113803\n      53.1000\n      C123\n      S\n    \n    \n      4\n      5\n      0\n      3\n      Allen, Mr. William Henry\n      male\n      35.0\n      0\n      0\n      373450\n      8.0500\n      NaN\n      S\n    \n  \n\n\n\n\nChecking the first rows of test\n\n\nCode\ntest.head()\n\n\n\n\n\n\n  \n    \n      \n      PassengerId\n      Pclass\n      Name\n      Sex\n      Age\n      SibSp\n      Parch\n      Ticket\n      Fare\n      Cabin\n      Embarked\n    \n  \n  \n    \n      0\n      892\n      3\n      Kelly, Mr. James\n      male\n      34.5\n      0\n      0\n      330911\n      7.8292\n      NaN\n      Q\n    \n    \n      1\n      893\n      3\n      Wilkes, Mrs. James (Ellen Needs)\n      female\n      47.0\n      1\n      0\n      363272\n      7.0000\n      NaN\n      S\n    \n    \n      2\n      894\n      2\n      Myles, Mr. Thomas Francis\n      male\n      62.0\n      0\n      0\n      240276\n      9.6875\n      NaN\n      Q\n    \n    \n      3\n      895\n      3\n      Wirz, Mr. Albert\n      male\n      27.0\n      0\n      0\n      315154\n      8.6625\n      NaN\n      S\n    \n    \n      4\n      896\n      3\n      Hirvonen, Mrs. Alexander (Helga E Lindqvist)\n      female\n      22.0\n      1\n      1\n      3101298\n      12.2875\n      NaN\n      S\n    \n  \n\n\n\n\nThis took me a whole day to figure out. End of Day1 ü§∑‚Äç‚ôÇÔ∏è"
  },
  {
    "objectID": "posts/2022-10-10-day-4-of-50daysofkaggle/index.en.html",
    "href": "posts/2022-10-10-day-4-of-50daysofkaggle/index.en.html",
    "title": "Day 4 of #50daysofkaggle",
    "section": "",
    "text": "Previously I managed to download the titanic zip file using the kaggle api and extract two datasets train and test .\n\n\n\n\nCode\nimport requests\nimport numpy as np\nimport pandas as pd\nimport kaggle \nimport zipfile \n\nkaggle.api.authenticate()\n\nkaggle.api.competition_download_files(\"titanic\", path = \".\")\n\nzf = zipfile.ZipFile(\"titanic.zip\")\ntrain = pd.read_csv(zf.open(\"train.csv\"))\ntest = pd.read_csv(zf.open(\"test.csv\"))\n\n\n\n\n\nLets see what we have here in the train data\n\n\nCode\ntrain.head()\n\n\n\n\n\n\n  \n    \n      \n      PassengerId\n      Survived\n      Pclass\n      Name\n      Sex\n      Age\n      SibSp\n      Parch\n      Ticket\n      Fare\n      Cabin\n      Embarked\n    \n  \n  \n    \n      0\n      1\n      0\n      3\n      Braund, Mr. Owen Harris\n      male\n      22.0\n      1\n      0\n      A/5 21171\n      7.2500\n      NaN\n      S\n    \n    \n      1\n      2\n      1\n      1\n      Cumings, Mrs. John Bradley (Florence Briggs Th...\n      female\n      38.0\n      1\n      0\n      PC 17599\n      71.2833\n      C85\n      C\n    \n    \n      2\n      3\n      1\n      3\n      Heikkinen, Miss. Laina\n      female\n      26.0\n      0\n      0\n      STON/O2. 3101282\n      7.9250\n      NaN\n      S\n    \n    \n      3\n      4\n      1\n      1\n      Futrelle, Mrs. Jacques Heath (Lily May Peel)\n      female\n      35.0\n      1\n      0\n      113803\n      53.1000\n      C123\n      S\n    \n    \n      4\n      5\n      0\n      3\n      Allen, Mr. William Henry\n      male\n      35.0\n      0\n      0\n      373450\n      8.0500\n      NaN\n      S\n    \n  \n\n\n\n\nChecking more details on train columns.\n\n\nCode\ntrain.info()\n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.7+ KB\n\n\nPassengerID is the unique identifier for each row while Survived is the column to be predicted. Finding only the numeric columns and dropping the above two (ref - this link)\n\n\nCode\nnum_col = train.select_dtypes(include=np.number).columns.tolist()\n\ndel num_col[0:2] #.remove() can remove only 1 item. so for more than 1, use for loop \nselect_col = num_col\n\n\nAmong the string columns, only Sex and Embarked are relevant for our analysis. Ref - selecting columns by intersection\n\n\nCode\nstr_col= [\"Sex\", \"Embarked\", \"Survived\"]\nstr_col\n\n\nselect_col.extend(str_col)\nselect_col\n\ntrain_eda= train[train.columns.intersection(select_col)]\n\ntrain_eda.info()\n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 891 entries, 0 to 890\nData columns (total 8 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Survived  891 non-null    int64  \n 1   Pclass    891 non-null    int64  \n 2   Sex       891 non-null    object \n 3   Age       714 non-null    float64\n 4   SibSp     891 non-null    int64  \n 5   Parch     891 non-null    int64  \n 6   Fare      891 non-null    float64\n 7   Embarked  889 non-null    object \ndtypes: float64(2), int64(4), object(2)\nmemory usage: 55.8+ KB\n\n\n\n\n\nSeems like the older folks were luckier than the younger ones\n\n\nCode\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.catplot(data = train_eda, x = \"Age\", y = \"Fare\", hue = \"Survived\")\n            \nplt.show()\n\n\n\n\n\nDistinction between Class 1 and Class 3 is clear - poorer folks in Class 3 were younger (mean being just under 30 years) than the richer folks in Class 1\n\n\nCode\nplt.clf()\nsns.boxplot(data = train_eda, y = \"Age\", x = \"Pclass\", hue = \"Survived\")\nplt.show()\n\n\n\n\n\nBelow graph shows us that among the survivors, there were a lot more women than men survived the disaster.\n\n\nCode\nplt.clf()\nplot = sns.FacetGrid(data = train_eda, col = \"Survived\", hue = \"Sex\", col_wrap = 2)\nplot.map(sns.scatterplot, \"Age\", \"Fare\")\nplot.axes[-1].legend()\nplt.show()\n\n\n<Figure size 672x480 with 0 Axes>\n\n\n\n\n\nWe continue to notice the clearer skew towards Class 1 (richer) compared to Class 3 (poorer)\n\n\nCode\nplt.clf()\nplot = sns.FacetGrid(data = train_eda, col = \"Survived\", hue = \"Pclass\", col_wrap = 2)\nplot.map(sns.scatterplot, \"Age\", \"Fare\")\nplot.axes[-1].legend()\nplt.show()\n\n\n<Figure size 672x480 with 0 Axes>\n\n\n\n\n\n\n\nCode\nplt.clf()\nplot = sns.FacetGrid(data = train_eda, col = \"Survived\", hue = \"SibSp\", col_wrap = 2)\nplot.map(sns.scatterplot, \"Age\", \"Fare\")\nplot.axes[-1].legend()\nplt.show()\n\n\n<Figure size 672x480 with 0 Axes>\n\n\n\n\n\n\n\n\n\n\nCode\ntrain_df = pd.get_dummies(train_eda, columns = [\"Sex\", \"Embarked\"])\n\ntrain_df.info()\n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 891 entries, 0 to 890\nData columns (total 11 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   Survived    891 non-null    int64  \n 1   Pclass      891 non-null    int64  \n 2   Age         714 non-null    float64\n 3   SibSp       891 non-null    int64  \n 4   Parch       891 non-null    int64  \n 5   Fare        891 non-null    float64\n 6   Sex_female  891 non-null    uint8  \n 7   Sex_male    891 non-null    uint8  \n 8   Embarked_C  891 non-null    uint8  \n 9   Embarked_Q  891 non-null    uint8  \n 10  Embarked_S  891 non-null    uint8  \ndtypes: float64(2), int64(4), uint8(5)\nmemory usage: 46.2 KB\n\n\nAnd day 2 comes to an endü§∑‚Äç‚ôÇÔ∏è"
  },
  {
    "objectID": "posts/2022-10-12-day-6-of-50daysofkaggle/index.en.html",
    "href": "posts/2022-10-12-day-6-of-50daysofkaggle/index.en.html",
    "title": "Day 6 of #50daysofkaggle",
    "section": "",
    "text": "Progress till date:\n\nDownload titanic dataset and assign to train & test\nRearranging the data\nEDA\n\nTo do today:\n\nwrite function to find share of survivors by each variable\nattempt to create model\n\n\n\nLoading the data using kaggle library and examining the top rows of relevant columns.\n\n\nCode\nimport requests\nimport numpy as np\nimport pandas as pd\nimport kaggle \nimport zipfile \n\nkaggle.api.authenticate()\n\nkaggle.api.competition_download_files(\"titanic\", path = \".\")\n\nzf = zipfile.ZipFile(\"titanic.zip\")\ntrain = pd.read_csv(zf.open(\"train.csv\"))\ntest = pd.read_csv(zf.open(\"test.csv\"))\n\n#Selecting only the numerical columns\nnum_col = train.select_dtypes(include=np.number).columns.tolist()\n\n#deslecting passenger ID and 'Survived' \ndel num_col[0:2] #.remove() can remove only 1 item. so for more than 1, use for loop \nselect_col = num_col\n\n#remaining columns\nstr_col= [\"Sex\", \"Embarked\", \"Survived\"]\n\n#Adding more elements into a list using `extend` and not `append`\nselect_col.extend(str_col)\n\ntrain_eda= train[train.columns.intersection(select_col)]\ntrain_eda.head()\n\n\n\n\n\n\n  \n    \n      \n      Survived\n      Pclass\n      Sex\n      Age\n      SibSp\n      Parch\n      Fare\n      Embarked\n    \n  \n  \n    \n      0\n      0\n      3\n      male\n      22.0\n      1\n      0\n      7.2500\n      S\n    \n    \n      1\n      1\n      1\n      female\n      38.0\n      1\n      0\n      71.2833\n      C\n    \n    \n      2\n      1\n      3\n      female\n      26.0\n      0\n      0\n      7.9250\n      S\n    \n    \n      3\n      1\n      1\n      female\n      35.0\n      1\n      0\n      53.1000\n      S\n    \n    \n      4\n      0\n      3\n      male\n      35.0\n      0\n      0\n      8.0500\n      S\n    \n  \n\n\n\n\nHow many columns with na values?\n\n\nCode\ntrain_eda.isna().sum().sort_values()\n\n\nSurvived      0\nPclass        0\nSex           0\nSibSp         0\nParch         0\nFare          0\nEmbarked      2\nAge         177\ndtype: int64\n\n\nAlmost 177 entries in the Age column have no value. Calculating the median age of remaining data.\n\n\nCode\ntrain_eda[\"Age\"].median() #28\n\n\n28.0\n\n\nReplacing these with the median age (28) instead of removing them.\n\n\nCode\ntrain_eda[\"Age\"].fillna(value = train_eda[\"Age\"].median(), inplace = True)\ntrain_eda.isna().sum().sort_values()\n\n\nC:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_5312\\1076914416.py:1: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\nSurvived    0\nPclass      0\nSex         0\nAge         0\nSibSp       0\nParch       0\nFare        0\nEmbarked    2\ndtype: int64\n\n\nToday I want to calculate the survival rate of each of these attributes (Pclass, Sex, Embarked).\n\n\nCode\ndf_copy2 = pd.DataFrame(columns = {\"category\", \"col\", \"survive_rate\"})\n\nfor t in [\"Pclass\", \"Sex\", \"Embarked\"]:\n  df_copy = train_eda.groupby([t])[\"Survived\"].mean().reset_index()\n  df_copy[\"category\"] = t\n  #trying to create a `tidy` version of the data \n  df_copy.rename(columns = {t: \"col\", \"Survived\": \"survive_rate\"}, errors = \"raise\", inplace = True)\n  df_copy = df_copy[[\"category\", \"col\", \"survive_rate\"]]\n  df_copy2= pd.concat([df_copy2, df_copy], ignore_index = True)\n\n\n#final table in a tidy format that can be used to create graphs. but that i'm keeping for later\ndf_copy2[[\"category\", \"col\", \"survive_rate\"]]\n\n\n\n\n\n\n  \n    \n      \n      category\n      col\n      survive_rate\n    \n  \n  \n    \n      0\n      Pclass\n      1\n      0.62963\n    \n    \n      1\n      Pclass\n      2\n      0.472826\n    \n    \n      2\n      Pclass\n      3\n      0.242363\n    \n    \n      3\n      Sex\n      female\n      0.742038\n    \n    \n      4\n      Sex\n      male\n      0.188908\n    \n    \n      5\n      Embarked\n      C\n      0.553571\n    \n    \n      6\n      Embarked\n      Q\n      0.38961\n    \n    \n      7\n      Embarked\n      S\n      0.336957\n    \n  \n\n\n\n\nWith this, its pretty clear that among the sex category, males had the least likelihood of surviving with 19%. The richer class 1 managed a 63% chance of survival while only 24% of the lower class 3 survived. Finally those that embarked from Cherbourg had a higher survival rate 55% compared to Southampton at 34%.\n\n\n\nSeperating the X & y. Here‚Äôs the first 5 rows of X\n\n\nCode\ntrain_eda.isna().sum().sort_values()\ntrain_eda = train_eda.dropna(axis = 0) #removing all rows with NA\n\nX = train_eda[[\"Age\", \"SibSp\", \"Parch\", \"Fare\"]]\n\nX = pd.concat([X,pd.get_dummies(data = train_eda[[\"Sex\", \"Embarked\", \"Pclass\"]], columns = [\"Sex\", \"Embarked\", \"Pclass\"])], axis = 1)\n\nX.head()\n\n\n\n\n\n\n  \n    \n      \n      Age\n      SibSp\n      Parch\n      Fare\n      Sex_female\n      Sex_male\n      Embarked_C\n      Embarked_Q\n      Embarked_S\n      Pclass_1\n      Pclass_2\n      Pclass_3\n    \n  \n  \n    \n      0\n      22.0\n      1\n      0\n      7.2500\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      1\n    \n    \n      1\n      38.0\n      1\n      0\n      71.2833\n      1\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n    \n    \n      2\n      26.0\n      0\n      0\n      7.9250\n      1\n      0\n      0\n      0\n      1\n      0\n      0\n      1\n    \n    \n      3\n      35.0\n      1\n      0\n      53.1000\n      1\n      0\n      0\n      0\n      1\n      1\n      0\n      0\n    \n    \n      4\n      35.0\n      0\n      0\n      8.0500\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      1\n    \n  \n\n\n\n\nFirst 5 rows of y\n\n\nCode\ny = train_eda[\"Survived\"].values\ny[0:5]\n\n\narray([0, 1, 1, 1, 0], dtype=int64)\n\n\nChecking dimensions of y & X\n\n\nCode\nlen(y) #889 after filling up the NA. previously 712\nX.shape #(889, 12)\n\n\n(889, 12)\n\n\n\n\nTransform X and printing the first 5 datapoints\n\n\nCode\nfrom sklearn import preprocessing\n\nX= preprocessing.StandardScaler().fit(X).transform(X)\nX[0:5]\n\n\narray([[-0.56367407,  0.43135024, -0.47432585, -0.50023975, -0.73534203,\n         0.73534203, -0.48271079, -0.30794088,  0.61679395, -0.56306042,\n        -0.51087465,  0.90032807],\n       [ 0.66921696,  0.43135024, -0.47432585,  0.78894661,  1.35991138,\n        -1.35991138,  2.07163382, -0.30794088, -1.62128697,  1.77600834,\n        -0.51087465, -1.11070624],\n       [-0.25545131, -0.47519908, -0.47432585, -0.48664993,  1.35991138,\n        -1.35991138, -0.48271079, -0.30794088,  0.61679395, -0.56306042,\n        -0.51087465,  0.90032807],\n       [ 0.43804989,  0.43135024, -0.47432585,  0.42286111,  1.35991138,\n        -1.35991138, -0.48271079, -0.30794088,  0.61679395,  1.77600834,\n        -0.51087465, -1.11070624],\n       [ 0.43804989, -0.47519908, -0.47432585, -0.4841333 , -0.73534203,\n         0.73534203, -0.48271079, -0.30794088,  0.61679395, -0.56306042,\n        -0.51087465,  0.90032807]])\n\n\n\n\n\nSplitting into test & train data and comparing the dimensions.\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\nprint ('Train set\\t :', X_train.shape,  y_train.shape,\n'\\nTest set\\t :', X_test.shape,  y_test.shape)\n\n\nTrain set    : (711, 12) (711,) \nTest set     : (178, 12) (178,)\n\n\n\n\n\nUsing KNN at k = 4\n\n\nCode\nfrom sklearn.neighbors import KNeighborsClassifier\nk = 4\nneighbours = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)\nneighbours\n\n\nKNeighborsClassifier(n_neighbors=4)\n\n\n\n\n\n\nCode\nyhat1 = neighbours.predict(X_test)\nyhat1[0:5]\n\n\narray([0, 1, 0, 0, 1], dtype=int64)\n\n\nCalculating the accuracy at k = 4\n\n\nCode\nfrom sklearn import metrics\n\nprint(\"Train set Accuracy \\t:\", metrics.accuracy_score(y_train, neighbours.predict(X_train)), \"\\nTest set Accuracy \\t:\", metrics.accuracy_score(y_test, yhat1))\n\n\nTrain set Accuracy  : 0.8509142053445851 \nTest set Accuracy   : 0.7584269662921348\n\n\n(without replacing na values, the previous test accuracy was 78%)\n\n\n\n\n\nCode\nfrom sklearn import metrics\n\nKs = 10\nmean_acc = np.zeros((Ks-1))\nstd_acc = np.zeros((Ks-1))\n\nfor n in range(1,Ks):\n    #Train Model and Predict  \n    neigh = KNeighborsClassifier(n_neighbors = n).fit(X_train,y_train)\n    yhat=neigh.predict(X_test)\n    mean_acc[n-1] = metrics.accuracy_score(y_test, yhat)\n    std_acc[n-1]=np.std(yhat==y_test)/np.sqrt(yhat.shape[0])\n\nmean_acc\n\n\narray([0.78651685, 0.76404494, 0.7752809 , 0.75842697, 0.78089888,\n       0.78651685, 0.80337079, 0.7752809 , 0.78089888])\n\n\nGlad that IBM coursera assignments came in handy! Now visualising the accuracy across each K\n\n\nCode\nimport matplotlib.pyplot as plt\n\nplt.plot(range(1,Ks),mean_acc,'g')\nplt.fill_between(range(1,Ks),mean_acc - 1 * std_acc,mean_acc + 1 * std_acc, alpha=0.10)\nplt.fill_between(range(1,Ks),mean_acc - 3 * std_acc,mean_acc + 3 * std_acc, alpha=0.10,color=\"green\")\nplt.legend(('Accuracy ', '+/- 1xstd','+/- 3xstd'))\nplt.ylabel('Accuracy ')\nplt.xlabel('Number of Neighbors (K)')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nLooks like accuracy of KNN is best at 7 neighbours. previously without replacing NA the accuracy was highest at k = 5\n\n\n\n\n\nCode\nk = 7\n\nneighbours_7 = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)\nyhat = neighbours_7.predict(X_test)\n\nprint(\"Train set Accuracy \\t:\", metrics.accuracy_score(y_train, neighbours_7.predict(X_train)),\"\\nTest set Accuracy \\t:\", metrics.accuracy_score(y_test, yhat),\"\\nRMSE \\t\\t\\t:\",metrics.mean_squared_error(y_test, yhat),\"\\nNormalised RMSE\\t\\t:\",metrics.mean_squared_error(y_test, yhat)/np.std(y_test))\n\n\nTrain set Accuracy  : 0.8509142053445851 \nTest set Accuracy   : 0.8033707865168539 \nRMSE            : 0.19662921348314608 \nNormalised RMSE     : 0.3997716243033934\n\n\nWe find that Test accuracy is around 80% for KNN1 with RMSE of 0.197 and Normalised RMSE of 40%2. formula for NRMSE here"
  },
  {
    "objectID": "posts/2022-10-14-day-8-of-kaggle/index.en.html",
    "href": "posts/2022-10-14-day-8-of-kaggle/index.en.html",
    "title": "Day 8 of #50daysofkaggle",
    "section": "",
    "text": "Progress till date:\n\nDownload titanic dataset and assign to train & test\nRearranging the data\nEDA (including plots and finding survival rate using .groupby())\nModelling\nData preparation - one-hot encoding the Sex, Pclass & Embarked columns - appending these to the numerical columns - normalising the data - splitting between train into X_train, y_train, X_test, y_test\nApplying KNN algo\n\nfinding the right K based on accuracy. (best at K = 7)\nCalculating the accuracy based on test\n\n\nTo do today: - Perform Decision Tree classification\n\n\nReading and printing the top 5 rows\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport zipfile\n\n\n#importing the zipfile already saved in the other folder. \nzf = zipfile.ZipFile(\"../2022-10-12-day-6-of-50daysofkaggle/titanic.zip\")\ntrain = pd.read_csv(zf.open(\"train.csv\"))\ntest = pd.read_csv(zf.open(\"test.csv\"))\n\n#Selecting only the numerical columns\nnum_col = train.select_dtypes(include=np.number).columns.tolist()\n\n#deslecting passenger ID and 'Survived' \ndel num_col[0:2] #.remove() can remove only 1 item. so for more than 1, use for loop \nselect_col = num_col\n\n#remaining columns\nstr_col= [\"Sex\", \"Embarked\", \"Survived\"]\n\n\n#Adding more elements into a list using `extend` and not `append`\nselect_col.extend(str_col)\n\ntrain_eda= train[train.columns.intersection(select_col)]\ntrain_eda.head()\n\n\n\n\n\n\n  \n    \n      \n      Survived\n      Pclass\n      Sex\n      Age\n      SibSp\n      Parch\n      Fare\n      Embarked\n    \n  \n  \n    \n      0\n      0\n      3\n      male\n      22.0\n      1\n      0\n      7.2500\n      S\n    \n    \n      1\n      1\n      1\n      female\n      38.0\n      1\n      0\n      71.2833\n      C\n    \n    \n      2\n      1\n      3\n      female\n      26.0\n      0\n      0\n      7.9250\n      S\n    \n    \n      3\n      1\n      1\n      female\n      35.0\n      1\n      0\n      53.1000\n      S\n    \n    \n      4\n      0\n      3\n      male\n      35.0\n      0\n      0\n      8.0500\n      S\n    \n  \n\n\n\n\n\n\n\nChecking all na values in the existing dataset\n\n\nCode\ntrain_eda.isna().sum().sort_values()\n\n\nSurvived      0\nPclass        0\nSex           0\nSibSp         0\nParch         0\nFare          0\nEmbarked      2\nAge         177\ndtype: int64\n\n\nFinding the mean\n\n\nCode\ntrain_eda[\"Age\"].median()\n\n\n28.0\n\n\nReplacing na cells with the mean\n\n\nCode\ntrain_eda[\"Age\"].fillna(value = train_eda[\"Age\"].median(), inplace = True)\ntrain_eda.isna().sum().sort_values()\n\n\nC:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_9908\\1076914416.py:1: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\nSurvived    0\nPclass      0\nSex         0\nAge         0\nSibSp       0\nParch       0\nFare        0\nEmbarked    2\ndtype: int64\n\n\nSidenote: Was getting a wierd warning (SettingWithCopyWarning) while using .fillna() to replace na with the median values. Turns out there‚Äôs a between calling a view or a copy. One way of avoiding this error is to use train_eda.loc[:,\"Age\"] instead of train_eda[\"Age\"]. This is because .loc returns the view (original) while using subsets. Elegant explanation here. Below code will not throw up a warning.\n\n\nCode\nxx = train_eda.copy()\nxx.loc[:,\"Age\"].fillna(value = xx.Age.median(), inplace = True)\nxx.isna().sum()\n\n\nSurvived    0\nPclass      0\nSex         0\nAge         0\nSibSp       0\nParch       0\nFare        0\nEmbarked    2\ndtype: int64\n\n\n\n\n\nSeperating X & y. Here‚Äôs the first 5 rows of X\n\n\nCode\ntrain_eda = train_eda.dropna(axis = 0) #removing all rows with NA\n\nX = train_eda[[\"Age\", \"SibSp\", \"Parch\", \"Fare\"]]\nX = pd.concat([X,pd.get_dummies(data = train_eda[[\"Sex\", \"Embarked\", \"Pclass\"]], columns = [\"Sex\", \"Embarked\", \"Pclass\"])], axis = 1)\n\nX.head()\n\n\n\n\n\n\n  \n    \n      \n      Age\n      SibSp\n      Parch\n      Fare\n      Sex_female\n      Sex_male\n      Embarked_C\n      Embarked_Q\n      Embarked_S\n      Pclass_1\n      Pclass_2\n      Pclass_3\n    \n  \n  \n    \n      0\n      22.0\n      1\n      0\n      7.2500\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      1\n    \n    \n      1\n      38.0\n      1\n      0\n      71.2833\n      1\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n    \n    \n      2\n      26.0\n      0\n      0\n      7.9250\n      1\n      0\n      0\n      0\n      1\n      0\n      0\n      1\n    \n    \n      3\n      35.0\n      1\n      0\n      53.1000\n      1\n      0\n      0\n      0\n      1\n      1\n      0\n      0\n    \n    \n      4\n      35.0\n      0\n      0\n      8.0500\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      1\n    \n  \n\n\n\n\nHere‚Äôs the first 5 rows of y\n\n\nCode\ny = train_eda[\"Survived\"].values\ny[0:5]\n\n\narray([0, 1, 1, 1, 0], dtype=int64)\n\n\ncomparing the shapes of X and y\n\n\nCode\nlen(y) #889 after filling up the NA. previously 712\nX.shape #(889, 12)\n\n\n(889, 12)\n\n\n\n\nStandardising and printing the first 5 datapoints.\n\n\nCode\nfrom sklearn import preprocessing\n\nX= preprocessing.StandardScaler().fit(X).transform(X)\nX[0:5]\n\n\narray([[-0.56367407,  0.43135024, -0.47432585, -0.50023975, -0.73534203,\n         0.73534203, -0.48271079, -0.30794088,  0.61679395, -0.56306042,\n        -0.51087465,  0.90032807],\n       [ 0.66921696,  0.43135024, -0.47432585,  0.78894661,  1.35991138,\n        -1.35991138,  2.07163382, -0.30794088, -1.62128697,  1.77600834,\n        -0.51087465, -1.11070624],\n       [-0.25545131, -0.47519908, -0.47432585, -0.48664993,  1.35991138,\n        -1.35991138, -0.48271079, -0.30794088,  0.61679395, -0.56306042,\n        -0.51087465,  0.90032807],\n       [ 0.43804989,  0.43135024, -0.47432585,  0.42286111,  1.35991138,\n        -1.35991138, -0.48271079, -0.30794088,  0.61679395,  1.77600834,\n        -0.51087465, -1.11070624],\n       [ 0.43804989, -0.47519908, -0.47432585, -0.4841333 , -0.73534203,\n         0.73534203, -0.48271079, -0.30794088,  0.61679395, -0.56306042,\n        -0.51087465,  0.90032807]])\n\n\n\n\n\nSplitting into test & train data and comparing the dimensions.\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\nprint ('Train set\\t :', X_train.shape,  y_train.shape,\n'\\nTest set\\t :', X_test.shape,  y_test.shape)\n\n\nTrain set    : (711, 12) (711,) \nTest set     : (178, 12) (178,)\n\n\n\n\n\nLets check the classification results using Decision trees. First 10 are as follows:\n\n\nCode\nfrom sklearn.tree import DecisionTreeClassifier\n\nDtree = DecisionTreeClassifier(criterion = \"entropy\", max_depth = 3)\nDtree.fit(X_train,y_train)\ny_test_hat = Dtree.predict(X_test)\nprint(\"First 10 actual\\t\\t:\", y_test[0:10],\"\\nFirst 10 predicted\\t:\", y_test_hat[0:10])\n\n\nFirst 10 actual     : [1 1 0 1 1 1 0 0 0 0] \nFirst 10 predicted  : [1 1 0 1 1 0 0 0 0 0]\n\n\n\n\n\nCalculating accuracy using Decision Tree classification for y_test\n\n\nCode\nfrom sklearn import metrics\n\nprint(\"Decision Tree Accuracy\\t:\", metrics.accuracy_score(y_test, y_test_hat),\"\\nRMSE\\t\\t\\t:\", metrics.mean_squared_error(y_test,y_test_hat),\"\\nNormalised RMSE\\t\\t:\", metrics.mean_squared_error(y_test,y_test_hat)/np.std(y_test))\n\n\nDecision Tree Accuracy  : 0.8314606741573034 \nRMSE            : 0.16853932584269662 \nNormalised RMSE     : 0.34266139226005143\n\n\nNot bad. We find that Test accuracy is around 83% for Decision Trees and RMSE of 0.168\n\n\n\nHere‚Äôs a neat little trick to see how the DT actually thinks.\n\n\nCode\nfrom sklearn import tree\nimport matplotlib.pyplot as plt\n\nplt.clf()\ntree.plot_tree(Dtree)\nplt.show()"
  },
  {
    "objectID": "posts/2022-10-16-day-10-of-50daysofkaggle/index.en.html",
    "href": "posts/2022-10-16-day-10-of-50daysofkaggle/index.en.html",
    "title": "Day 10 of #50daysofkaggle",
    "section": "",
    "text": "Part of an ongoing series to familiarise working on kaggle\nProgress till date:\n\nDownload titanic dataset and assign to train & test\nRearranging the data\nEDA (including plots and finding survival rate using .groupby())\nModelling\nData preparation - one-hot encoding the Sex, Pclass & Embarked columns - appending these to the numerical columns - normalising the data - splitting between train into X_train, y_train, X_test, y_test\nApplying KNN algo\n\nfinding the right K based on accuracy. (best at K = 7)\nCalculating the accuracy based on test\n\nApplying Decision Trees algo\n\nwith criterion = entropy and max_depth = 3\nsligthly better accuracy in prediction than KNN\n\n\nTo do today: - classification using Support Vector Machines algo\n\n\nReading and printing the top 5 rows\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport zipfile\n\n#importing the zipfile already saved in the other folder. \nzf = zipfile.ZipFile(\"../2022-10-12-day-6-of-50daysofkaggle/titanic.zip\")\ntrain = pd.read_csv(zf.open(\"train.csv\"))\ntest = pd.read_csv(zf.open(\"test.csv\"))\n\n#Selecting only the numerical columns\nnum_col = train.select_dtypes(include=np.number).columns.tolist()\n\n#deslecting passenger ID and 'Survived' \ndel num_col[0:2] #.remove() can remove only 1 item. so for more than 1, use for loop \nselect_col = num_col\n\n#remaining columns\nstr_col= [\"Sex\", \"Embarked\", \"Survived\"]\n#Adding more elements into a list using `extend` and not `append`\nselect_col.extend(str_col)\n\ntrain_eda= train[train.columns.intersection(select_col)]\ntrain_eda\n\n\n\n\n\n\n  \n    \n      \n      Survived\n      Pclass\n      Sex\n      Age\n      SibSp\n      Parch\n      Fare\n      Embarked\n    \n  \n  \n    \n      0\n      0\n      3\n      male\n      22.0\n      1\n      0\n      7.2500\n      S\n    \n    \n      1\n      1\n      1\n      female\n      38.0\n      1\n      0\n      71.2833\n      C\n    \n    \n      2\n      1\n      3\n      female\n      26.0\n      0\n      0\n      7.9250\n      S\n    \n    \n      3\n      1\n      1\n      female\n      35.0\n      1\n      0\n      53.1000\n      S\n    \n    \n      4\n      0\n      3\n      male\n      35.0\n      0\n      0\n      8.0500\n      S\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      886\n      0\n      2\n      male\n      27.0\n      0\n      0\n      13.0000\n      S\n    \n    \n      887\n      1\n      1\n      female\n      19.0\n      0\n      0\n      30.0000\n      S\n    \n    \n      888\n      0\n      3\n      female\n      NaN\n      1\n      2\n      23.4500\n      S\n    \n    \n      889\n      1\n      1\n      male\n      26.0\n      0\n      0\n      30.0000\n      C\n    \n    \n      890\n      0\n      3\n      male\n      32.0\n      0\n      0\n      7.7500\n      Q\n    \n  \n\n891 rows √ó 8 columns\n\n\n\n\n\n\nChecking all na values in the existing dataset.\n\n\nCode\ntrain_eda.isna().sum().sort_values()\n\n\nSurvived      0\nPclass        0\nSex           0\nSibSp         0\nParch         0\nFare          0\nEmbarked      2\nAge         177\ndtype: int64\n\n\nReplacing empty cells with median age (28)\n\n\nCode\nmedian_age = train_eda.Age.median() #28\ntrain_eda.loc[train_eda.Age.isna(), \"Age\"] = median_age #.loc returns the view and doesn't throw warning msg\ntrain_eda.isna().sum().sort_values()\n\n\nSurvived    0\nPclass      0\nSex         0\nAge         0\nSibSp       0\nParch       0\nFare        0\nEmbarked    2\ndtype: int64\n\n\n\n\n\nSeperating X & y\n\n\nCode\ntrain_eda = train_eda.dropna(axis = 0) #removing all rows with NA\n\nX = train_eda[[\"Age\", \"SibSp\", \"Parch\", \"Fare\"]]\nX = pd.concat([X,pd.get_dummies(data = train_eda[[\"Sex\", \"Embarked\", \"Pclass\"]], columns = [\"Sex\", \"Embarked\", \"Pclass\"])], axis = 1)\n\ny = train_eda[\"Survived\"].values\n\n\n\n\nTransform X and printing the first 5 datapoints\n\n\nCode\nfrom sklearn import preprocessing\n\nX= preprocessing.StandardScaler().fit(X).transform(X)\nX[0:5]\n\n\narray([[-0.56367407,  0.43135024, -0.47432585, -0.50023975, -0.73534203,\n         0.73534203, -0.48271079, -0.30794088,  0.61679395, -0.56306042,\n        -0.51087465,  0.90032807],\n       [ 0.66921696,  0.43135024, -0.47432585,  0.78894661,  1.35991138,\n        -1.35991138,  2.07163382, -0.30794088, -1.62128697,  1.77600834,\n        -0.51087465, -1.11070624],\n       [-0.25545131, -0.47519908, -0.47432585, -0.48664993,  1.35991138,\n        -1.35991138, -0.48271079, -0.30794088,  0.61679395, -0.56306042,\n        -0.51087465,  0.90032807],\n       [ 0.43804989,  0.43135024, -0.47432585,  0.42286111,  1.35991138,\n        -1.35991138, -0.48271079, -0.30794088,  0.61679395,  1.77600834,\n        -0.51087465, -1.11070624],\n       [ 0.43804989, -0.47519908, -0.47432585, -0.4841333 , -0.73534203,\n         0.73534203, -0.48271079, -0.30794088,  0.61679395, -0.56306042,\n        -0.51087465,  0.90032807]])\n\n\n\n\n\nSplitting into test & train data and comparing the dimensions.\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\nprint ('Train set\\t :', X_train.shape,  y_train.shape,\n'\\nTest set\\t :', X_test.shape,  y_test.shape)\n\n\nTrain set    : (711, 12) (711,) \nTest set     : (178, 12) (178,)\n\n\n\n\n\n\nLets check the classification results using SVM. First 10 are as follows:\n\n\nCode\nfrom sklearn import svm\nclf = svm.SVC(kernel='rbf')\nclf.fit(X_train, y_train) \n\nyhat_svm = clf.predict(X_test)\n\nprint(\"First 10 actual\\t\\t:\", y_test[0:10],\"\\nFirst 10 predicted\\t:\", yhat_svm[0:10])\n\n\nFirst 10 actual     : [1 1 0 1 1 1 0 0 0 0] \nFirst 10 predicted  : [0 1 0 1 1 0 0 0 0 0]\n\n\n\n\n\n\nCode\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport itertools\n\n# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_test, yhat_svm)\nnp.set_printoptions(precision=2)\n\nprint (classification_report(y_test, yhat_svm))\n\n\n              precision    recall  f1-score   support\n\n           0       0.78      0.95      0.85       105\n           1       0.90      0.60      0.72        73\n\n    accuracy                           0.81       178\n   macro avg       0.84      0.78      0.79       178\nweighted avg       0.83      0.81      0.80       178\n\n\n\n\n\n\n\n\nCode\nfrom sklearn import metrics\n\nprint(\"SVM Accuracy\\t:\", metrics.accuracy_score(y_test, yhat_svm),\"\\nRMSE\\t\\t\\t:\", metrics.mean_squared_error(y_test,yhat_svm),\"\\nNormalised RMSE\\t:\", metrics.mean_squared_error(y_test,yhat_svm)/np.std(y_test))\n\n\nSVM Accuracy    : 0.8089887640449438 \nRMSE            : 0.19101123595505617 \nNormalised RMSE : 0.38834957789472496\n\n\nAchieved 81% accuracy using SVM with RMSE of 0.1911. This is is not as good as Decision Trees which resulted in RMSE of 0.168\nTherefore after 10 days of struggle, I have come to the conclusion that Decision Trees is a good classification algorithm for the Titanic dataset."
  },
  {
    "objectID": "posts/2022-10-27-holiday-break-for-50daysofkaggle/index.en.html",
    "href": "posts/2022-10-27-holiday-break-for-50daysofkaggle/index.en.html",
    "title": "Holiday break for #50daysofkaggle",
    "section": "",
    "text": "Diwali is here\nIts festive season and I‚Äôve been really tied up with a bunch of things\n\n4 day long weekend where it was just about family and festivities. frankly one of the most rejuvenating and happy experiences to spend time with my daughter. She‚Äôs at the stage where she‚Äôs old enough to keep herself occupied alone. Its a pleasure to just watch her paint, draw, sing, role-play, read and whatever it is she loves doing.\nJob hunting has been taken up by a notch. Four major initiatives:\n\napplying more aggresively on applying to more companies in a day (spread across sites such as linkedin, iimjobs and naukri)\nOpening up the longlist to include SEA markets (Phillipines, Vietnam, Singapore, Indonesia)\nvolunteered to take up a course at FLAME starting in December. Its a 30 hour course that‚Äôs going to be quite gruelling to deliver. So will need to really start preparing as early as possible\nbuilding longlist of contacts for outreach. Downloaded LinkedIn contacts lists and sorted in order of priority\n\n\nCurrently pausing blog related updates to focus on picking up the job hunt search. Hopefully, post-Diwali there will be some action once people are back in offices!"
  },
  {
    "objectID": "about.html#why-the-null-hypothesis",
    "href": "about.html#why-the-null-hypothesis",
    "title": "About Me",
    "section": "Why the Null Hypothesis?",
    "text": "Why the Null Hypothesis?\nBritish Nobel-laureate Ronald Coase famously said ‚ÄúIf you torture the data long enough, it will confess to anything.‚Äù True words that ring loudly in any digital-first company. As someone who‚Äôs cut their teeth in quantitative research, I firmly it is consumer behaviour & insight that is sacred and we must seek it out with the help of data (big or small).\nIn statistics, the null hypothesis conjecture is the first step to begin any inferential analysis. Similarly, I begin every growth project with a null hypothesis to help me understand consumer behaviour better. In some ways this blog is an attempt at trying a growth hack on myself. I‚Äôve been passionately teaching myself new tools to make more meaningful impact towards business. I hope to document and share my learnings through this journal."
  },
  {
    "objectID": "about.html#skills",
    "href": "about.html#skills",
    "title": "About Me",
    "section": "Skills",
    "text": "Skills\n\n\n\n\n\n\n\n\n\n\nR Programming\n\nPython Programming\n\nStatistics & Machine Learning\n\n\n\nConsumer Research\n\nGrowth Marketing\n\nBusiness Management"
  },
  {
    "objectID": "about.html#qualifications",
    "href": "about.html#qualifications",
    "title": "About Me",
    "section": "Qualifications",
    "text": "Qualifications\n\nWorkEducationCertificationsTeaching\n\n\nMX Player (Feb ‚Äô20 - Oct ‚Äô22)\nDirector, Consumer Insights & Strategy\n\nHeading the Consumer Insights division and leading a team of data scientists/analysts to centralize all data requirements across departments (such as SVOD, Marketing, Content & Product)\nDoubled the platform share of acquired shows and drove launch strategy of 35+ acquired shows cross-functionally with Product, Programming and Marketing teams\nDevised 5x growth of International dubbed category MX VDESI; responsible for GTM strategies that led to growth in user retention & consumption\n\nViu India (Sep ‚Äô16 - Feb ‚Äô20)\nAssociate Director, Growth Marketing\n\nSpearheaded the platform growth with 6x increase in user base & 7x increase in app installs. Managing annual marketing budgets and building the OTT audience acquisition models\nDeveloped Marketing Measurement Model to optimize TV GRP-to-new installs. This modelling mix was increased marketing efficiencies by 15%\n20% reduction in marketing costs by geo-mapping user consumption density at zip-code level thereby building more focused media plan for Telugu markets\n\nSony Pictures Network India (Jun ‚Äô08 - Sep ‚Äô15)\nManagment Trainee to Sr.¬†Manager\n\nMultiple roles in the company with wide experience across Strategy, Research, Marketing, Revenue Planning and Content Acquisition\n\n\n\n Post Graduate Diploma in Communications, 2008\n\nMICA (Mudra Institute of Communications, Ahmedabad)\n\n B.E (Computer Science & Engineering), 2006\n\nVasavi College of Engineering (affiliated to Osmania University), Hyderabad\n\n\n\n\nIBM Data Science Professional Certificate, Sep‚Äô22\nMachine learning, Feb ‚Äô21 (Stanford University)\n\nExecutive Program in Strategic Digital Marketing, Dec‚Äô19 (University of Cambridge Judge Business School)\nProfessional Certificate Programme in Business Negotiation, Feb ‚Äô22 (SP Jain Institute of Management & Research)\n\n\n\nVisiting Faculty at Flame University, Pune"
  }
]