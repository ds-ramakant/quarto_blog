---
title: "Day 6 of #50daysofkaggle"
author: ''
date: "2022-10-12"
slug: "day-6-of-50daysofkaggle"
categories: [kaggle]
subtitle: 'Classification using KNN'
summary: 'Classification using KNN'
authors: [Me]
lastmod: "2022-10-12T15:38:17+05:30"
featured: no
image: titanic_KNN.jpg
projects: [50daysofkaggle]
format:
  html:
    code-fold: true
    code-block-border-left: true
    toc: true
execute: 
  warning: false
---

# Day 6: The Titanic Dataset

Progress till date:

-   Download titanic dataset and assign to `train` & `test`
-   Rearranging the data
-   EDA

To do today:

-   write function to find share of survivors by each variable
-   attempt to create model

## Reading the data

Loading the data using kaggle library and examining the top rows of relevant columns.

```{python}
import requests
import numpy as np
import pandas as pd
import kaggle 
import zipfile 

kaggle.api.authenticate()

kaggle.api.competition_download_files("titanic", path = ".")

zf = zipfile.ZipFile("titanic.zip")
train = pd.read_csv(zf.open("train.csv"))
test = pd.read_csv(zf.open("test.csv"))

#Selecting only the numerical columns
num_col = train.select_dtypes(include=np.number).columns.tolist()

#deslecting passenger ID and 'Survived' 
del num_col[0:2] #.remove() can remove only 1 item. so for more than 1, use for loop 
select_col = num_col

#remaining columns
str_col= ["Sex", "Embarked", "Survived"]

#Adding more elements into a list using `extend` and not `append`
select_col.extend(str_col)

train_eda= train[train.columns.intersection(select_col)]
train_eda.head()
```

How many columns with `na` values?

```{python}
train_eda.isna().sum().sort_values()
```

Almost 177 entries in the `Age` column have no value. Calculating the median age of remaining data.

```{python}
train_eda["Age"].median() #28
```

Replacing these with the median age (28) instead of removing them.

```{python}
train_eda["Age"].fillna(value = train_eda["Age"].median(), inplace = True)
train_eda.isna().sum().sort_values()
```

Today I want to calculate the survival rate of each of these attributes (`Pclass, Sex, Embarked`).

```{python}

df_copy2 = pd.DataFrame(columns = {"category", "col", "survive_rate"})

for t in ["Pclass", "Sex", "Embarked"]:
  df_copy = train_eda.groupby([t])["Survived"].mean().reset_index()
  df_copy["category"] = t
  #trying to create a `tidy` version of the data 
  df_copy.rename(columns = {t: "col", "Survived": "survive_rate"}, errors = "raise", inplace = True)
  df_copy = df_copy[["category", "col", "survive_rate"]]
  df_copy2= pd.concat([df_copy2, df_copy], ignore_index = True)


#final table in a tidy format that can be used to create graphs. but that i'm keeping for later
df_copy2[["category", "col", "survive_rate"]]
```

With this, its pretty clear that among the `sex` category, males had the least likelihood of surviving with 19%. The richer `class 1` managed a 63% chance of survival while only 24% of the lower `class 3` survived. Finally those that `embarked` from Cherbourg had a higher survival rate 55% compared to Southampton at 34%.

## Model building

Seperating the X & y. Here's the first 5 rows of `X`

```{python}

train_eda.isna().sum().sort_values()
train_eda = train_eda.dropna(axis = 0) #removing all rows with NA

X = train_eda[["Age", "SibSp", "Parch", "Fare"]]

X = pd.concat([X,pd.get_dummies(data = train_eda[["Sex", "Embarked", "Pclass"]], columns = ["Sex", "Embarked", "Pclass"])], axis = 1)

X.head()
```

First 5 rows of `y`

```{python}

y = train_eda["Survived"].values
y[0:5]
```

Checking dimensions of `y` & `X`

```{python}

len(y) #889 after filling up the NA. previously 712
X.shape #(889, 12)
```

### Normalising the data

Transform `X` and printing the first 5 datapoints

```{python}
from sklearn import preprocessing

X= preprocessing.StandardScaler().fit(X).transform(X)
X[0:5]
```

### Splitting into Test & Train data

Splitting into `test` & `train` data and comparing the dimensions.

```{python}

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)
print ('Train set\t :', X_train.shape,  y_train.shape,
'\nTest set\t :', X_test.shape,  y_test.shape)


```

### K Nearest Neighbours

Using KNN at k = 4

```{python}

from sklearn.neighbors import KNeighborsClassifier
k = 4
neighbours = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)
neighbours
```

#### Predicting the output `yhat` and checking accuracy

```{python}

yhat1 = neighbours.predict(X_test)
yhat1[0:5]
```

Calculating the accuracy at k = 4

```{python}
from sklearn import metrics

print("Train set Accuracy \t:", metrics.accuracy_score(y_train, neighbours.predict(X_train)), "\nTest set Accuracy \t:", metrics.accuracy_score(y_test, yhat1))

```

*(without replacing `na` values, the previous test accuracy was 78%)*

#### Checking for other K

```{python}
from sklearn import metrics

Ks = 10
mean_acc = np.zeros((Ks-1))
std_acc = np.zeros((Ks-1))

for n in range(1,Ks):
    #Train Model and Predict  
    neigh = KNeighborsClassifier(n_neighbors = n).fit(X_train,y_train)
    yhat=neigh.predict(X_test)
    mean_acc[n-1] = metrics.accuracy_score(y_test, yhat)
    std_acc[n-1]=np.std(yhat==y_test)/np.sqrt(yhat.shape[0])

mean_acc
```

Glad that IBM coursera assignments came in handy! Now visualising the accuracy across each `K`

```{python}
import matplotlib.pyplot as plt

plt.plot(range(1,Ks),mean_acc,'g')
plt.fill_between(range(1,Ks),mean_acc - 1 * std_acc,mean_acc + 1 * std_acc, alpha=0.10)
plt.fill_between(range(1,Ks),mean_acc - 3 * std_acc,mean_acc + 3 * std_acc, alpha=0.10,color="green")
plt.legend(('Accuracy ', '+/- 1xstd','+/- 3xstd'))
plt.ylabel('Accuracy ')
plt.xlabel('Number of Neighbors (K)')
plt.tight_layout()
plt.show()
```

Looks like accuracy of KNN is best at 7 neighbours. *previously without replacing NA the accuracy was highest at k = 5*

#### Redo with `K = 7`

```{python}
k = 7

neighbours_7 = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)
yhat = neighbours_7.predict(X_test)

print("Train set Accuracy \t:", metrics.accuracy_score(y_train, neighbours_7.predict(X_train)),"\nTest set Accuracy \t:", metrics.accuracy_score(y_test, yhat),"\nRMSE \t\t\t:",metrics.mean_squared_error(y_test, yhat),"\nNormalised RMSE\t\t:",metrics.mean_squared_error(y_test, yhat)/np.std(y_test))
```

We find that Test accuracy is around **80% for KNN**[^1] with **RMSE of 0.197** and **Normalised RMSE of 40%**[^2]. [formula for NRMSE here](https://www.marinedatascience.co/blog/2019/01/07/normalizing-the-rmse/)

[^1]: *pretty much the same as previous attempt before replacing NA*

[^2]: *actually NRMSE is not needed as all models are of the same scale. This is used typically for model comparisons across log, decimal etc scales*
