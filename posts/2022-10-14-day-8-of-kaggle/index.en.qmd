---
title: "Day 8 of #50daysofkaggle"
author: ''
date: '2022-10-14'
slug: day-8-of-kaggle
categories: [kaggle]
subtitle: 'Decision Tree'
summary: 'Classification through Decision Trees'
authors: [Me]
lastmod: '2022-10-14T09:32:49+05:30'
featured: no
image: titanic_DT.jpg
projects: [50daysofkaggle]

#removes warning messages
execute: 
  warning: true
#enabling folding code blocks, title of contents on upper-right & left border for code
format: 
  html: 
    code-fold: true
    code-block-border-left: true
    toc: true
---

# Day 8: Titanic Dataset

Progress till date:

-   Download titanic dataset and assign to `train` & `test`
-   Rearranging the data
-   EDA (including plots and finding survival rate using `.groupby()`)
-   Modelling
-   Data preparation - one-hot encoding the `Sex`, `Pclass` & `Embarked` columns - appending these to the numerical columns - normalising the data - splitting between `train` into `X_train`, `y_train`, `X_test`, `y_test`
-   Applying KNN algo
    -   finding the right K based on accuracy. (best at K = 7)
    -   Calculating the accuracy based on `test`

To do today: - Perform Decision Tree classification

## Loading the data

Reading and printing the top 5 rows

```{python}
import numpy as np
import pandas as pd
import zipfile


#importing the zipfile already saved in the other folder. 
zf = zipfile.ZipFile("../2022-10-12-day-6-of-50daysofkaggle/titanic.zip")
train = pd.read_csv(zf.open("train.csv"))
test = pd.read_csv(zf.open("test.csv"))

#Selecting only the numerical columns
num_col = train.select_dtypes(include=np.number).columns.tolist()

#deslecting passenger ID and 'Survived' 
del num_col[0:2] #.remove() can remove only 1 item. so for more than 1, use for loop 
select_col = num_col

#remaining columns
str_col= ["Sex", "Embarked", "Survived"]


#Adding more elements into a list using `extend` and not `append`
select_col.extend(str_col)

train_eda= train[train.columns.intersection(select_col)]
train_eda.head()
```

## Cleaning up the data

Checking all `na` values in the existing dataset

```{python}
train_eda.isna().sum().sort_values()
```

Finding the mean

```{python}
train_eda["Age"].median()
```

Replacing `na` cells with the mean

```{python}
train_eda["Age"].fillna(value = train_eda["Age"].median(), inplace = True)
train_eda.isna().sum().sort_values()
```

**Sidenote**: Was getting a wierd warning (`SettingWithCopyWarning`) while using `.fillna()` to replace na with the median values. Turns out there's a between calling a view or a copy. One way of avoiding this error is to use `train_eda.loc[:,"Age"]` instead of `train_eda["Age"]`. This is because `.loc` returns the view (original) while using subsets. [Elegant explanation here](https://stackoverflow.com/a/54914752/7938068 "Stockoverflow solution"). Below code will not throw up a warning.

```{python}
xx = train_eda.copy()
xx.loc[:,"Age"].fillna(value = xx.Age.median(), inplace = True)
xx.isna().sum()

```

## Model Building

Seperating X & y. Here's the first 5 rows of X

```{python}
train_eda = train_eda.dropna(axis = 0) #removing all rows with NA

X = train_eda[["Age", "SibSp", "Parch", "Fare"]]
X = pd.concat([X,pd.get_dummies(data = train_eda[["Sex", "Embarked", "Pclass"]], columns = ["Sex", "Embarked", "Pclass"])], axis = 1)

X.head()
```

Here's the first 5 rows of `y`

```{python}
y = train_eda["Survived"].values
y[0:5]

```

comparing the shapes of `X` and `y`

```{python}
len(y) #889 after filling up the NA. previously 712
X.shape #(889, 12)
```

### Normalising the data

Standardising and printing the first 5 datapoints.

```{python}
from sklearn import preprocessing

X= preprocessing.StandardScaler().fit(X).transform(X)
X[0:5]
```

### Splitting into Test & Train data

Splitting into `test` & `train` data and comparing the dimensions.

```{python}

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)
print ('Train set\t :', X_train.shape,  y_train.shape,
'\nTest set\t :', X_test.shape,  y_test.shape)


```

### Decision Trees

Lets check the classification results using Decision trees. First 10 are as follows:

```{python}
from sklearn.tree import DecisionTreeClassifier

Dtree = DecisionTreeClassifier(criterion = "entropy", max_depth = 3)
Dtree.fit(X_train,y_train)
y_test_hat = Dtree.predict(X_test)
print("First 10 actual\t\t:", y_test[0:10],"\nFirst 10 predicted\t:", y_test_hat[0:10])

```

### Checking Accuracy of DT

Calculating accuracy using Decision Tree classification for `y_test`

```{python}
from sklearn import metrics

print("Decision Tree Accuracy\t:", metrics.accuracy_score(y_test, y_test_hat),"\nRMSE\t\t\t:", metrics.mean_squared_error(y_test,y_test_hat),"\nNormalised RMSE\t\t:", metrics.mean_squared_error(y_test,y_test_hat)/np.std(y_test))
```

Not bad. We find that Test accuracy is around **83% for Decision Trees** and **RMSE of 0.168**

### Visualising the DT

Here's a neat little trick to see how the DT actually thinks.

```{python}
from sklearn import tree
import matplotlib.pyplot as plt

plt.clf()
tree.plot_tree(Dtree)
plt.show()
```
