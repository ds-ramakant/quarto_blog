---
title: 'Day 10 of #50daysofkaggle'
author: ''
date: "2022-10-16"
slug: "day-10-of-50daysofkaggle"
categories: kaggle
tags:
- 50daysofkaggle
- kaggle
- machinelearning
- python
subtitle: Support Vector Machines
summary: Classification through SVM
authors: []
lastmod: "2022-10-18T10:40:27+05:30"
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---



<div id="day-10-titanic-dataset" class="section level1">
<h1>Day 10: Titanic Dataset</h1>
<p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD
Part of an ongoing series to <a href="https://www.ds-ramakant.com/tag/50daysofkaggle/">familiarise working on kaggle</a>
=======
&gt;&gt;&gt;&gt;&gt;&gt;&gt; bb37a8424f98fd445e88c591fe7996f72dfa5d2f</p>
<p>Progress till date:</p>
<ul>
<li>Download titanic dataset and assign to <code>train</code> &amp; <code>test</code></li>
<li>Rearranging the data</li>
<li>EDA (including plots and finding survival rate using <code>.groupby()</code>)</li>
<li>Modelling</li>
<li>Data preparation
- one-hot encoding the <code>Sex</code>, <code>Pclass</code> &amp; <code>Embarked</code> columns
- appending these to the numerical columns
- normalising the data
- splitting between <code>train</code> into <code>X_train</code>, <code>y_train</code>, <code>X_test</code>, <code>y_test</code></li>
<li>Applying KNN algo
<ul>
<li>finding the right K based on accuracy. (best at K = 7)</li>
<li>Calculating the accuracy based on <code>test</code></li>
</ul></li>
<li>Applying Decision Trees algo
<ul>
<li>with <code>criterion = entropy</code> and <code>max_depth = 3</code></li>
<li>sligthly better accuracy in prediction than KNN</li>
</ul></li>
</ul>
<p>To do today:
- classification using Support Vector Machines algo</p>
<div id="reading-the-data" class="section level2">
<h2>Reading the data</h2>
<pre class="python"><code>import numpy as np
import pandas as pd
import zipfile

#importing the zipfile already saved in the other folder. 
zf = zipfile.ZipFile(&quot;../2022-10-12-day-6-of-50daysofkaggle/titanic.zip&quot;)
train = pd.read_csv(zf.open(&quot;train.csv&quot;))
test = pd.read_csv(zf.open(&quot;test.csv&quot;))

#Selecting only the numerical columns
num_col = train.select_dtypes(include=np.number).columns.tolist()

#deslecting passenger ID and &#39;Survived&#39; 
del num_col[0:2] #.remove() can remove only 1 item. so for more than 1, use for loop 
select_col = num_col

#remaining columns
str_col= [&quot;Sex&quot;, &quot;Embarked&quot;, &quot;Survived&quot;]
str_col

#Adding more elements into a list using `extend` and not `append`</code></pre>
<pre><code>## [&#39;Sex&#39;, &#39;Embarked&#39;, &#39;Survived&#39;]</code></pre>
<pre class="python"><code>select_col.extend(str_col)
select_col</code></pre>
<pre><code>## [&#39;Pclass&#39;, &#39;Age&#39;, &#39;SibSp&#39;, &#39;Parch&#39;, &#39;Fare&#39;, &#39;Sex&#39;, &#39;Embarked&#39;, &#39;Survived&#39;]</code></pre>
<pre class="python"><code>train_eda= train[train.columns.intersection(select_col)]
train_eda</code></pre>
<pre><code>##      Survived  Pclass     Sex   Age  SibSp  Parch     Fare Embarked
## 0           0       3    male  22.0      1      0   7.2500        S
## 1           1       1  female  38.0      1      0  71.2833        C
## 2           1       3  female  26.0      0      0   7.9250        S
## 3           1       1  female  35.0      1      0  53.1000        S
## 4           0       3    male  35.0      0      0   8.0500        S
## ..        ...     ...     ...   ...    ...    ...      ...      ...
## 886         0       2    male  27.0      0      0  13.0000        S
## 887         1       1  female  19.0      0      0  30.0000        S
## 888         0       3  female   NaN      1      2  23.4500        S
## 889         1       1    male  26.0      0      0  30.0000        C
## 890         0       3    male  32.0      0      0   7.7500        Q
## 
## [891 rows x 8 columns]</code></pre>
</div>
<div id="cleaning-up-the-data" class="section level2">
<h2>Cleaning up the data</h2>
<p><strong>Sidenote</strong>: Was getting a wierd warning (<code>SettingWithCopyWarning</code>) while using <code>.fillna()</code> to replace na with the median values. Turns out thereâ€™s a between calling a view or a copy. One way of avoiding this error is to use <code>train_eda.loc[:,"Age"]</code> instead of <code>train_eda["Age"]</code>. This is because <code>.loc</code> returns the view (original) while using subsets. <a href="https://stackoverflow.com/a/54914752/7938068" title="Stockoverflow solution">Elegant explanation here</a>. Below code will not throw up a warning.</p>
<pre class="python"><code>train_eda.isna().sum().sort_values()</code></pre>
<pre><code>## Survived      0
## Pclass        0
## Sex           0
## SibSp         0
## Parch         0
## Fare          0
## Embarked      2
## Age         177
## dtype: int64</code></pre>
<pre class="python"><code>median_age = train_eda.Age.median() #28
train_eda.loc[train_eda.Age.isna(), &quot;Age&quot;] = median_age #.loc returns the view and doesn&#39;t throw warning msg
train_eda.isna().sum().sort_values()</code></pre>
<pre><code>## Survived    0
## Pclass      0
## Sex         0
## Age         0
## SibSp       0
## Parch       0
## Fare        0
## Embarked    2
## dtype: int64</code></pre>
</div>
<div id="model-building" class="section level2">
<h2>Model Building</h2>
<p>Seperating X &amp; y</p>
<pre class="python"><code>train_eda = train_eda.dropna(axis = 0) #removing all rows with NA

X = train_eda[[&quot;Age&quot;, &quot;SibSp&quot;, &quot;Parch&quot;, &quot;Fare&quot;]]
X</code></pre>
<pre><code>##       Age  SibSp  Parch     Fare
## 0    22.0      1      0   7.2500
## 1    38.0      1      0  71.2833
## 2    26.0      0      0   7.9250
## 3    35.0      1      0  53.1000
## 4    35.0      0      0   8.0500
## ..    ...    ...    ...      ...
## 886  27.0      0      0  13.0000
## 887  19.0      0      0  30.0000
## 888  28.0      1      2  23.4500
## 889  26.0      0      0  30.0000
## 890  32.0      0      0   7.7500
## 
## [889 rows x 4 columns]</code></pre>
<pre class="python"><code>X = pd.concat([X,pd.get_dummies(data = train_eda[[&quot;Sex&quot;, &quot;Embarked&quot;, &quot;Pclass&quot;]], columns = [&quot;Sex&quot;, &quot;Embarked&quot;, &quot;Pclass&quot;])], axis = 1)

X.head()</code></pre>
<pre><code>##     Age  SibSp  Parch     Fare  ...  Embarked_S  Pclass_1  Pclass_2  Pclass_3
## 0  22.0      1      0   7.2500  ...           1         0         0         1
## 1  38.0      1      0  71.2833  ...           0         1         0         0
## 2  26.0      0      0   7.9250  ...           1         0         0         1
## 3  35.0      1      0  53.1000  ...           1         1         0         0
## 4  35.0      0      0   8.0500  ...           1         0         0         1
## 
## [5 rows x 12 columns]</code></pre>
<pre class="python"><code>y = train_eda[&quot;Survived&quot;].values
y[0:5]</code></pre>
<pre><code>## array([0, 1, 1, 1, 0], dtype=int64)</code></pre>
<pre class="python"><code>len(y) #889 after filling up the NA. previously 712</code></pre>
<pre><code>## 889</code></pre>
<pre class="python"><code>X.shape #(889, 12)</code></pre>
<pre><code>## (889, 12)</code></pre>
<div id="normalising-the-data" class="section level3">
<h3>Normalising the data</h3>
<pre class="python"><code>from sklearn import preprocessing

X= preprocessing.StandardScaler().fit(X).transform(X)
X[0:5]</code></pre>
<pre><code>## array([[-0.56367407,  0.43135024, -0.47432585, -0.50023975, -0.73534203,
##          0.73534203, -0.48271079, -0.30794088,  0.61679395, -0.56306042,
##         -0.51087465,  0.90032807],
##        [ 0.66921696,  0.43135024, -0.47432585,  0.78894661,  1.35991138,
##         -1.35991138,  2.07163382, -0.30794088, -1.62128697,  1.77600834,
##         -0.51087465, -1.11070624],
##        [-0.25545131, -0.47519908, -0.47432585, -0.48664993,  1.35991138,
##         -1.35991138, -0.48271079, -0.30794088,  0.61679395, -0.56306042,
##         -0.51087465,  0.90032807],
##        [ 0.43804989,  0.43135024, -0.47432585,  0.42286111,  1.35991138,
##         -1.35991138, -0.48271079, -0.30794088,  0.61679395,  1.77600834,
##         -0.51087465, -1.11070624],
##        [ 0.43804989, -0.47519908, -0.47432585, -0.4841333 , -0.73534203,
##          0.73534203, -0.48271079, -0.30794088,  0.61679395, -0.56306042,
##         -0.51087465,  0.90032807]])</code></pre>
</div>
<div id="splitting-into-test-train-data" class="section level3">
<h3>Splitting into Test &amp; Train data</h3>
<pre class="python"><code>
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)
print (&#39;Train set\t :&#39;, X_train.shape,  y_train.shape,
&#39;\nTest set\t :&#39;, X_test.shape,  y_test.shape)
</code></pre>
<pre><code>## Train set     : (711, 12) (711,) 
## Test set  : (178, 12) (178,)</code></pre>
</div>
</div>
<div id="support-vector-machines" class="section level2">
<h2>Support Vector Machines</h2>
<pre class="python"><code>from sklearn import svm
clf = svm.SVC(kernel=&#39;rbf&#39;)
clf.fit(X_train, y_train) </code></pre>
<pre><code>## SVC()</code></pre>
<pre class="python"><code>yhat_svm = clf.predict(X_test)

print(&quot;First 10 actual\t\t:&quot;, y_test[0:10],&quot;\nFirst 10 predicted\t:&quot;, yhat_svm[0:10])</code></pre>
<pre><code>## First 10 actual      : [1 1 0 1 1 1 0 0 0 0] 
## First 10 predicted   : [0 1 0 1 1 0 0 0 0 0]</code></pre>
<div id="confusion-matrix-using-svm" class="section level3">
<h3>Confusion matrix using SVM</h3>
<pre class="python"><code>from sklearn.metrics import classification_report, confusion_matrix
import itertools

# Compute confusion matrix
cnf_matrix = confusion_matrix(y_test, yhat_svm)
np.set_printoptions(precision=2)

print (classification_report(y_test, yhat_svm))</code></pre>
<pre><code>##               precision    recall  f1-score   support
## 
##            0       0.78      0.95      0.85       105
##            1       0.90      0.60      0.72        73
## 
##     accuracy                           0.81       178
##    macro avg       0.84      0.78      0.79       178
## weighted avg       0.83      0.81      0.80       178</code></pre>
<pre class="python"><code>from sklearn import metrics

print(&quot;SVM Accuracy\t:&quot;, metrics.accuracy_score(y_test, yhat_svm),&quot;\nRMSE\t\t:&quot;, metrics.mean_squared_error(y_test,yhat_svm),&quot;\nNormalised RMSE\t:&quot;, metrics.mean_squared_error(y_test,yhat_svm)/np.std(y_test))</code></pre>
<pre><code>## SVM Accuracy : 0.8089887640449438 
## RMSE     : 0.19101123595505617 
## Normalised RMSE  : 0.38834957789472496</code></pre>
<p>Achieved <strong>81%</strong> accuracy using SVM with <strong>RMSE of 0.1911</strong>. This is is not as good as Decision Trees which resulted in <a href="https://www.ds-ramakant.com/post/day-8-of-kaggle/">RMSE of 0.168</a></p>
<p>Therefore after 10 days of struggle, I have come to the conclusion that Decision Trees is a good classification algorithm for the Titanic dataset.</p>
</div>
</div>
</div>
