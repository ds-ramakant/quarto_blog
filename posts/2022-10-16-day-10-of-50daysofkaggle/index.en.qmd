---
title: 'Day 10 of #50daysofkaggle'
author: ''
date: "2022-10-16"
slug: "day-10-of-50daysofkaggle"
categories: kaggle
tags:
- 50daysofkaggle
- kaggle
- machinelearning
- python
subtitle: Support Vector Machines
summary: Classification through SVM
authors: []
lastmod: "2022-10-18T10:40:27+05:30"
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---
# Day 10: Titanic Dataset


Part of an ongoing series to [familiarise working on kaggle](https://www.ds-ramakant.com/tag/50daysofkaggle/)

Progress till date:

-   Download titanic dataset and assign to `train` & `test`
-   Rearranging the data
-   EDA (including plots and finding survival rate using `.groupby()`)
-   Modelling
  -  Data preparation
    -  one-hot encoding the `Sex`, `Pclass` & `Embarked` columns
    -  appending these to the numerical columns
    -  normalising the data
    -  splitting between `train` into `X_train`, `y_train`, `X_test`, `y_test`
  - Applying KNN algo
    -  finding the right K based on accuracy. (best at K = 7)
    -  Calculating the accuracy based on `test`
  - Applying Decision Trees algo
    -  with `criterion = entropy` and `max_depth = 3` 
    -  sligthly better accuracy in prediction than KNN
    
To do today:
- classification using Support Vector Machines algo

## Reading the data
```{python}
import numpy as np
import pandas as pd
import zipfile

#importing the zipfile already saved in the other folder. 
zf = zipfile.ZipFile("../2022-10-12-day-6-of-50daysofkaggle/titanic.zip")
train = pd.read_csv(zf.open("train.csv"))
test = pd.read_csv(zf.open("test.csv"))

#Selecting only the numerical columns
num_col = train.select_dtypes(include=np.number).columns.tolist()

#deslecting passenger ID and 'Survived' 
del num_col[0:2] #.remove() can remove only 1 item. so for more than 1, use for loop 
select_col = num_col

#remaining columns
str_col= ["Sex", "Embarked", "Survived"]
str_col

#Adding more elements into a list using `extend` and not `append`
select_col.extend(str_col)
select_col

train_eda= train[train.columns.intersection(select_col)]
train_eda
```


  
## Cleaning up the data

**Sidenote**: Was getting a wierd warning (`SettingWithCopyWarning`) while using `.fillna()` to replace na with the median values. Turns out there's a between calling a view or a copy. One way of avoiding this error is to use `train_eda.loc[:,"Age"]` instead of `train_eda["Age"]`. This is because `.loc` returns the view (original) while using subsets. [Elegant explanation here](https://stackoverflow.com/a/54914752/7938068 "Stockoverflow solution"). Below code will not throw up a warning. 
```{python}
train_eda.isna().sum().sort_values()
median_age = train_eda.Age.median() #28
train_eda.loc[train_eda.Age.isna(), "Age"] = median_age #.loc returns the view and doesn't throw warning msg
train_eda.isna().sum().sort_values()
```

## Model Building
Seperating X & y
```{python}
train_eda = train_eda.dropna(axis = 0) #removing all rows with NA

X = train_eda[["Age", "SibSp", "Parch", "Fare"]]
X

X = pd.concat([X,pd.get_dummies(data = train_eda[["Sex", "Embarked", "Pclass"]], columns = ["Sex", "Embarked", "Pclass"])], axis = 1)

X.head()

y = train_eda["Survived"].values
y[0:5]

len(y) #889 after filling up the NA. previously 712
X.shape #(889, 12)

```
### Normalising the data

```{python}
from sklearn import preprocessing

X= preprocessing.StandardScaler().fit(X).transform(X)
X[0:5]
```

### Splitting into Test & Train data

```{python}

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)
print ('Train set\t :', X_train.shape,  y_train.shape,
'\nTest set\t :', X_test.shape,  y_test.shape)


```

## Support Vector Machines

```{python}
from sklearn import svm
clf = svm.SVC(kernel='rbf')
clf.fit(X_train, y_train) 

yhat_svm = clf.predict(X_test)

print("First 10 actual\t\t:", y_test[0:10],"\nFirst 10 predicted\t:", yhat_svm[0:10])

```
### Confusion matrix using SVM

```{python}
from sklearn.metrics import classification_report, confusion_matrix
import itertools

# Compute confusion matrix
cnf_matrix = confusion_matrix(y_test, yhat_svm)
np.set_printoptions(precision=2)

print (classification_report(y_test, yhat_svm))
```


```{python}
from sklearn import metrics

print("SVM Accuracy\t:", metrics.accuracy_score(y_test, yhat_svm),"\nRMSE\t\t:", metrics.mean_squared_error(y_test,yhat_svm),"\nNormalised RMSE\t:", metrics.mean_squared_error(y_test,yhat_svm)/np.std(y_test))
```
Achieved **81%** accuracy using SVM with **RMSE of 0.1911**. This is is not as good as Decision Trees which resulted in [RMSE of 0.168](https://www.ds-ramakant.com/post/day-8-of-kaggle/)

Therefore after 10 days of struggle, I have come to the conclusion that Decision Trees is a good classification algorithm for the Titanic dataset. 

```{python junk code, eval=FALSE, include=FALSE}
train_eda.loc[:,"Age"]
median_age

train_eda.loc[train_eda.Age.isna(), "Age"] = median_age

```

