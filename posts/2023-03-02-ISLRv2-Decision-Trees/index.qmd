---
title: "ISLR Lab - Decision Trees"
subtitle: "Back to basics"
description: "Theoretical revision"
date: '2023-03-02'
categories: [R, ISLR]
featured: no
execute:
  warning: false
image: dtree.jpg
---

```{r}
library(tree)
library(ISLR2)
```

In today's lab , we will be using the `Carseats` dataset from the `ISLR2` package.

```{r}
str(Carseats)
```

Creating a column called `High` which takes a Y/N value depending on the sales and then merge it with the `Carseats` df.

```{r}
attach(Carseats)
High <- factor(ifelse(Sales <= 8, "No", "Yes"))
Carseats <- data.frame(Carseats, High)
```

# Fitting Classification Trees on `Carseats`

Creating a classification tree to predict `High` using all variables except `Sales`

```{r}

set.seed(1)
tree.carseats <- tree(High ~ .-Sales, data = Carseats)
summary(tree.carseats)
```

Misclassification error of 9% is a good fit. Let's try plotting it

```{r}
plot(tree.carseats)
text(tree.carseats, pretty  =0 )
```

## Splitting and fitting the model

```{r}
set.seed(2)

train <- sample(1:nrow(Carseats), 200)
Carseats.test <- Carseats[-train,]

High.test <- High[-train]

tree.carseats <- tree(High ~ .-Sales, data = Carseats, 
                      subset = train)
```

checking the top few rows of predicted columns

```{r}
tree.predict <- predict(tree.carseats, Carseats.test, 
                        type = "class") #type is needed to declare classification model
head(tree.predict)
```

Comparing predicted with actual values

```{r}
table(tree.predict, High.test)
```

What's the accuracy?

```{r}
(102+52)/200
```

77%

## Pruning the tree for improved classification

To improve the accuracy, lets attempt to prune the tree. For this `cv.tree()` function is used to determine the optimal level of tree complexity. Here the `FUN` argument is taken as `prune.misclass` to indicate that the cross-validation and tree pruning should be guided by the **classification error** instead of the default **deviance.**

```{r}
set.seed(7)
cv.carseats <- cv.tree(tree.carseats, FUN = prune.misclass)
names(cv.carseats)
```

Note to self:

-   `k` is the regularisation parameter $\alpha$ (alpha)
-   `size` is \# of terminal nodes for each tree
-   `dev` is the number of cross-validation errors

```{r}
cv.carseats
```

Visualising the tree. The classification error is least at `size  = 9`

```{r}
par(mfrow = c(1,2))
plot(cv.carseats$size, cv.carseats$dev, type = "b")
plot(cv.carseats$k, cv.carseats$dev, type = "b")
```

Using the `prune.misclass()` function to prune the tree to the 9-node specification.

```{r}
prune.carseats= prune.misclass(tree.carseats, best = 9)
plot(prune.carseats)
text(prune.carseats, pretty = 0)
```

Checking the accuracy in the good-old fashioned way

```{r}
prune.tree.pred <- predict(prune.carseats, Carseats.test, type = "class")
table(prune.tree.pred, High.test)
```

So what's the accuracy?

```{r}
(97+58)/200
```

**77.5%** which is slightly better than the non-pruned tree.

# Fitting regression trees on `Boston` dataset

`Boston` dataset contains housing values of 506 suburbs of Boston. We are trying to predict the median value of the owner-occupied homes `medv`

```{r}
str(Boston)
```

Creating the training set for `Boston` which is half the size of the original

```{r}
set.seed(1)
train <- sample(1:nrow(Boston), nrow(Boston)/2)
```

Building the tree

```{r}
tree.boston <- tree(medv ~ ., data = Boston, subset = train)
summary(tree.boston)
```

only 4 predictors were used. Plotting the decision tree

```{r}
plot(tree.boston)
text(tree.boston, pretty = 0)
```

## Making the predictions

```{r}

yhat <- predict(tree.boston, newdata = Boston[-train,])
boston.test <- Boston[-train,"medv"]

plot(yhat, boston.test)
abline(0,1, col = "red")

```

Mean Square Error is defined as :

```{r}
mean((yhat - boston.test)^2)
```

Standard Deviation the square root of MSE which is

```{r}
(mean((yhat - boston.test)^2))^0.5
```
