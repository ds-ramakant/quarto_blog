---
title: "Day 14 of #50daysofKaggle"
subtitle: "Roadmap to Tidymodels - Part 2"
description: "Screening multiple models using workflows"
date: '2023-03-20'
categories: [R, kaggle]
featured: no
execute:
  warning: false
image: Rplot.png
---

How hard is it to evaluate multiple models when working on a given data problem?

If you're using the `tidymodels` package, the answer is surprisingly simple.

Today's post is an attempt to use the `tidymodels` framework to screen multiple models. Inspiration for this post comes from the [Ch 15 of the Tidymodels with R textbook](https://www.tmwr.org/workflow-sets.html) along with two more noteworthy blogs

-   [Olivier Gimenez's post](https://oliviergimenez.github.io/blog/learning-machine-learning/) that had me marvelling at the feature engineering
-   [Tural Sadigov's post](https://rpubs.com/tsadigov/titanic_tidymodels) on handling multiple models

I'm skipping the EDA component as I've covered it in the previous posts. Moving on to some boring (but very necessary) sections.

As always, please feel free to leave a comment using the side-barüëâüèº

# Loading the data

`test` and `train` data is inputted and merged into a single dataframe

```{r}
library(tidyverse)
library(tidymodels)


titanic_train <- read.csv("train.csv", header = T)

#adding column to identify source
titanic_train <- titanic_train %>% 
  mutate(source = "train")

titanic_test <- read.csv("test.csv", header = T)
#adding column to identify source
titanic_test <- titanic_test %>% 
  mutate(Survived = NA,
         source = "test")

#merging the data. step1 starts here
titanic_data <- bind_rows(titanic_train, titanic_test)

glimpse(titanic_data)
```

## Cleaning data

Checking for `NA` values in the full df tells us that Age column has 86 missing in `test` & 177 missing values in `train` while there's 1 missing NA value in Fare. The 418 missing values in Survived in `test` are the ones we need to predict.

```{r}
titanic_data %>% 
  group_by(source) %>% 
  summarise_all(~ sum(is.na(.)))
```

### Janitor::clean_names()

Now i've got a thing about keeping column names clean so summoning `janitor` with a magic wand:

```{r}
titanic_data <- titanic_data %>% 
  mutate(family_count = SibSp+Parch+1) %>% 
  janitor::clean_names()
names(titanic_data)
```

Voila. Everything now in lower case and snake case!

### Imputing `NA` values in `embarked`

Now under **embarked** there are two rows that don't have `NA` but are blank. That's a bit oddü§®

```{r}
titanic_data %>% 
  count(embarked, sort = T)
```

Since it is only 2 such rows, I'll be replacing the `NA` values with the most repeated **embarked** value. So now zero empty values in **embarked**

```{r}
mode_embarked <- titanic_data %>% 
  count(embarked, sort = T) %>% 
  select(embarked) %>% 
  head(1) %>% 
#this beautiful func comes from the purrr package. equivalent of .[[1]]
  pluck(1)

titanic_data <- titanic_data %>% 
  mutate(embarked = if_else(embarked == "", mode_embarked, embarked))

titanic_data %>% 
  count(embarked, sort = T)
```

### Imputing `NA` values in `age`

The **age** column has a bunch of missing values. My approach today is to substitute them with the median values when grouped by **sex** and **class**. Here's a function written to impute within the test and train data accordingly.

*note: Feature engineering functions can be addressed in the `tidymodels`* *framework at recipe stage.*

```{r}
median_age_calc <- function(df){
  median_ages <- df %>% 
    group_by(sex, pclass) %>% 
    summarise(age = median(age,na.rm = T))
  df %>%
    mutate(age = case_when((sex =="male" & pclass ==1 & is.na(age)) ~ median_ages$age[1],
                           (sex =="male" & pclass ==2 & is.na(age)) ~ median_ages$age[2],
                           (sex =="male" & pclass ==3 & is.na(age)) ~ median_ages$age[3],
                           (sex =="female" & pclass ==1 & is.na(age)) ~ median_ages$age[4],
                           (sex =="female" & pclass ==2 & is.na(age)) ~ median_ages$age[5],
                           (sex =="female" & pclass ==3 & is.na(age)) ~ median_ages$age[6],
                           .default = age)
    )
}

titanic_data <- titanic_data %>% 
  median_age_calc() %>% ungroup()

```

Are there any `na` values in the titanic data now?

```{r}
titanic_data %>% 
  select(source, survived, sex, pclass, fare, age) %>% 
  group_by(source) %>% 
  summarise_all(~ sum(is.na(.)))
```

Phew. So **age** is covered but one `NA` in **fare**. What are the median age values now?

```{r}
titanic_data %>% 
  group_by(pclass, sex) %>%
  summarise(median_age = median(age))
```

### Imputing `NA` values for `fare`

There's this one person (passenger_id = 1044, male and pclass = 3) who has a `na` value in **fare** from `test` data. Replacing it with the median value.

```{r}
titanic_data %>% 
  select(source, name, passenger_id, sex, age, pclass, fare) %>% 
  filter(is.na(fare))
```

```{r}
titanic_data %>% 
  group_by(sex, pclass) %>% 
  summarise(median_fare = median(fare, na.rm = T))
```

Replacing in the main df

```{r}
titanic_data <- titanic_data %>% 
  mutate(fare = if_else(is.na(fare), 
#from the above table. Urgh!! hard coding for that 1 guy!! how inelegant.
                        7.8958, age))
```

checking if has been replaced.

```{r}
titanic_data %>% 
  select(source, name, passenger_id, sex, age, pclass, fare) %>% 
  filter(is.na(fare))
```

# Feature Engineering

For this post, I've focussed on a different approach while predicting survival probabilities. The guiding principle here is to club passengers from the same family and/or the same ticket_ID. The `titanic` dataset poses the challenge of having people within the same family purchase different tickets while at the same time so **surnames** really are not always the best grouping feature.

One such example is the Cacic family:

```{r}
titanic_data %>% 
  select(name, sex,age, ticket, survived) %>% 
  filter(str_detect(name, "Cacic"))
```

How about only clubbing with only **surname**? Well, that also gets tricky because sometimes we have instances of people within the same **ticket** sharing multiple **surnames.**

Case in point is ticket 1601:

```{r}
titanic_data %>% 
  select(name, sex, age, ticket, survived) %>% 
  filter(ticket==1601)
```

In times of crisis, it is expected that groups that traveled together will look out for their own. While this is not a thumb rule, I wanted to create a custom column that will combine the **ticket** and **surnames**.

So first step is to finding the surnames. That means using regex. Lovely ü§¢

## Splitting names

With absolutely no guilt, I confess that this took me almost an entire day to figure out. And am I glad to have done it. The best thing about the tidyverse approach is the onus on making readable data. For that I'm grateful to discover functions like `seperate_wider_regex`.

Essentially, it is a delimiter that breaks up columns based on the string patterns. So neat!

```{r}
names_with_splchar <- regex("[A-Za-z]+[\\'\\-\\s]+[A-Za-z]+")
names_with_3words <- regex("[A-Za-z]+\\s[A-Za-z]+\\s[A-Za-z]+")
names_with_1word <- regex("[A-Za-z]+") 
names_with_2words <- regex("[A-Za-z]+\\s+[A-Za-z]+") # for 'the countess'


titanic_data <- titanic_data %>% 
  separate_wider_regex(
    name, 
    patterns = c(
#IMP: ordering of regex patterns changes the outcome
      surname = str_c(c(names_with_splchar, 
                        names_with_3words,
                        names_with_1word), 
                      collapse = "|"),    # picks the first word before comma
      ", ",                               # the comma  
#IMP: ordering of regex patterns changes the outcome
      title = str_c(c(names_with_2words , # two words with special char in between like 'the countess'
                      names_with_1word),  # one word such as Mr Miss Mrs etc
                    collapse = "|"),      
      ". ",                               # the dot
      given_name = ".+"),                 # picks anything else which occurs at least once
    cols_remove = F                       # retains the original column    
  ) 

titanic_data %>% 
  select(name, title, surname, given_name) %>% 
  head(10)
```

What is the break-up of titles now?

```{r}
titanic_data %>% 
  count(title, sort= T)
```

## Creating a custom grouping

The **ticket** is going to be broken up into **ticket_tail** (the last character) and **ticket_head** (all but the last character). Then we merge **surname** and **ticket_head** to create a **group_id**

```{r}
titanic_data <- titanic_data %>% 
  mutate(ticket_head = substr(ticket, 1, nchar(ticket)-1),
         ticket_tail = substr(ticket, nchar(ticket), nchar(ticket)),
         group_id = paste0(surname, "_", ticket_head)) 
```

Creating columns that indicate the number of people and other flags

```{r}
titanic_data <- titanic_data %>% 
  add_count(group_id) %>% rename(pax_in_group = n)

titanic_data <- titanic_data %>% 
  mutate(flag = case_when ((family_count==1 & pax_in_group==1) ~ "1_solo",
                           family_count == pax_in_group ~ "2_family_full",
                           !(family_count == pax_in_group) ~ "3_clubbed",
                           .default = "x"))

titanic_data <- titanic_data %>% 
  add_count(ticket_head) %>% 
  rename(pax_in_ticket_head = n)

# how many instances of the same ticket having multiple groups? 
titanic_data <- titanic_data %>% 
  group_by(ticket) %>% 
  mutate(groups_in_ticket = n_distinct(group_id)) %>% ungroup()

# which tickets that have more than 1 groups in them? 
#     these passengers will have ticket_grouping precedence as they may include 
#     nannies, relatives & friends that don't share the same surname
ticket_with_multiple_groups <- titanic_data %>% 
  filter(!groups_in_ticket==1) %>% 
  count(ticket, sort = T)

titanic_data <- titanic_data %>% 
  mutate(final_grouping = if_else(ticket %in% ticket_with_multiple_groups$ticket, 
                             ticket, group_id),
         final_label = if_else(ticket %in% ticket_with_multiple_groups$ticket,
                         "4_ticket_grouping", flag)) 
```

Since the code now has become a bit too long and I'm creating a checkpost here with a new df called `titanic_data2`

```{r}
titanic_data2 <- titanic_data %>% 
  select(source, passenger_id, survived,  sex, age, fare, pclass, embarked, 
         family_count, pax_in_group, pax_in_ticket_head, groups_in_ticket,
         final_grouping) %>% 
  mutate(final_grouping = as_factor(final_grouping),
         survived = as_factor(survived),
         pclass = as_factor(pclass),
         sex = as_factor(sex),
         embarked = as_factor(embarked))
glimpse(titanic_data2)
```

Now this is the part I don't get. Why does Kaggle call it `train` and `test` when it can be easily told to be `given_data` and `to_predict` data?

```{r}
to_predict <- titanic_data2 %>% 
  filter(source == "test") %>% select(-survived, -source)
given_data <- titanic_data2 %>% 
  filter(source == "train") %>% select(-source)
```

`given_data` has all the necessary columns we need to train the algo on

```{r}
glimpse(given_data)
```

`to_predict` dataframe has everything else except **survived** column that needs to be predicted.

```{r}
glimpse(to_predict)
```

Phew. finally done with the pre-processing. Thanks for sticking around. Here have a gif.

![](https://media.giphy.com/media/lSVXGiITXtabGGxAyg/giphy.gif){width="75%"}

# Model Building

So today I'm going to be taking different classification models and compare the findings on a 10-fold bootstrap resample. The 4 selected models are:

1.  Logistic classification
2.  Random Forest
3.  Support Vector Machines
4.  Decision Trees

The [Tidymodels textbook](https://www.tmwr.org/workflows.html#workflow-sets-intro) is possibly the best place to begin understanding about workflow. In a nutshell:

1.  Create resampling folds

2.  Describe each model (I'm going to be using untuned parameters for sake of simplicity)

3.  create a base recipe for `given_data`

4.  Defining a workflow that uses the recipe for each model

5.  Fit the resampled folds on the workflow_set

6.  Find the winning model - since we're dealing with classification, I am using `roc_auc`

7.  fit the winning model to the `given data`

8.  Generate predictions on `to_predict` df using the best fit

9.  submit on kaggle

10. ...

11. profit? (well not exactly. but you can create another blog and share your new found knowledge with the worldü§ì)

So lets get cooking!

## Step 1: creating resampling folds

```{r}
set.seed(2023)
titanic_folds <- bootstraps(data = given_data, 
                            times = 15)
titanic_folds

```

## Step 2: Recipe

Creating the `base_recipe` object that has only 3 steps

```{r}
base_recipe <- recipe(survived ~ ., data = given_data) %>% 
  update_role(passenger_id, new_role = "id_variable") %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_normalize(all_numeric_predictors()) 
base_recipe
```

Or if one would like to see it in a `tidy` format.

```{r}
tidy(base_recipe)
```

## Step 3: Model definitions

All the four models are defined as:

```{r}
#logistic regression
glm_model <- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")

#random forest
rf_model <- rand_forest(trees = 1000) %>% 
  set_engine("ranger") %>% 
  set_mode("classification")

#support vector machines
svm_model <- svm_rbf() %>% # rbf - radial based
  set_engine("kernlab") %>% 
  set_mode("classification")

#decision tree
dt_model <- decision_tree(mode = "classification", 
                          tree_depth = 3) %>% 
  set_engine("rpart")
```

## Step 4: Workflow set

Just as you would with a single model, the workflow combines the base recipe with the multiple models by using lists when declaring the object.

```{r}
titanic_wf_set <- workflow_set(
  list(base_recipe),
  list(glm_model, rf_model, svm_model, dt_model),
  cross = T
)
titanic_wf_set
```

In the table above, the **result** column shows a list\[0\] implying that it is currently empty. This is because till now, we've only defined the workflows and models but are yet to pass the data though it.

Obligatory shout out to the phenom Julia Silge whose YT tutorials and blogs have been my learning books at each step. These steps are succinctly explained in her [Tidy Tuesday post](https://juliasilge.com/blog/giant-pumpkins/) which I highly recommend.

## Step 5: Fitting on resampled folds

So this part of it needs to be dealt with care. Resampling may take a while...

```{r}
start_time <- Sys.time()
set.seed(2023)
doParallel::registerDoParallel()
titanic_rs <- workflow_map(
  titanic_wf_set,
  "fit_resamples",
  resamples = titanic_folds
)
end_time <- Sys.time()
```

... which is ...

```{r}
end_time - start_time
```

Hmm... So moving on. What does the workflow_set object look like now?

```{r}
titanic_rs
```

## Step 6: Finding the winning model

This is how the `tidymodels` package all fits in. If you've got all the steps covered this far, the decision making shouldn't take much effort

```{r}
collect_metrics(titanic_rs)
```

Since we're looking at classification, lets see which one of the models resulted in the best `roc_auc`?

```{r}
collect_metrics(titanic_rs) %>% 
  filter(.metric == "roc_auc") %>% 
  arrange(desc(mean))
```

Looks like the `rand_forest` is the winner!

![](https://media.giphy.com/media/dZLx6Lf738Jy46tUR4/giphy.gif){width="360"}

Another wonderful way it all ties in together is the visualisation with a single line of code.

```{r}
autoplot(titanic_rs)
```

Pieces of code that just fit into each other feels like a spoon-full of ice cream!

## Step 7 & 8: Fitting the winner and predictions

From the object `titanic_rs` we need to pick the winning model and fit to `to_predict`

```{r}
final_fit <- extract_workflow(titanic_rs, 
                              "recipe_rand_forest") %>% 
  fit(given_data)

final_fit
```

Creating a new dataframe for predictions

```{r}
final_predictions <- predict(object = final_fit, 
                             new_data = to_predict)

final_predictions <- final_predictions %>% 
  rename(Survived = .pred_class) %>% 
  bind_cols(PassengerId = to_predict$passenger_id)
head(final_predictions)
```

## Step 9: Submit to Kaggle

Converting the `final_predictions` to csv and uploading to kaggle

```{r}
write.csv(final_predictions, row.names = F, 
          file = "submissions.csv")
```

Uploaded it to Kaggle and behold...

![](images/image-1373401355.png)

Not bad !!! that's like... top 12% percentile. Woohoo!

![.. And so did you, dear reader!!](https://media.giphy.com/media/7zYKTVt3vvbj7SC2Bl/giphy.gif){width="50%"}

# Understanding variable importance

Since we've got a random forest winner, I wanted to take a look at the importance of the variables used.

Inspired from Julia's [Tidy Tuesday on Ikea Prices](https://juliasilge.com/blog/ikea-prices/). For a ranger model, we do need to go back to the model specification itself and update the engine with `importance = "permutation"` in order to compute feature importance. This means fitting the model one more time.

```{r}
library(vip)

rf_model2 <- rand_forest(trees = 1000) %>% 
  set_engine("ranger", importance = "permutation") %>% 
  set_mode("classification")

workflow() %>% 
  add_recipe(base_recipe) %>% 
  add_model(rf_model2) %>% 
  fit(given_data) %>% 
  pull_workflow_fit() %>% 
  vip(aesthetics = list(alpha= 0.8, fill = "green"))
```

This is interesting to me. While I was expecting the **gender** and **class** predictors to play an important part in the model, other grouped columns like **family_count, pax_in_group & pax_in_ticket_head** also added in improving the accuracy.

# Tailpiece regarding `autoplot()`

After model creation, I couldn't help wonder at the marvel behind the `autoplot()` function. How can a single function decipher convey such meaningful depth?

## Peeking under the hood

I tried to replicate the same errorbar plot generated by the function. For me, this was possibly one of the bigger conceptual lessons about `tidymodels` framework. Here's how I did it:

There are 15 bootsample folds that were generated. The `workflow_set` maps the `fit_resamples` function on the workflow. This implies that each of the 4 models have generated 2 pairs of metrics (`accuracy` & `roc_auc`) for each of the 15 resamples.

The `collect_metrics` function generates the mean vale of each metric allowing us to chose the best one.

The manner in which all of this is done within a single object is sublime. Information is stored in tibbles within tibbles!

For instance, let us look at the class of `titanic_rs`

```{r}
class(titanic_rs)
```

It is a tibble with 4 columns out of which `results` is a list object consisting of more tibbles.

```{r}
titanic_rs
```

What does the first tibble in the `results` list look like? This corresponds to the logistic regression model as shown in the table above.

```{r}
titanic_rs$result[[1]]
```

What does the `.metrics` list contain?

```{r}
titanic_rs$result[[1]]$.metrics[[1]]
```

So `.metrics` is a list of results that is generated for each of the folds. The above tibble is the resulting metrics after cross-validation of the first fold of `titanic_folds` using `glm` (logisitic classification).

If the intent is to create the errorgraph manually, then we'll need to extract the data within each of these tibbles.

Now I wasn't able to check the code for `autoplot()` and I'm pretty certain there's a more elegant method out there. Hit me up if you feel there's a better way to extract data from within tibbles in the comments here üëâüèº

First step is to create an empty tibble consisting of 4 models, 2 metrics and 80 empty cells that need to be filled in from `titanic_rs` object.

```{r}
tidy_titanic_rs <- tibble(wf = rep(unique(titanic_rs$wflow_id), 15*2),
                          metrics = rep(c(rep("accuracy", 4), 
                                          rep("roc_auc",4)),
                                        15),
                          values = rep(NA, 4*15*2))
head(tidy_titanic_rs, 8)
```

Dimensions of this empty table?

```{r}
dim(tidy_titanic_rs)
```

To extract the values from the tibbles in `titanic_rs` and save in `tidy_titanic_rs`, I proceeded to employ a nested `for` loop. Dont' judge, i say!

## Extracting the tibbles

`pluck` has got to be the coolest function I've ever come across!

```{r}
bigtable <- purrr:::pluck(titanic_rs, 4)
wflow_id_titanic <- unique(titanic_rs$wflow_id)
for(i in 1:length(wflow_id_titanic)){
    
  wflow_id <- wflow_id_titanic[i]
  smalltable <- bigtable[[i]]
  
  for(j in 1:length(smalltable$.metrics)){
    smallertable <- purrr::pluck(smalltable$.metrics, j)
    tidy_titanic_rs$values[(tidy_titanic_rs$wf==wflow_id & 
                              tidy_titanic_rs$metrics=="accuracy")][j] <- smallertable$.estimate[smallertable$.metric == "accuracy"]
    tidy_titanic_rs$values[(tidy_titanic_rs$wf==wflow_id & 
                              tidy_titanic_rs$metrics=="roc_auc")][j] <- smallertable$.estimate[smallertable$.metric == "roc_auc"]
    
  }
}

tidy_titanic_rs2 <- tidy_titanic_rs %>% 
  group_by(wf, metrics) %>% 
  summarise(value_min = mean(values) - 0.5*sd(values),
            value_max = mean(values) + 0.5*sd(values),
            value_mean = mean(values)) %>% ungroup() %>% 
  right_join(tidy_titanic_rs, by = c("wf", "metrics"))

```

For some reason, `value_max` and `value_min` for the error-bar are 0.5 \* ùùà or approximately [¬±19% of the mean of values](https://mathbitsnotebook.com/Algebra2/Statistics/STstandardNormalDistribution.html)

## Manually generated plot

With the data in a rectangular tidy format, the rest of the magic is handled by `ggplot`

***cracks knuckles***

```{r}
tidy_titanic_rs2 %>% 
ggplot(aes(x = reorder(wf, desc(value_mean)), 
             y = values, 
             color = wf))+
  geom_errorbar(aes(ymax = value_max, ymin = value_min), 
                width = 0.1)+
  geom_point(aes(y= value_mean))+
  scale_y_continuous(breaks = seq(0.65, 0.9, 0.05),
                     limits = c(0.65, 0.9))+
  theme(legend.position = "none")+
  labs(x = NULL, y = NULL)+
  facet_wrap(~metrics)
```

And here once again is the auto-generated plot. Not bad, eh?

```{r}
autoplot(titanic_rs)
```

![Oooh yeeah!](https://media.giphy.com/media/3oEduKVQdG4c0JVPSo/giphy.gif){width="50%"}

# 
